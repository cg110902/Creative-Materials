# ğŸ“‹ Claudeå†™ä½œæ‰§è¡ŒSOP v3.0


---

## ã€æ¨¡å—0ã€‘GLOBAL_CONFIG - å…¨å±€é…ç½®

```python
# ==================== å…¨å±€å¸¸é‡ ====================
CONST WORD_COUNT_TARGET = 4000  # ç›®æ ‡å­—æ•°
CONST WORD_COUNT_MIN = 3000     # æœ€å°å­—æ•°ï¼ˆç¡¬çº¦æŸï¼‰
CONST WORD_COUNT_MAX = 6000     # æœ€å¤§å­—æ•°ï¼ˆç¡¬çº¦æŸï¼‰
CONST MAX_REWRITE_ATTEMPTS = 2  # æœ€å¤§é‡å†™æ¬¡æ•°

# ==================== ç•ªèŒ„é£æ ¼æ ¸å¿ƒè§„åˆ™ ====================
CONST TOMATO_CORE_RULES = {
    "pace": "å¿«èŠ‚å¥ï¼Œä¸æ‹–æ³¥å¸¦æ°´",
    "hook_frequency": 600,           # æ¯600å­—ä¸€ä¸ªå°é’©å­
    "dialogue_ratio": [0.35, 0.45],  # å¯¹è¯å æ¯”35%-45%
    "inner_monologue_max": 0.15,     # å†…å¿ƒæˆä¸è¶…è¿‡15%
    "paragraph_max_length": 150,     # å•æ®µä¸è¶…è¿‡150å­—
    "info_density_min": 0.01,        # æ¯100å­—è‡³å°‘1ä¸ªä¿¡æ¯ç‚¹
    "conflict_intensity_floor": 30,  # å†²çªå¼ºåº¦åº•çº¿
    "boredom_threshold": 60          # æ— èŠåº¦é˜ˆå€¼
}

# ==================== Claudeç—…ç—‡å®Œæ•´æ¸…å•ï¼ˆ20æ¡ï¼‰====================
CONST CLAUDE_ALL20_DISEASES = {
    "D1_è¿‡åº¦å±•å¼€": "æ¯ä¸ªä¿¡æ¯ç‚¹éƒ½å±•å¼€200å­—",
    "D2_è¿‡åº¦è§£é‡Š": "æ¯ä¸ªé€»è¾‘éƒ½è¦ç»™è¯»è€…è§£é‡Šæ¸…æ¥š",
    "D3_è¿‡åº¦å†…å¿ƒæˆ": "ä¸»è§’ä¸€ä¸ªå°åŠ¨ä½œé…500å­—å¿ƒç†æ´»åŠ¨",
    "D4_è¿‡åº¦å®‰å…¨": "é£è¯é€ å¥è°¨æ…å¾—åƒåœ¨å†™æ³•å¾‹æ–‡ä¹¦",  # ã€æ–°å¢ã€‘
    "D5_è¿‡åº¦å†—ä½™": "åŒä¸€ä¸ªæ„æ€ç”¨3ç§æ–¹å¼é‡å¤è¡¨è¾¾",      # ã€æ–°å¢ã€‘
    "D6_è¿‡åº¦å¹³å‡": "æ¯ä¸ªåœºæ™¯ã€æ¯ä¸ªè§’è‰²çš„ç¬”å¢¨åˆ†é…éƒ½å¾ˆå‡åŒ€", # ã€æ–°å¢ã€‘
    "D7_è¿‡åº¦é€»è¾‘": "æ‰€æœ‰è¡Œä¸ºéƒ½è¦æœ‰æ˜ç¡®çš„å› æœé“¾",        # ã€æ–°å¢ã€‘
    "D8_è¿‡åº¦å®Œæ•´": "æ¯ä¸ªåœºæ™¯éƒ½è¦æœ‰'èµ·æ‰¿è½¬åˆ'",          # ã€æ–°å¢ã€‘
    "D9_è¿‡åº¦ç¤¼è²Œ": "è§’è‰²è¯´è¯éƒ½å¾ˆå®¢æ°”ï¼Œæ²¡æœ‰ç²—é²/æš´èº/çœç•¥", # ã€æ–°å¢ã€‘
    "D10_è¿‡åº¦å¯¹ç§°": "ä¸»è§’åšAï¼Œé…è§’å°±ä¼šå›åº”Bï¼ŒèŠ‚å¥å‘†æ¿",  # ã€æ–°å¢ã€‘
    "D11_ä¿¡æ¯å¯†åº¦ä¸è¶³": "ç”¨200å­—è®²ä¸€ä¸ªåªéœ€è¦20å­—çš„äº‹",   # ã€æ–°å¢ã€‘
    "D12_å£æ°´è¯æ»¥ç”¨": "å¤§é‡çš„'ä¼¼ä¹''ä»¿ä½›''æˆ–è®¸''å¯èƒ½'ç­‰æ¨¡ç³Šè¯",
    "D13_å¥—è·¯åŒ–è½¬æŠ˜": "æ€»æ˜¯ç”¨'ç„¶è€Œ''ä½†æ˜¯''å°±åœ¨è¿™æ—¶'ç­‰è€å¥—è½¬æŠ˜è¯",
    "D14_è¿‡åº¦é“ºå«": "ä¸€ä¸ªå°äº‹ä»¶å‰é¢é“ºå«300å­—",          # ã€æ–°å¢ã€‘
    "D15_ç»“å°¾æ€»ç»“ç™–": "æ¯ä¸ªåœºæ™¯ç»“å°¾éƒ½è¦æ¥ä¸€å¥æ€»ç»“æ€§çš„è¯",
    "D16_ä¸æ•¢ç•™ç™½": "æ‰€æœ‰ä¼ç¬”éƒ½è¦æš—ç¤ºï¼Œç”Ÿæ€•è¯»è€…çœ‹ä¸æ‡‚",
    "D17_ä¸æ•¢è·³è·ƒ": "æ—¶é—´/ç©ºé—´åˆ‡æ¢éƒ½è¦è¯¦ç»†äº¤ä»£è¿‡æ¸¡",     # ã€æ–°å¢ã€‘
    "D18_æƒ…ç»ªè¯æ»¥ç”¨": "åŠ¨ä¸åŠ¨å°±'éœ‡æƒŠ''ææƒ§''æ„¤æ€’'ç­‰å¼ºæƒ…ç»ªè¯",
    "D19_åŠ¨ä½œé“¾å®Œæ•´ç™–": "'ä»–ç«™èµ·æ¥ï¼Œèµ°åˆ°é—¨å£ï¼Œä¼¸æ‰‹æ¨å¼€é—¨ï¼Œè¿ˆæ­¥èµ°å‡ºå»'",
    "D20_è§†è§’æ¼‚ç§»": "æ˜æ˜æ˜¯ä¸»è§’è§†è§’ï¼Œå´çªç„¶çŸ¥é“äº†é…è§’çš„æƒ³æ³•"
}

# ==================== å¸¸ç”¨çˆ½ç‚¹ç±»å‹ï¼ˆç²¾ç®€åˆ°5ç§ï¼‰====================
CONST COOL_POINT_TYPES = {
    "æ‰“è„¸çˆ½": {
        "ç»“æ„": "è¢«è´¨ç–‘ â†’ ç¬é—´åè½¬ â†’ å¯¹æ–¹éœ‡æƒŠ",
        "å¼ºåº¦": 80,
        "ä½¿ç”¨åœºæ™¯": ["å±•ç¤ºå®åŠ›", "æ‹†ç©¿ä¼ªè£…", "åé©³è´¨ç–‘"]
    },
    "è£…é€¼çˆ½": {
        "ç»“æ„": "ä½è°ƒéšè—å®åŠ› â†’ å…³é”®æ—¶åˆ»å±•ç° â†’ ä¼—äººéœ‡æ’¼",
        "å¼ºåº¦": 85,
        "ä½¿ç”¨åœºæ™¯": ["å±æœºæ—¶åˆ»", "è¢«è½»è§†å", "ä¿æŠ¤ä»–äºº"]
    },
    "å¤ä»‡çˆ½": {
        "ç»“æ„": "è¢«æ¬ºè´Ÿ â†’ è®°ä¸‹ä»‡æ¨ â†’ å½“åœºæˆ–å»¶åæŠ¥å¤",
        "å¼ºåº¦": 90,
        "ä½¿ç”¨åœºæ™¯": ["è¢«ç¾è¾±", "è¢«é™·å®³", "è¢«èƒŒå›"]
    },
    "å‡çº§çˆ½": {
        "ç»“æ„": "çªç ´å¢ƒç•Œ â†’ å®åŠ›æš´æ¶¨ â†’ ç«‹åˆ»éªŒè¯",
        "å¼ºåº¦": 95,
        "ä½¿ç”¨åœºæ™¯": ["ä¿®ç‚¼", "é¡¿æ‚Ÿ", "è·å¾—ä¼ æ‰¿"]
    },
    "åæ€çˆ½": {
        "ç»“æ„": "æ¿’æ­»ç»å¢ƒ â†’ çªç„¶ç¿»ç›˜ â†’ åæ€æ•Œäºº",
        "å¼ºåº¦": 100,
        "ä½¿ç”¨åœºæ™¯": ["ç”Ÿæ­»å±æœº", "ç»å¢ƒ", "èƒŒæ°´ä¸€æˆ˜"]
    }
}

# ==================== å…·ä½“æ€§ç­‰çº§è¡¨ï¼ˆç®€åŒ–ï¼‰====================
CONST SPECIFICITY_RULES = {
    "importance >= 8": "å…·ä½“",    # "ä»–ä¼¸å‡ºå³æ‰‹ï¼Œäº”æŒ‡å¾®æ›²ï¼Œæ¡ä½é—¨æŠŠæ‰‹"
    "importance 5-7": "ä¸­ç­‰",     # "ä»–æ¡ä½é—¨æŠŠæ‰‹"
    "importance < 5": "æŠ½è±¡"      # "ä»–å¼€é—¨"
}

# ==================== èŠ‚å¥æ§åˆ¶è§„åˆ™ï¼ˆç”¨å¥å¼ä»£æ›¿æ³¢å½¢ï¼‰====================
CONST RHYTHM_RULES = {
    "å¿«èŠ‚å¥": {
        "sentence_length": [5, 15],      # å¥é•¿5-15å­—
        "paragraph_length": [30, 80],    # æ®µé•¿30-80å­—
        "pattern": "çŸ­å¥+çŸ­å¥+çŸ­å¥"
    },
    "ä¸­èŠ‚å¥": {
        "sentence_length": [15, 25],
        "paragraph_length": [80, 120],
        "pattern": "é•¿çŸ­å¥äº¤é”™"
    },
    "æ…¢èŠ‚å¥": {
        "sentence_length": [20, 40],
        "paragraph_length": [100, 150],
        "pattern": "é•¿å¥ï¼Œè¯¦ç»†æå†™"
    }
}

# ==================== Show vs Tellè§„åˆ™ï¼ˆåŠ¨æ€ï¼‰====================
CONST SHOW_TELL_RULES = {
    "æƒ…ç»ª": {
        "importance >= 7": "SHOW_DETAILED",  # è¯¦å†™ç”Ÿç†ååº”
        "importance 4-6": "SHOW_BRIEF",      # ç®€å†™ç”Ÿç†ååº”
        "importance < 4": "TELL"              # ç›´æ¥è¯´
    },
    "åŠ¨ä½œ": "SHOW_MOSTLY",     # åŠ¨ä½œä»¥Showä¸ºä¸»
    "ä¿¡æ¯": "TELL_OK",         # ä¿¡æ¯å¯ä»¥Tell
    "ç¯å¢ƒ": "SHOW_SELECTIVELY" # ç¯å¢ƒé€‰æ‹©æ€§Show
}
```

---

## ã€æ¨¡å—1ã€‘MAIN_EXECUTION_LOOP - ä¸»æ‰§è¡Œæµç¨‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰

```python
FUNCTION MAIN_EXECUTION(CAPSULE):
    """
    ä¸»æ‰§è¡Œæµç¨‹ï¼šé›†æˆå®æ—¶è½»é‡æ£€æŸ¥ + æœ€ç»ˆå…¨å±€è¯Šæ–­
    """
    
    # ========== STEP 1: è§£æèƒ¶å›Š ==========
    PRINT "[SYSTEM] å¼€å§‹è§£æä¿¡æ¯èƒ¶å›Š..."
    parsed_data = PARSE_CAPSULE(CAPSULE)
    VALIDATE_CAPSULE_INTEGRITY(parsed_data)
    
    # ========== STEP 2: ç”Ÿæˆä»»åŠ¡æ¸…å•ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰==========
    PRINT "[SYSTEM] ç”Ÿæˆæ‹ŸäººåŒ–ä»»åŠ¡æ¸…å•..."
    humanizer = INIT_HUMANIZE_ENGINE_V3(parsed_data)
    # è¿”å›æ˜ç¡®çš„ä»»åŠ¡æ¸…å•ï¼Œè€Œéæ¦‚ç‡é…ç½®
    
    # ========== STEP 3: åˆå§‹åŒ–å®æ—¶ç›‘æ§å™¨ ==========
    monitors = {
        "word_count": 0,
        "last_info_point": 0,
        "last_conflict": 0,
        "scene_checks": []  # æ¯ä¸ªåœºæ™¯çš„è½»é‡æ£€æŸ¥ç»“æœ
    }
    
    # ========== STEP 4: åˆ†åœºæ™¯å†™ä½œï¼ˆé›†æˆè½»é‡æ£€æŸ¥ï¼‰==========
    chapter_content = ""
    scene_count = GET_SCENE_COUNT_FROM_CAPSULE(parsed_data)
    
    FOR scene_idx = 1 TO scene_count:
        PRINT "[WRITING] æ­£åœ¨å†™ä½œåœºæ™¯ {scene_idx}/{scene_count}..."
        
        # 4.1 å†™ä½œåœºæ™¯
        scene_text = WRITE_SCENE_V3(
            scene_idx, 
            parsed_data, 
            humanizer, 
            chapter_content
        )
        
        chapter_content += scene_text
        monitors.word_count += LENGTH(scene_text)
        
        # 4.2 å®æ—¶è½»é‡æ£€æŸ¥ï¼ˆå…³é”®æ”¹è¿›ï¼‰
        check_result = LIGHTWEIGHT_SCENE_CHECK(
            scene_text, 
            scene_idx, 
            parsed_data, 
            monitors
        )
        
        monitors.scene_checks.APPEND(check_result)
        
        # 4.3 ç´§æ€¥å¹²é¢„ï¼ˆä»…å¤„ç†ä¸¥é‡é—®é¢˜ï¼‰
        IF check_result.severity == "CRITICAL":
            PRINT "[EMERGENCY] åœºæ™¯{scene_idx}å‘ç°ä¸¥é‡é—®é¢˜ï¼Œç«‹å³é‡å†™..."
            scene_text = REWRITE_SCENE_WITH_FIX(
                scene_idx, 
                parsed_data, 
                humanizer, 
                check_result.issue
            )
            chapter_content = REPLACE_LAST_SCENE(chapter_content, scene_text)
        END IF
        
        # 4.4 å­—æ•°é¢„è­¦
        IF monitors.word_count > WORD_COUNT_MAX * 0.85:
            PRINT "[WARNING] å­—æ•°æ¥è¿‘ä¸Šé™ï¼Œå‡†å¤‡æ”¶å°¾..."
            humanizer.force_ending = TRUE
        END IF
    END FOR
    
    # ========== STEP 5: é›†ä¸­åClaudeç—…æ¶¦è‰²ï¼ˆå…³é”®æ”¹è¿›ï¼‰==========
    PRINT "[SYSTEM] æ‰§è¡Œé›†ä¸­åClaudeç—…æ¶¦è‰²..."
    chapter_content = APPLY_ALL_ANTI_DISEASE_RULES(chapter_content, parsed_data)
    
    # ========== STEP 6: å…¨å±€è¯Šæ–­ ==========
    PRINT "[SYSTEM] å¼€å§‹å…¨å±€è¯Šæ–­..."
    diagnosis = DIAGNOSE_CHAPTER_V3(chapter_content, parsed_data, monitors)
    
    # ========== STEP 7: ä¿®æ­£æˆ–é‡å†™ï¼ˆåŠ å…¥é—®é¢˜åˆ†æï¼‰==========
    IF diagnosis.has_critical_issues:
        IF NOT EXISTS(humanizer, "rewrite_count"):
            humanizer.rewrite_count = 0
        END IF
        
        IF humanizer.rewrite_count < MAX_REWRITE_ATTEMPTS:
            # å…³é”®æ”¹è¿›ï¼šåŠ å…¥é—®é¢˜åˆ†æ
            PRINT "[SYSTEM] åˆ†æé—®é¢˜åŸå› ..."
            problem_analysis = ANALYZE_WHAT_WENT_WRONG(diagnosis, parsed_data)
            
            PRINT "[SYSTEM] ç”Ÿæˆä¿®æ­£æŒ‡ä»¤..."
            fix_instruction = GENERATE_FIX_INSTRUCTION(problem_analysis)
            
            humanizer.rewrite_count += 1
            PRINT "[SYSTEM] ç¬¬{humanizer.rewrite_count}æ¬¡é‡å†™ï¼ˆé’ˆå¯¹æ€§ä¿®æ­£ï¼‰..."
            
            # é€’å½’é‡å†™ï¼Œä½†å¸¦ä¸Šä¿®æ­£æŒ‡ä»¤
            RETURN MAIN_EXECUTION_WITH_FIX(CAPSULE, fix_instruction, humanizer)
        ELSE:
            PRINT "[ERROR] å·²è¾¾æœ€å¤§é‡å†™æ¬¡æ•°ï¼Œå¼ºåˆ¶äº¤ä»˜"
            diagnosis.forced_delivery = TRUE
        END IF
    END IF
    
    # ========== STEP 8: æå–Fact ==========
    new_facts = EXTRACT_FACTS(chapter_content, parsed_data)
    
    # ========== STEP 9: äº¤ä»˜ ==========
    RETURN DELIVER_OUTPUT_V3(chapter_content, new_facts, diagnosis, monitors)
END FUNCTION
```

---

## ã€æ¨¡å—2ã€‘HUMANIZE_ENGINE_V3 - æ‹ŸäººåŒ–å¼•æ“ï¼ˆæ ¸å¿ƒæ”¹è¿›ï¼‰

```python
FUNCTION INIT_HUMANIZE_ENGINE_V3(parsed_data):
    """
    æ‹ŸäººåŒ–å¼•æ“v3ï¼šä»æ¦‚ç‡æœºåˆ¶æ”¹ä¸ºä»»åŠ¡æ¸…å•æœºåˆ¶
    æ ¸å¿ƒæ”¹è¿›ï¼šæ˜ç¡®å‘Šè¯‰Claude"å“ªä¸ªåœºæ™¯å¿…é¡»åšä»€ä¹ˆ"
    """
    
    humanizer = {
        "task_checklist": [],    # ä»»åŠ¡æ¸…å•ï¼ˆæ ¸å¿ƒï¼‰
        "scene_budget": {},      # æ¯ä¸ªåœºæ™¯çš„å­—æ•°é¢„ç®—
        "cool_point_plan": {},   # çˆ½ç‚¹è®¡åˆ’
        "foreshadow_plan": {},   # ä¼ç¬”è®¡åˆ’
        "rewrite_count": 0,
        "force_ending": FALSE
    }
    
    # ========== ç”Ÿæˆä»»åŠ¡æ¸…å• ==========
    scene_count = ESTIMATE_SCENE_COUNT(parsed_data)
    
    FOR scene_idx = 1 TO scene_count:
        tasks = []
        
        # ä»»åŠ¡1ï¼šå¼€ç¯‡æ–¹å¼ï¼ˆæ˜ç¡®æŒ‡å®šï¼‰
        IF scene_idx == 1:
            tasks.APPEND({
                "type": "å¼€ç¯‡",
                "action": RANDOM_CHOICE(["åŠ¨ä½œç›´å…¥", "å¯¹è¯ç›´å…¥", "æ„Ÿå®˜ç›´å…¥", "å†²çªç›´å…¥"]),
                "priority": "MUST"
            })
        END IF
        
        # ä»»åŠ¡2ï¼šè·³è·ƒå¼åˆ‡å…¥ï¼ˆ30%åœºæ™¯ï¼‰
        IF scene_idx > 1 AND scene_idx <= scene_count * 0.3:
            tasks.APPEND({
                "type": "è·³è·ƒåˆ‡å…¥",
                "action": "åˆ é™¤åœºæ™¯å‰3å¥é“ºå«ï¼Œç›´æ¥ä»åŠ¨ä½œ/å¯¹è¯å¼€å§‹",
                "priority": "MUST"
            })
        END IF
        
        # ä»»åŠ¡3ï¼šæ•…æ„çœç•¥ï¼ˆé€‰æ‹©2-3ä¸ªåœºæ™¯ï¼‰
        IF scene_idx IN RANDOM_SAMPLE(RANGE(1, scene_count), 2):
            tasks.APPEND({
                "type": "æ•…æ„çœç•¥",
                "action": "æŸä¸ªåŠ¨æœºä¸è§£é‡Šï¼Œè®©è¯»è€…çŒœ",
                "priority": "SHOULD"
            })
        END IF
        
        # ä»»åŠ¡4ï¼šç¬”å¢¨å¤±è¡¡ï¼ˆé€‰æ‹©1-2ä¸ªåœºæ™¯ï¼‰
        IF scene_idx IN RANDOM_SAMPLE(RANGE(2, scene_count), 2):
            tasks.APPEND({
                "type": "ç¬”å¢¨å¤±è¡¡",
                "action": "æŸä¸ªæ¬¡è¦è§’è‰²/ç‰©å“çªç„¶å¤šå†™50å­—",
                "priority": "SHOULD"
            })
        END IF
        
        # ä»»åŠ¡5ï¼šå¯¹è¯ä¸å›åº”ï¼ˆåœºæ™¯å†…ä»»åŠ¡ï¼‰
        IF RANDOM() < 0.25:  # 25%åœºæ™¯
            tasks.APPEND({
                "type": "å¯¹è¯ä¸å›åº”",
                "action": "è‡³å°‘æœ‰1å¤„ï¼šAè¯´è¯ï¼ŒBä¸æ¥èŒ¬ï¼Œç›´æ¥åšåˆ«çš„",
                "priority": "SHOULD"
            })
        END IF
        
        humanizer.task_checklist[scene_idx] = tasks
    END FOR
    
    # ========== åˆ†é…åœºæ™¯å­—æ•°é¢„ç®— ==========
    total_budget = WORD_COUNT_TARGET
    scene_importances = GET_SCENE_IMPORTANCES(parsed_data, scene_count)
    
    FOR scene_idx = 1 TO scene_count:
        importance = scene_importances[scene_idx]
        # é‡è¦åœºæ™¯å¤šç»™å­—æ•°
        humanizer.scene_budget[scene_idx] = total_budget * (importance / SUM(scene_importances))
    END FOR
    
    # ========== çˆ½ç‚¹è®¡åˆ’ ==========
    # ä»Capsuleç›´æ¥è¯»å–ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™è‡ªåŠ¨è§„åˆ’
    IF parsed_data.goals.cool_point_plan EXISTS:
        humanizer.cool_point_plan = parsed_data.goals.cool_point_plan
    ELSE:
        # è‡ªåŠ¨è§„åˆ’ï¼šå¹³å‡æ¯1500å­—ä¸€ä¸ªçˆ½ç‚¹
        cool_point_count = WORD_COUNT_TARGET / 1500
        cool_point_scenes = DISTRIBUTE_EVENLY(RANGE(1, scene_count), cool_point_count)
        
        FOR scene_idx IN cool_point_scenes:
            suitable_type = SELECT_COOL_POINT_TYPE_FOR_SCENE(scene_idx, parsed_data)
            humanizer.cool_point_plan[scene_idx] = suitable_type
        END FOR
    END IF
    
    # ========== ä¼ç¬”è®¡åˆ’ ==========
    # ä»Â§16è¯»å–ï¼Œæ˜ç¡®æ˜¾çœ¼åº¦
    FOR foreshadow IN parsed_data.goals.foreshadowing:
        scene_idx = CHOOSE_SCENE_FOR_FORESHADOW(foreshadow, scene_count)
        humanizer.foreshadow_plan[scene_idx] = {
            "content": foreshadow.content,
            "visibility": foreshadow.visibility  # ç›´æ¥ä»CapsuleæŒ‡å®š
        }
    END FOR
    
    PRINT "[HUMANIZE] ä»»åŠ¡æ¸…å•ç”Ÿæˆå®Œæˆ"
    PRINT "[HUMANIZE] åœºæ™¯æ•°é‡: {scene_count}"
    PRINT "[HUMANIZE] çˆ½ç‚¹è®¡åˆ’: {humanizer.cool_point_plan}"
    
    RETURN humanizer
END FUNCTION
```

---

## ã€æ¨¡å—3ã€‘LIGHTWEIGHT_SCENE_CHECK - å®æ—¶è½»é‡æ£€æŸ¥ï¼ˆæ–°å¢ï¼‰

```python
FUNCTION LIGHTWEIGHT_SCENE_CHECK(scene_text, scene_idx, parsed_data, monitors):
    """
    åœºæ™¯å†™å®Œåçš„è½»é‡æ£€æŸ¥
    åªæ£€æŸ¥æœ€å…³é”®çš„é—®é¢˜ï¼Œé¿å…è¿‡åº¦æ£€æŸ¥
    """
    
    check_result = {
        "scene_idx": scene_idx,
        "severity": "OK",  # OK / WARNING / CRITICAL
        "issue": NULL,
        "word_count": LENGTH(scene_text)
    }
    
    # ========== æ£€æŸ¥1ï¼šçº¢çº¿è¿åï¼ˆCRITICALï¼‰==========
    FOR redline IN parsed_data.constraints.redlines:
        IF CHECK_REDLINE_VIOLATION(scene_text, redline):
            check_result.severity = "CRITICAL"
            check_result.issue = f"è¿åçº¢çº¿ï¼š{redline}"
            RETURN check_result
        END IF
    END FOR
    
    # ========== æ£€æŸ¥2ï¼šå­—æ•°ä¸¥é‡è¶…æ ‡ï¼ˆCRITICALï¼‰==========
    IF LENGTH(scene_text) > 1200:  # å•åœºæ™¯è¶…è¿‡1200å­—
        check_result.severity = "CRITICAL"
        check_result.issue = f"åœºæ™¯{scene_idx}å­—æ•°è¿‡å¤š({LENGTH(scene_text)}å­—)ï¼Œä¼šå¯¼è‡´å…¨ç« è¶…æ ‡"
        RETURN check_result
    END IF
    
    # ========== æ£€æŸ¥3ï¼šä¿¡æ¯å¯†åº¦è¿‡ä½ï¼ˆWARNINGï¼‰==========
    info_density = COUNT_NEW_INFO(scene_text) / LENGTH(scene_text)
    IF info_density < TOMATO_CORE_RULES.info_density_min:
        check_result.severity = "WARNING"
        check_result.issue = f"åœºæ™¯{scene_idx}ä¿¡æ¯å¯†åº¦è¿‡ä½({info_density})"
        # WARNINGä¸è§¦å‘é‡å†™ï¼Œä»…è®°å½•
    END IF
    
    # ========== æ£€æŸ¥4ï¼šå†²çªç¼ºå¤±ï¼ˆWARNINGï¼‰==========
    words_since_conflict = monitors.word_count - monitors.last_conflict
    IF words_since_conflict > 600:
        check_result.severity = "WARNING"
        check_result.issue = f"å·²è¿ç»­{words_since_conflict}å­—æ— å†²çª"
    END IF
    
    RETURN check_result
END FUNCTION
```

---

## ã€æ¨¡å—4ã€‘WRITE_SCENE_V3 - åœºæ™¯å†™ä½œå™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
FUNCTION WRITE_SCENE_V3(scene_idx, parsed_data, humanizer, previous_content):
    """
    å†™ä½œå•ä¸ªåœºæ™¯ï¼ˆv3ç®€åŒ–ç‰ˆï¼‰
    æ ¸å¿ƒæ”¹è¿›ï¼šæ‰§è¡Œä»»åŠ¡æ¸…å•ï¼Œè€Œéæ¦‚ç‡å†³ç­–
    """
    
    scene_text = ""
    scene_budget = humanizer.scene_budget[scene_idx]
    tasks = humanizer.task_checklist[scene_idx]
    
    # ========== STEP 1: æ‰§è¡Œå¼€ç¯‡ä»»åŠ¡ ==========
    opening_task = FIND_TASK(tasks, "å¼€ç¯‡")
    IF opening_task:
        scene_text = EXECUTE_OPENING_TASK(opening_task, parsed_data)
    ELSE:
        scene_text = WRITE_DEFAULT_OPENING(scene_idx, parsed_data)
    END IF
    
    # ========== STEP 2: å†™ä½œåœºæ™¯ä¸»ä½“ ==========
    # å°†åœºæ™¯æ‹†è§£ä¸ºå†™ä½œå•å…ƒ
    units = DECOMPOSE_SCENE_TO_UNITS(scene_idx, parsed_data)
    
    FOR unit IN units:
        # 2.1 å­—æ•°æ§åˆ¶
        IF LENGTH(scene_text) > scene_budget * 0.9:
            BREAK  # æ¥è¿‘é¢„ç®—ï¼Œåœæ­¢
        END IF
        
        # 2.2 æ ¹æ®é‡è¦æ€§å†³å®šå±•å¼€ç¨‹åº¦
        expansion_level = DECIDE_EXPANSION_BY_IMPORTANCE(unit.importance)
        
        # 2.3 å†™ä½œå•å…ƒ
        unit_text = WRITE_UNIT_BY_TYPE(
            unit, 
            expansion_level, 
            parsed_data
        )
        
        scene_text += unit_text
    END FOR
    
    # ========== STEP 3: æ‰§è¡Œæ‹ŸäººåŒ–ä»»åŠ¡ ==========
    FOR task IN tasks:
        IF task.type != "å¼€ç¯‡":  # å¼€ç¯‡ä»»åŠ¡å·²æ‰§è¡Œ
            scene_text = EXECUTE_HUMANIZE_TASK(scene_text, task)
        END IF
    END FOR
    
    # ========== STEP 4: æ³¨å…¥çˆ½ç‚¹ï¼ˆå¦‚æœè®¡åˆ’ä¸­æœ‰ï¼‰==========
    IF scene_idx IN humanizer.cool_point_plan:
        cool_type = humanizer.cool_point_plan[scene_idx]
        cool_text = GENERATE_COOL_POINT(cool_type, parsed_data)
        scene_text = INSERT_COOL_POINT(scene_text, cool_text)
    END IF
    
    # ========== STEP 5: åŸ‹ä¼ç¬”ï¼ˆå¦‚æœè®¡åˆ’ä¸­æœ‰ï¼‰==========
    IF scene_idx IN humanizer.foreshadow_plan:
        foreshadow = humanizer.foreshadow_plan[scene_idx]
        foreshadow_text = WRITE_FORESHADOW(foreshadow, parsed_data)
        scene_text = INSERT_FORESHADOW(scene_text, foreshadow_text)
    END IF
    
    # ========== STEP 6: åœºæ™¯ç»“å°¾ ==========
    ending = WRITE_SCENE_ENDING(scene_idx, parsed_data, humanizer)
    scene_text += ending
    
    RETURN scene_text
END FUNCTION

# ==================== è¾…åŠ©å‡½æ•° ====================

FUNCTION DECIDE_EXPANSION_BY_IMPORTANCE(importance):
    """
    æ ¹æ®é‡è¦æ€§å†³å®šå±•å¼€ç¨‹åº¦ï¼ˆè§„åˆ™åŒ–ï¼‰
    """
    IF importance >= 8:
        RETURN "EXPAND"      # å±•å¼€50-100å­—
    ELSE IF importance >= 5:
        RETURN "BRIEF"       # ç®€å†™20-50å­—
    ELSE:
        RETURN "SKIP"        # ä¸€å¥è¯å¸¦è¿‡æˆ–è·³è¿‡
    END IF
END FUNCTION

FUNCTION WRITE_UNIT_BY_TYPE(unit, expansion_level, parsed_data):
    """
    æ ¹æ®å•å…ƒç±»å‹å†™ä½œ
    """
    SWITCH unit.type:
        CASE "åŠ¨ä½œ":
            RETURN WRITE_ACTION(unit, expansion_level, parsed_data)
        CASE "å¯¹è¯":
            RETURN WRITE_DIALOGUE(unit, expansion_level, parsed_data)
        CASE "æƒ…ç»ª":
            RETURN WRITE_EMOTION(unit, expansion_level, parsed_data)
        CASE "æå†™":
            RETURN WRITE_DESCRIPTION(unit, expansion_level, parsed_data)
        CASE "å†…å¿ƒæˆ":
            IF expansion_level == "EXPAND":
                RETURN WRITE_INNER_MONOLOGUE(unit, parsed_data)
            ELSE:
                RETURN ""  # ä½é‡è¦åº¦çš„å†…å¿ƒæˆç›´æ¥è·³è¿‡
            END IF
    END SWITCH
END FUNCTION

FUNCTION EXECUTE_HUMANIZE_TASK(scene_text, task):
    """
    æ‰§è¡Œæ‹ŸäººåŒ–ä»»åŠ¡
    """
    SWITCH task.type:
        CASE "è·³è·ƒåˆ‡å…¥":
            # åˆ é™¤å‰3å¥
            RETURN REMOVE_FIRST_N_SENTENCES(scene_text, 3)
        
        CASE "æ•…æ„çœç•¥":
            # æ‰¾åˆ°ä¸€ä¸ªåŠ¨æœºæè¿°ï¼Œåˆ é™¤
            RETURN REMOVE_ONE_MOTIVATION(scene_text)
        
        CASE "ç¬”å¢¨å¤±è¡¡":
            # æ‰¾åˆ°ä¸€ä¸ªæ¬¡è¦å…ƒç´ ï¼Œå¤šå†™50å­—
            RETURN EXPAND_MINOR_ELEMENT(scene_text, 50)
        
        CASE "å¯¹è¯ä¸å›åº”":
            # å·²åœ¨å¯¹è¯å†™ä½œæ—¶å¤„ç†
            RETURN scene_text
        
        DEFAULT:
            RETURN scene_text
    END SWITCH
END FUNCTION
```

---

## ã€æ¨¡å—5ã€‘ANTI_DISEASE_PROTOCOL_V3 - é›†ä¸­åClaudeç—…ï¼ˆå…³é”®æ”¹è¿›ï¼‰

```python
FUNCTION APPLY_ALL_ANTI_DISEASE_RULES(chapter_content, parsed_data):
    """
    é›†ä¸­åº”ç”¨æ‰€æœ‰åClaudeç—…è§„åˆ™
    åœ¨æ¶¦è‰²é˜¶æ®µç»Ÿä¸€å¤„ç†ï¼Œè€Œéåˆ†æ•£åœ¨å†™ä½œè¿‡ç¨‹ä¸­
    """
    
    PRINT "[POLISH] å¼€å§‹é›†ä¸­åClaudeç—…æ¶¦è‰²..."
    
    polished = chapter_content
    
    # ========== ç—…ç—‡1ï¼šåˆ é™¤å£æ°´è¯ ==========
    polished = REMOVE_FILLER_WORDS(polished)
    # åˆ é™¤ï¼šä¼¼ä¹/ä»¿ä½›/æˆ–è®¸/å¯èƒ½/å¤§æ¦‚/å¥½åƒ
    
    # ========== ç—…ç—‡2ï¼šåˆ é™¤å¥—è·¯è½¬æŠ˜è¯ ==========
    polished = REMOVE_CLICHE_TRANSITIONS(polished)
    # åˆ é™¤ï¼šç„¶è€Œ/ä½†æ˜¯/å°±åœ¨è¿™æ—¶/çªç„¶/å¿½ç„¶
    
    # ========== ç—…ç—‡3ï¼šæ›¿æ¢æƒ…ç»ªè¯ä¸ºç”Ÿç†ååº” ==========
    polished = REPLACE_EMOTION_WORDS_WITH_PHYSIOLOGY(polished)
    # "ä»–æ„Ÿåˆ°ææƒ§" â†’ "ä»–çš„åèƒŒå‘å‡‰"
    
    # ========== ç—…ç—‡4ï¼šç®€åŒ–åŠ¨ä½œé“¾ ==========
    polished = SIMPLIFY_ACTION_CHAINS(polished)
    # "ç«™èµ·æ¥â†’èµ°åˆ°é—¨å£â†’æ¨é—¨â†’èµ°å‡ºå»" â†’ "ä»–æ¨é—¨å‡ºå»"
    
    # ========== ç—…ç—‡5ï¼šåˆ é™¤æ®µè½æ€»ç»“å¥ ==========
    polished = REMOVE_PARAGRAPH_SUMMARIES(polished)
    # åˆ é™¤"ä»–æ˜ç™½äº†""ä»–å†³å®šäº†""ä»–æ„è¯†åˆ°"ç»“å°¾
    
    # ========== ç—…ç—‡6ï¼šå‹ç¼©è¿‡é•¿æ®µè½ ==========
    polished = COMPRESS_LONG_PARAGRAPHS(polished, MAX_LENGTH=150)
    # å•æ®µ>150å­—çš„ï¼Œæå–æ ¸å¿ƒé‡å†™
    
    # ========== ç—…ç—‡7ï¼šå‡å°‘å†…å¿ƒæˆ ==========
    polished = REDUCE_INNER_MONOLOGUE(polished, MAX_RATIO=0.15)
    # å†…å¿ƒæˆ>15%çš„ï¼Œåˆ å‡åˆ°15%
    
    # ========== ç—…ç—‡8ï¼šåˆ é™¤å†—ä½™ ==========
    polished = REMOVE_REDUNDANCY(polished)
    # åŒä¸€æ„æ€è¯´2-3éçš„ï¼Œåªä¿ç•™1é
    
    # ========== ç—…ç—‡9ï¼šä¼˜åŒ–å¥é•¿ï¼ˆé•¿çŸ­å¥äº¤é”™ï¼‰==========
    polished = OPTIMIZE_SENTENCE_LENGTH(polished)
    # é¿å…è¿ç»­5å¥éƒ½æ˜¯20å­—ä»¥ä¸Šæˆ–10å­—ä»¥ä¸‹
    
    # ========== ç—…ç—‡10ï¼šåˆ é™¤è§£é‡Šæ€§å†…å¿ƒæˆ ==========
    polished = REMOVE_EXPLANATORY_THOUGHTS(polished)
    # "ä»–è¿™æ ·åšæ˜¯å› ä¸º..." â†’ ç›´æ¥åˆ é™¤ï¼Œè®©è¯»è€…è‡ªå·±æ¨æ–­
    
	
	
	
	
	
	# ========== ç—…ç—‡11ï¼šè¿‡åº¦å®‰å…¨ï¼ˆè¿½åŠ ï¼‰==========
    polished = REMOVE_HEDGING_LANGUAGE(polished)
    # åˆ é™¤è¿‡åº¦è°¨æ…çš„è¡¨è¾¾ï¼š"å¯ä»¥è¯´""åŸºæœ¬ä¸Š""åœ¨æŸç§æ„ä¹‰ä¸Š"
    
    # ========== ç—…ç—‡12ï¼šè¿‡åº¦å†—ä½™ï¼ˆè¿½åŠ ï¼‰==========
    polished = REMOVE_REPETITION(polished)
    # æ£€æµ‹åŒä¸€æ„æ€ç”¨ä¸åŒæ–¹å¼é‡å¤2-3éçš„æ®µè½ï¼Œåªä¿ç•™1é
    
    # ========== ç—…ç—‡13ï¼šè¿‡åº¦é“ºå«ï¼ˆè¿½åŠ ï¼‰==========
    polished = COMPRESS_SETUP(polished)
    # æ£€æµ‹äº‹ä»¶å‰è¶…è¿‡100å­—çš„é“ºå«ï¼Œå‹ç¼©åˆ°30-50å­—
    
    # ========== ç—…ç—‡14ï¼šä¸æ•¢è·³è·ƒï¼ˆè¿½åŠ ï¼‰==========
    polished = REMOVE_TRANSITIONS(polished)
    # åˆ é™¤"è¿‡äº†Xæ—¶é—´""ä»–èµ°åˆ°äº†Yåœ°"ç­‰è¿‡æ¸¡å¥
    
    # ========== ç—…ç—‡15ï¼šè¿‡åº¦å¹³å‡ï¼ˆè¿½åŠ ï¼‰==========
    # æ³¨ï¼šæ­¤é¡¹åœ¨å†™ä½œé˜¶æ®µå·²é€šè¿‡"ç¬”å¢¨å¤±è¡¡"ä»»åŠ¡å¤„ç†ï¼Œæ¶¦è‰²ä¸å¤„ç†
    
    # ========== ç—…ç—‡16ï¼šè¿‡åº¦é€»è¾‘ï¼ˆè¿½åŠ ï¼‰==========
    polished = REMOVE_CAUSALITY_EXPLANATION(polished)
    # åˆ é™¤"å› ä¸ºXæ‰€ä»¥Y""ä¹‹æ‰€ä»¥Xæ˜¯å› ä¸ºY"ç­‰è§£é‡Šæ€§è¿æ¥
    
    # ========== ç—…ç—‡17ï¼šè¿‡åº¦å®Œæ•´ï¼ˆè¿½åŠ ï¼‰==========
    polished = BREAK_SCENE_STRUCTURE(polished)
    # è¯†åˆ«"èµ·æ‰¿è½¬åˆ"å®Œæ•´çš„åœºæ™¯ï¼Œéšæœºåˆ é™¤"æ‰¿"æˆ–"è½¬"
    
    # ========== ç—…ç—‡18ï¼šè¿‡åº¦ç¤¼è²Œï¼ˆè¿½åŠ ï¼‰==========
    polished = ADD_CONVERSATIONAL_ROUGHNESS(polished)
    # åœ¨å¯¹è¯ä¸­æ·»åŠ çœç•¥ã€æ‰“æ–­ã€å•å­—å›åº”
    
    # ========== ç—…ç—‡19ï¼šè¿‡åº¦å¯¹ç§°ï¼ˆè¿½åŠ ï¼‰==========
    # æ³¨ï¼šæ­¤é¡¹åœ¨å†™ä½œé˜¶æ®µå·²é€šè¿‡"å¯¹è¯ä¸å›åº”"ä»»åŠ¡å¤„ç†ï¼Œæ¶¦è‰²ä¸å¤„ç†
    
    # ========== ç—…ç—‡20ï¼šä¿¡æ¯å¯†åº¦ä¸è¶³ï¼ˆè¿½åŠ ï¼‰==========
    polished = COMPRESS_LOW_DENSITY_PARAGRAPHS(polished)
    # æ£€æµ‹ä¿¡æ¯å¯†åº¦<0.005çš„æ®µè½ï¼Œå‹ç¼©æˆ–åˆ é™¤
    
    PRINT "[POLISH] åClaudeç—…æ¶¦è‰²å®Œæˆï¼ˆ20æ¡è§„åˆ™å·²åº”ç”¨ï¼‰"
    
    RETURN polished
END FUNCTION
	
	
	
	


# ==================== å…·ä½“å®ç°å‡½æ•°ï¼ˆç¤ºä¾‹ï¼‰====================

FUNCTION REMOVE_FILLER_WORDS(text):
    """åˆ é™¤å£æ°´è¯"""
    filler_words = ["ä¼¼ä¹", "ä»¿ä½›", "æˆ–è®¸", "å¯èƒ½", "å¤§æ¦‚", "å¥½åƒ", "æŸç§ç¨‹åº¦ä¸Š"]
    FOR word IN filler_words:
        text = REPLACE(text, word, "")
    END FOR
    RETURN CLEAN_WHITESPACE(text)
END FUNCTION

FUNCTION REPLACE_EMOTION_WORDS_WITH_PHYSIOLOGY(text):
    """æ›¿æ¢æƒ…ç»ªè¯ä¸ºç”Ÿç†ååº”"""
    emotion_map = {
        "éœ‡æƒŠ": ["ç³å­”éª¤ç„¶æ”¶ç¼©", "å‘¼å¸ä¸€æ»", "åƒµåœ¨åŸåœ°"],
        "ææƒ§": ["åèƒŒå‘å‡‰", "è…¿å‘è½¯", "å†·æ±—", "å¿ƒè·³éª¤åœ"],
        "æ„¤æ€’": ["å¤ªé˜³ç©´è·³åŠ¨", "æ‹³å¤´æ”¥ç´§", "è„¸é¢Šå‘çƒ«"],
        "ç„¦è™‘": ["èƒƒéƒ¨æ”¶ç´§", "æ‰‹å¿ƒå†’æ±—", "å‘¼å¸å˜æµ…"],
        "ç´§å¼ ": ["å–‰å’™å‘å¹²", "å¿ƒè·³åŠ é€Ÿ", "è‚Œè‚‰ç´§ç»·"]
    }
    
    FOR emotion_word, reactions IN emotion_map.ITEMS():
        # æ‰¾åˆ°"ä»–æ„Ÿåˆ°XX"çš„æ¨¡å¼
        pattern = f"(ä»–|å¥¹).*æ„Ÿåˆ°{emotion_word}"
        matches = FIND_ALL(text, pattern)
        
        FOR match IN matches:
            # æ›¿æ¢ä¸ºéšæœºé€‰æ‹©çš„ç”Ÿç†ååº”
            reaction = RANDOM_CHOICE(reactions)
            subject = EXTRACT_SUBJECT(match)  # "ä»–"æˆ–"å¥¹"
            replacement = f"{subject}çš„{reaction}"
            text = REPLACE(text, match, replacement)
        END FOR
    END FOR
    
    RETURN text
END FUNCTION

FUNCTION SIMPLIFY_ACTION_CHAINS(text):
    """ç®€åŒ–åŠ¨ä½œé“¾"""
    # æ£€æµ‹åŠ¨ä½œé“¾æ¨¡å¼ï¼šè¿ç»­4ä¸ªä»¥ä¸ŠåŠ¨ä½œ
    pattern = r"(ä»–|å¥¹)(\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\w{2,5})"
    
    matches = FIND_ALL(text, pattern)
    
    FOR match IN matches:
        actions = EXTRACT_ACTIONS(match)  # æå–æ‰€æœ‰åŠ¨ä½œ
        key_action = SELECT_KEY_ACTION(actions)  # é€‰æ‹©å…³é”®åŠ¨ä½œ
        subject = EXTRACT_SUBJECT(match)  # "ä»–"æˆ–"å¥¹"
        
        simplified = f"{subject}{key_action}"
        text = REPLACE(text, match.full_text, simplified)
    END FOR
    
    RETURN text
END FUNCTION


# ==================== æ–°å¢ç—…ç—‡å¤„ç†å‡½æ•°ï¼ˆD11-D20ï¼‰====================

FUNCTION REMOVE_HEDGING_LANGUAGE(text):
    """
    D4/D11ï¼šåˆ é™¤è¿‡åº¦è°¨æ…çš„è¡¨è¾¾
    """
    hedging_phrases = [
        "å¯ä»¥è¯´", "åŸºæœ¬ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š", "ä»æŸç§è§’åº¦", 
        "ç›¸å¯¹è€Œè¨€", "æ€»çš„æ¥è¯´", "ä¸€å®šç¨‹åº¦ä¸Š"
    ]
    FOR phrase IN hedging_phrases:
        text = REPLACE(text, phrase, "")
    END FOR
    RETURN CLEAN_WHITESPACE(text)
END FUNCTION

FUNCTION REMOVE_REPETITION(text):
    """
    D5/D12ï¼šåˆ é™¤åŒä¸€æ„æ€çš„é‡å¤è¡¨è¾¾
    """
    paragraphs = SPLIT_PARAGRAPHS(text)
    
    FOR i, para IN ENUMERATE(paragraphs):
        sentences = SPLIT_SENTENCES(para)
        
        # æ£€æµ‹ç›¸é‚»3å¥æ˜¯å¦è¡¨è¾¾ç›¸åŒæ„æ€
        FOR j = 0 TO LENGTH(sentences) - 3:
            IF ARE_SEMANTICALLY_SIMILAR(sentences[j], sentences[j+1], sentences[j+2]):
                # åˆ é™¤åä¸¤å¥ï¼Œåªä¿ç•™ç¬¬ä¸€å¥
                sentences = DELETE(sentences, [j+1, j+2])
            END IF
        END FOR
        
        paragraphs[i] = JOIN(sentences, "")
    END FOR
    
    RETURN JOIN(paragraphs, "\n\n")
END FUNCTION

FUNCTION COMPRESS_SETUP(text):
    """
    D14ï¼šå‹ç¼©è¿‡åº¦é“ºå«
    """
    paragraphs = SPLIT_PARAGRAPHS(text)
    
    FOR i = 0 TO LENGTH(paragraphs) - 2:
        current = paragraphs[i]
        next = paragraphs[i+1]
        
        # æ£€æµ‹é“ºå«æ¨¡å¼ï¼šå½“å‰æ®µè½>100å­— + ä¸‹ä¸€æ®µè½åŒ…å«åŠ¨ä½œè¯
        IF LENGTH(current) > 100 AND CONTAINS_ACTION_TRIGGER(next):
            # å‹ç¼©å½“å‰æ®µè½åˆ°30-50å­—
            core = EXTRACT_CORE_INFO(current)
            paragraphs[i] = REWRITE_BRIEFLY(core, MAX_LENGTH=50)
        END IF
    END FOR
    
    RETURN JOIN(paragraphs, "\n\n")
END FUNCTION

FUNCTION REMOVE_TRANSITIONS(text):
    """
    D17ï¼šåˆ é™¤è¿‡æ¸¡å¥
    """
    transition_patterns = [
        r"è¿‡äº†.{1,5}(æ—¶é—´|åˆ†é’Ÿ|å°æ—¶|å¤©)",
        r"(ä»–|å¥¹).{0,5}(èµ°åˆ°|æ¥åˆ°|åˆ°äº†).{2,10}",
        r"ç‰‡åˆ»ä¹‹å",
        r"ä¸å¤šæ—¶",
        r"é¡»è‡¾"
    ]
    
    FOR pattern IN transition_patterns:
        matches = FIND_ALL(text, pattern)
        FOR match IN matches:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹çš„è¿‡æ¸¡å¥ï¼ˆå‰åéƒ½æœ‰æ¢è¡Œï¼‰
            IF IS_STANDALONE_SENTENCE(text, match):
                text = REPLACE(text, match.full_sentence, "")
            END IF
        END FOR
    END FOR
    
    RETURN CLEAN_WHITESPACE(text)
END FUNCTION

FUNCTION REMOVE_CAUSALITY_EXPLANATION(text):
    """
    D7/D16ï¼šåˆ é™¤å› æœè§£é‡Š
    """
    causality_patterns = [
        r"å› ä¸º.{5,30}æ‰€ä»¥",
        r"ä¹‹æ‰€ä»¥.{5,30}æ˜¯å› ä¸º",
        r"è¿™(æ˜¯|æ ·åš)(æ˜¯)å› ä¸º",
        r"æ­£(æ˜¯|)ç”±äº"
    ]
    
    FOR pattern IN causality_patterns:
        matches = FIND_ALL(text, pattern)
        FOR match IN matches:
            # ä¿ç•™"æ‰€ä»¥"åé¢çš„å†…å®¹ï¼Œåˆ é™¤"å› ä¸º"éƒ¨åˆ†
            result_part = EXTRACT_RESULT_CLAUSE(match)
            text = REPLACE(text, match.full_text, result_part)
        END FOR
    END FOR
    
    RETURN text
END FUNCTION

FUNCTION BREAK_SCENE_STRUCTURE(text):
    """
    D8/D17ï¼šæ‰“ç ´åœºæ™¯å®Œæ•´ç»“æ„
    """
    scenes = DETECT_SCENES(text)
    
    FOR scene IN scenes:
        structure = ANALYZE_SCENE_STRUCTURE(scene)
        
        # å¦‚æœåœºæ™¯æœ‰å®Œæ•´çš„"èµ·æ‰¿è½¬åˆ"
        IF structure.has_all_four_parts:
            # éšæœºåˆ é™¤"æ‰¿"æˆ–"è½¬"ï¼ˆ30%æ¦‚ç‡ï¼‰
            IF RANDOM() < 0.3:
                part_to_remove = RANDOM_CHOICE(["æ‰¿", "è½¬"])
                scene.content = REMOVE_PART(scene.content, part_to_remove)
                text = REPLACE_SCENE(text, scene)
            END IF
        END IF
    END FOR
    
    RETURN text
END FUNCTION

FUNCTION ADD_CONVERSATIONAL_ROUGHNESS(text):
    """
    D9/D18ï¼šå¢åŠ å¯¹è¯çš„"ç²—ç³™æ„Ÿ"
    """
    dialogues = EXTRACT_ALL_DIALOGUES(text)
    
    FOR dialogue IN dialogues:
        # 30%æ¦‚ç‡åšä»¥ä¸‹å¤„ç†ä¹‹ä¸€
        IF RANDOM() < 0.3:
            modification = RANDOM_CHOICE([
                "TRUNCATE",      # è¯´åŠå¥è¯
                "SINGLE_CHAR",   # æ”¹æˆ"å—¯""å•Š""å“¦"
                "ADD_PAUSE"      # æ·»åŠ "â€¦â€¦"
            ])
            
            SWITCH modification:
                CASE "TRUNCATE":
                    # "ä½ è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ" â†’ "ä½ è¿™æ˜¯ä»€ä¹ˆâ€”â€”"
                    dialogue.content = TRUNCATE_AT_RANDOM(dialogue.content)
                
                CASE "SINGLE_CHAR":
                    # å®Œæ•´å›ç­” â†’ "å—¯"
                    IF LENGTH(dialogue.content) > 10 AND IS_RESPONSE(dialogue):
                        dialogue.content = RANDOM_CHOICE(["å—¯", "å•Š", "å“¦", "å‘µ"])
                    END IF
                
                CASE "ADD_PAUSE":
                    # æ·»åŠ çŠ¹è±«åœé¡¿
                    dialogue.content = INSERT_PAUSE(dialogue.content)
            END SWITCH
            
            text = REPLACE_DIALOGUE(text, dialogue)
        END IF
    END FOR
    
    RETURN text
END FUNCTION

FUNCTION COMPRESS_LOW_DENSITY_PARAGRAPHS(text):
    """
    D11/D20ï¼šå‹ç¼©ä½ä¿¡æ¯å¯†åº¦æ®µè½
    """
    paragraphs = SPLIT_PARAGRAPHS(text)
    
    FOR i, para IN ENUMERATE(paragraphs):
        density = CALCULATE_INFO_DENSITY(para)
        
        # ä¿¡æ¯å¯†åº¦<0.005ï¼ˆæ¯200å­—ä¸åˆ°1ä¸ªä¿¡æ¯ç‚¹ï¼‰
        IF density < 0.005 AND LENGTH(para) > 100:
            # æå–æ ¸å¿ƒä¿¡æ¯ï¼Œé‡å†™ä¸ºç®€çŸ­ç‰ˆæœ¬
            core = EXTRACT_CORE_INFO(para)
            
            IF LENGTH(core) < 20:
                # æ ¸å¿ƒä¿¡æ¯å¤ªå°‘ï¼Œç›´æ¥åˆ é™¤
                paragraphs[i] = ""
            ELSE:
                # å‹ç¼©åˆ°30-50å­—
                paragraphs[i] = REWRITE_BRIEFLY(core, MAX_LENGTH=50)
            END IF
        END IF
    END FOR
    
    RETURN JOIN(FILTER(paragraphs, NOT_EMPTY), "\n\n")
END FUNCTION

# ==================== è¾…åŠ©æ£€æµ‹å‡½æ•° ====================

FUNCTION ARE_SEMANTICALLY_SIMILAR(sent1, sent2, sent3):
    """åˆ¤æ–­3å¥è¯æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # æå–å…³é”®è¯ï¼Œè®¡ç®—é‡å åº¦
    keywords1 = EXTRACT_KEYWORDS(sent1)
    keywords2 = EXTRACT_KEYWORDS(sent2)
    keywords3 = EXTRACT_KEYWORDS(sent3)
    
    overlap_12 = LENGTH(INTERSECT(keywords1, keywords2)) / LENGTH(UNION(keywords1, keywords2))
    overlap_23 = LENGTH(INTERSECT(keywords2, keywords3)) / LENGTH(UNION(keywords2, keywords3))
    
    RETURN overlap_12 > 0.6 AND overlap_23 > 0.6
END FUNCTION

FUNCTION CONTAINS_ACTION_TRIGGER(text):
    """æ£€æµ‹æ˜¯å¦åŒ…å«åŠ¨ä½œè§¦å‘è¯"""
    action_triggers = ["çªç„¶", "è¿™æ—¶", "å°±åœ¨", "åªè§", "å¿½ç„¶", "çŒ›åœ°"]
    RETURN ANY(trigger IN text FOR trigger IN action_triggers)
END FUNCTION

FUNCTION IS_STANDALONE_SENTENCE(text, match):
    """åˆ¤æ–­åŒ¹é…æ˜¯å¦æ˜¯ç‹¬ç«‹å¥å­"""
    start = match.start_pos
    end = match.end_pos
    
    # æ£€æŸ¥å‰åæ˜¯å¦æœ‰å¥å·/æ¢è¡Œ
    before_char = text[start-1] IF start > 0 ELSE "\n"
    after_char = text[end] IF end < LENGTH(text) ELSE "\n"
    
    RETURN before_char IN ["ã€‚", "\n"] AND after_char IN ["ã€‚", "\n"]
END FUNCTION

FUNCTION CALCULATE_INFO_DENSITY(paragraph):
    """è®¡ç®—æ®µè½ä¿¡æ¯å¯†åº¦"""
    info_count = COUNT_NEW_INFO(paragraph)
    char_count = LENGTH(paragraph)
    RETURN info_count / char_count
END FUNCTION




```

---

## ã€æ¨¡å—6ã€‘DIAGNOSE_CHAPTER_V3 - å…¨å±€è¯Šæ–­ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

```python
FUNCTION DIAGNOSE_CHAPTER_V3(chapter_content, parsed_data, monitors):
    """
    å…¨å±€è¯Šæ–­ï¼ˆv3ä¼˜åŒ–ç‰ˆï¼‰
    è¾“å‡ºåˆ†å±‚æŠ¥å‘Šï¼Œå¿«é€Ÿå®šä½é—®é¢˜
    """
    
    diagnosis = {
        "passed": TRUE,
        "summary": {},      # å¿«é€Ÿæ‘˜è¦
        "critical": [],     # å¿…é¡»ä¿®å¤
        "warnings": [],     # å»ºè®®ä¼˜åŒ–
        "details": {}       # è¯¦ç»†æ•°æ®
    }
    
    # ========== è®¡ç®—ç»Ÿè®¡æ•°æ® ==========
    stats = {
        "word_count": LENGTH(chapter_content),
        "dialogue_ratio": CALCULATE_DIALOGUE_RATIO(chapter_content),
        "inner_ratio": CALCULATE_INNER_MONOLOGUE_RATIO(chapter_content),
        "paragraph_count": COUNT_PARAGRAPHS(chapter_content),
        "avg_para_length": AVG_PARAGRAPH_LENGTH(chapter_content),
        "info_density": CALCULATE_INFO_DENSITY(chapter_content),
        "disease_count": COUNT_CLAUDE_DISEASES_V3(chapter_content),
        "scene_checks": monitors.scene_checks  # å®æ—¶æ£€æŸ¥ç»“æœ
    }
    
    # ========== å¿«é€Ÿæ‘˜è¦ ==========
    diagnosis.summary = {
        "status": "âœ… é€šè¿‡" IF diagnosis.passed ELSE "âŒ æœªé€šè¿‡",
        "word_count": f"{stats.word_count}å­—",
        "quality_score": CALCULATE_QUALITY_SCORE(stats)  # 0-100
    }
    
    # ========== å…³é”®æ£€æŸ¥ ==========
    
    # æ£€æŸ¥1ï¼šå­—æ•°èŒƒå›´
    IF stats.word_count < WORD_COUNT_MIN:
        diagnosis.critical.APPEND({
            "issue": f"å­—æ•°ä¸è¶³ï¼ˆ{stats.word_count}å­—ï¼Œæœ€ä½{WORD_COUNT_MIN}å­—ï¼‰",
            "fix": "éœ€è¦æ‰©å……å…³é”®æƒ…èŠ‚æˆ–å¢åŠ å†²çª"
        })
        diagnosis.passed = FALSE
    END IF
    
    # æ£€æŸ¥2ï¼šæ ¸å¿ƒä»»åŠ¡
    core_mission = parsed_data.goals.core_mission
    IF NOT CHECK_MISSION_COMPLETED(chapter_content, core_mission):
        diagnosis.critical.APPEND({
            "issue": f"æ ¸å¿ƒä»»åŠ¡æœªå®Œæˆï¼š{core_mission}",
            "fix": "å¿…é¡»é‡å†™æˆ–è¡¥å……å…³é”®æƒ…èŠ‚"
        })
        diagnosis.passed = FALSE
    END IF
    
    # æ£€æŸ¥3ï¼šçº¢çº¿è¿å
    FOR redline IN parsed_data.constraints.redlines:
        IF CHECK_REDLINE_VIOLATION(chapter_content, redline):
            diagnosis.critical.APPEND({
                "issue": f"è¿åçº¢çº¿ï¼š{redline}",
                "fix": "å¿…é¡»åˆ é™¤è¿è§„å†…å®¹"
            })
            diagnosis.passed = FALSE
        END IF
    END FOR
    
    # æ£€æŸ¥4ï¼šå¿…é¡»åŸ‹çš„é’©å­
    FOR hook IN parsed_data.goals.hooks_to_plant:
        IF NOT CHECK_HOOK_PLANTED(chapter_content, hook):
            diagnosis.critical.APPEND({
                "issue": f"é’©å­æœªåŸ‹ï¼š{hook}",
                "fix": "éœ€è¦è¡¥å……ä¼ç¬”"
            })
            # é’©å­æœªåŸ‹ä¸ç®—CRITICALï¼Œä½†è¦æ ‡è®°
        END IF
    END FOR
    
    # ========== è­¦å‘Šé¡¹ ==========
    
    # è­¦å‘Š1ï¼šå¯¹è¯å æ¯”
    dialogue_ratio = stats.dialogue_ratio
    target_range = TOMATO_CORE_RULES.dialogue_ratio
    IF dialogue_ratio < target_range[0] OR dialogue_ratio > target_range[1]:
        diagnosis.warnings.APPEND({
            "issue": f"å¯¹è¯å æ¯”åç¦»ç›®æ ‡ï¼ˆå½“å‰{dialogue_ratio*100:.1f}%ï¼Œç›®æ ‡{target_range[0]*100}-{target_range[1]*100}%ï¼‰",
            "fix": "è°ƒæ•´å¯¹è¯/æå†™æ¯”ä¾‹"
        })
    END IF
    
    # è­¦å‘Š2ï¼šClaudeç—…ç—‡
    IF stats.disease_count > 5:
        diagnosis.warnings.APPEND({
            "issue": f"æ£€å‡º{stats.disease_count}ä¸ªClaudeç—…ç—‡",
            "fix": "å·²è‡ªåŠ¨æ¶¦è‰²ï¼Œä½†ä»éœ€äººå·¥æ£€æŸ¥"
        })
    END IF
    
    # è­¦å‘Š3ï¼šä¿¡æ¯å¯†åº¦
    IF stats.info_density < TOMATO_CORE_RULES.info_density_min:
        diagnosis.warnings.APPEND({
            "issue": f"ä¿¡æ¯å¯†åº¦è¿‡ä½ï¼ˆ{stats.info_density:.3f}ï¼‰",
            "fix": "å‹ç¼©æå†™æˆ–å¢åŠ ä¿¡æ¯ç‚¹"
        })
    END IF
    
    # ========== è¯¦ç»†æ•°æ® ==========
    diagnosis.details = stats
    
    RETURN diagnosis
END FUNCTION

FUNCTION CALCULATE_QUALITY_SCORE(stats):
    """
    è®¡ç®—ç»¼åˆè´¨é‡åˆ†ï¼ˆ0-100ï¼‰
    """
    score = 100
    
    # å­—æ•°ï¼šåœ¨ç›®æ ‡èŒƒå›´å†…+20ï¼Œè¶…å‡º-20
    IF stats.word_count < WORD_COUNT_MIN OR stats.word_count > WORD_COUNT_MAX:
        score -= 20
    ELSE IF ABS(stats.word_count - WORD_COUNT_TARGET) < 500:
        score += 10
    END IF
    
    # å¯¹è¯å æ¯”ï¼šåœ¨ç›®æ ‡èŒƒå›´å†…+20
    IF stats.dialogue_ratio >= 0.35 AND stats.dialogue_ratio <= 0.45:
        score += 15
    ELSE:
        score -= 10
    END IF
    
    # ä¿¡æ¯å¯†åº¦ï¼šè¾¾æ ‡+15
    IF stats.info_density >= TOMATO_CORE_RULES.info_density_min:
        score += 15
    ELSE:
        score -= 10
    END IF
    
    # Claudeç—…ç—‡ï¼šæ¯ä¸ª-3åˆ†
    score -= stats.disease_count * 3
    
    # æ®µè½é•¿åº¦ï¼šå¹³å‡<150å­—+10åˆ†
    IF stats.avg_para_length <= 150:
        score += 10
    END IF
    
    RETURN CLAMP(score, 0, 100)
END FUNCTION
```

---

## ã€æ¨¡å—7ã€‘PROBLEM_ANALYSIS - é—®é¢˜åˆ†æï¼ˆæ–°å¢ï¼‰

```python
FUNCTION ANALYZE_WHAT_WENT_WRONG(diagnosis, parsed_data):
    """
    åˆ†æè¯Šæ–­å¤±è´¥çš„æ ¹æœ¬åŸå› 
    é¿å…ç›²ç›®é‡å†™
    """
    
    analysis = {
        "root_cause": "",
        "affected_scenes": [],
        "fix_strategy": ""
    }
    
    # ========== åˆ†æCriticalé—®é¢˜ ==========
    FOR issue IN diagnosis.critical:
        IF "å­—æ•°ä¸è¶³" IN issue.issue:
            analysis.root_cause = "å†…å®¹é‡ä¸è¶³"
            analysis.fix_strategy = "å±•å¼€é‡è¦æƒ…èŠ‚ï¼Œå¢åŠ å†²çªå¼ºåº¦"
            
        ELSE IF "æ ¸å¿ƒä»»åŠ¡æœªå®Œæˆ" IN issue.issue:
            analysis.root_cause = "é—æ¼å…³é”®æƒ…èŠ‚"
            analysis.fix_strategy = "è¡¥å……ä»»åŠ¡ç›¸å…³çš„åœºæ™¯"
            
        ELSE IF "è¿åçº¢çº¿" IN issue.issue:
            # ä»ç« èŠ‚å†…å®¹ä¸­æ‰¾åˆ°è¿è§„ä½ç½®
            violation_location = FIND_VIOLATION_LOCATION(chapter_content, issue)
            analysis.affected_scenes = [violation_location.scene_idx]
            analysis.root_cause = "é€»è¾‘é”™è¯¯æˆ–ç†è§£åå·®"
            analysis.fix_strategy = f"åˆ é™¤åœºæ™¯{violation_location.scene_idx}çš„è¿è§„å†…å®¹"
            
        ELSE IF "é’©å­æœªåŸ‹" IN issue.issue:
            analysis.root_cause = "é—æ¼ä¼ç¬”"
            analysis.fix_strategy = "åœ¨é€‚å½“åœºæ™¯è¡¥å……ä¼ç¬”"
        END IF
    END FOR
    
    # ========== åˆ†æWarningï¼ˆè¾…åŠ©åˆ¤æ–­ï¼‰==========
    IF LENGTH(diagnosis.warnings) > 3:
        analysis.root_cause += " + é£æ ¼åç¦»"
        analysis.fix_strategy += " + è°ƒæ•´èŠ‚å¥å’Œæ¯”ä¾‹"
    END IF
    
    PRINT "[ANALYSIS] é—®é¢˜æ ¹æºï¼š{analysis.root_cause}"
    PRINT "[ANALYSIS] ä¿®å¤ç­–ç•¥ï¼š{analysis.fix_strategy}"
    
    RETURN analysis
END FUNCTION

FUNCTION GENERATE_FIX_INSTRUCTION(problem_analysis):
    """
    æ ¹æ®é—®é¢˜åˆ†æç”Ÿæˆä¿®æ­£æŒ‡ä»¤
    """
    
    fix_instruction = {
        "focus_scenes": problem_analysis.affected_scenes,
        "must_do": [],
        "must_avoid": []
    }
    
    IF "å†…å®¹é‡ä¸è¶³" IN problem_analysis.root_cause:
        fix_instruction.must_do.APPEND("å±•å¼€é‡è¦æƒ…èŠ‚è‡³200-300å­—")
        fix_instruction.must_do.APPEND("å¢åŠ 1-2ä¸ªå†²çªåœºæ™¯")
    END IF
    
    IF "é—æ¼å…³é”®æƒ…èŠ‚" IN problem_analysis.root_cause:
        fix_instruction.must_do.APPEND("è¡¥å……æ ¸å¿ƒä»»åŠ¡ç›¸å…³åœºæ™¯")
    END IF
    
    IF "é€»è¾‘é”™è¯¯" IN problem_analysis.root_cause:
        fix_instruction.must_avoid.APPEND("é¿å…è¿åçº¢çº¿çš„å†…å®¹")
        fix_instruction.focus_scenes = problem_analysis.affected_scenes
    END IF
    
    IF "é£æ ¼åç¦»" IN problem_analysis.root_cause:
        fix_instruction.must_do.APPEND("å¢åŠ å¯¹è¯å æ¯”è‡³35%-45%")
        fix_instruction.must_do.APPEND("æ§åˆ¶æ®µè½é•¿åº¦<150å­—")
    END IF
    
    RETURN fix_instruction
END FUNCTION

FUNCTION MAIN_EXECUTION_WITH_FIX(CAPSULE, fix_instruction, humanizer):
    """
    å¸¦ä¿®æ­£æŒ‡ä»¤çš„é‡å†™
    """
    
    # åœ¨humanizerä¸­æ³¨å…¥ä¿®æ­£æŒ‡ä»¤
    humanizer.fix_instruction = fix_instruction
    
    # é‡æ–°æ‰§è¡Œä¸»æµç¨‹
    RETURN MAIN_EXECUTION(CAPSULE)
    
    # æ³¨ï¼šå†™ä½œæ—¶ä¼šæ£€æŸ¥humanizer.fix_instructionï¼Œ
    # é’ˆå¯¹focus_scenesé‡ç‚¹ä¿®æ­£
END FUNCTION
```

---

## ã€æ¨¡å—8ã€‘EMOTION_WRITER_V3 - æƒ…ç»ªå†™ä½œå™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
FUNCTION WRITE_EMOTION(unit, expansion_level, parsed_data):
    """
    å†™æƒ…ç»ªå•å…ƒï¼ˆv3ç®€åŒ–ç‰ˆï¼‰
    æ ¹æ®expansion_levelåŠ¨æ€Show/Tell
    """
    
    emotion_text = ""
    emotion_type = unit.emotion_type  # å¦‚ï¼šç„¦è™‘
    emotion_intensity = unit.intensity  # 0-100
    
    # ========== æ ¹æ®expansion_levelå†³å®šShow/Tell ==========
    IF expansion_level == "EXPAND":
        # è¯¦å†™ï¼šç”Ÿç†ååº” + ç®€çŸ­å†…å¿ƒæˆ
        micro_expressions = GENERATE_MICRO_EXPRESSIONS(emotion_type, emotion_intensity)
        emotion_text = WRITE_MICRO_EXPRESSIONS(micro_expressions)
        
        # åŠ ä¸€å¥å†…å¿ƒæˆï¼ˆ<30å­—ï¼‰
        IF parsed_data.emotions.protagonist.active_thought:
            inner = parsed_data.emotions.protagonist.active_thought
            emotion_text += f""{inner[:30]}""  # é™åˆ¶30å­—
        END IF
        
    ELSE IF expansion_level == "BRIEF":
        # ç®€å†™ï¼šåªå†™ç”Ÿç†ååº”
        micro_expressions = GENERATE_MICRO_EXPRESSIONS(emotion_type, emotion_intensity)
        emotion_text = WRITE_MICRO_EXPRESSIONS(micro_expressions[:2])  # åªå–2ä¸ª
        
    ELSE:  # SKIP
        # è·³è¿‡ï¼šä¸å†™æƒ…ç»ª
        emotion_text = ""
    END IF
    
    RETURN emotion_text
END FUNCTION

FUNCTION GENERATE_MICRO_EXPRESSIONS(emotion_type, intensity):
    """ç”Ÿæˆç”Ÿç†ååº”"""
    emotion_map = {
        "ç„¦è™‘": ["èƒƒéƒ¨æ”¶ç´§", "å‘¼å¸å˜æµ…", "æ‰‹å¿ƒå†’æ±—"],
        "ææƒ§": ["åèƒŒå‘å‡‰", "è…¿å‘è½¯", "ç³å­”æ”¶ç¼©"],
        "æ„¤æ€’": ["å¤ªé˜³ç©´è·³åŠ¨", "æ‹³å¤´æ”¥ç´§", "è„¸é¢Šå‘çƒ«"],
        "å…´å¥‹": ["å¿ƒè·³åŠ å¿«", "å‘¼å¸æ€¥ä¿ƒ", "çœ¼ç›å‘äº®"]
    }
    
    reactions = emotion_map.GET(emotion_type, ["èº«ä½“ç´§ç»·"])
    
    # æ ¹æ®å¼ºåº¦é€‰æ‹©æ•°é‡
    IF intensity > 70:
        RETURN reactions  # å…¨éƒ¨
    ELSE IF intensity > 40:
        RETURN reactions[:2]  # å‰2ä¸ª
    ELSE:
        RETURN reactions[:1]  # å‰1ä¸ª
    END IF
END FUNCTION
```

---

## ã€æ¨¡å—9ã€‘DELIVERY_V3 - äº¤ä»˜åè®®ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

```python
FUNCTION DELIVER_OUTPUT_V3(chapter_content, new_facts, diagnosis, monitors):
    """
    äº¤ä»˜è¾“å‡ºï¼ˆv3åˆ†å±‚ç‰ˆï¼‰
    """
    
    output = {}
    
    # ========== è¾“å‡º1ï¼šç« èŠ‚æ­£æ–‡ ==========
    output["chapter_content"] = chapter_content
    
    # ========== è¾“å‡º2ï¼šå¿«é€Ÿæ‘˜è¦ ==========
    output["summary"] = f"""
## å¿«é€Ÿæ‘˜è¦
{diagnosis.summary.status} | å­—æ•°: {diagnosis.summary.word_count} | è´¨é‡åˆ†: {diagnosis.summary.quality_score}/100
"""
    
    # ========== è¾“å‡º3ï¼šé—®é¢˜æ¸…å•ï¼ˆå¦‚æœæœ‰ï¼‰==========
    IF LENGTH(diagnosis.critical) > 0:
        output["summary"] += "\n### âš ï¸ å¿…é¡»ä¿®å¤\n"
        FOR issue IN diagnosis.critical:
            output["summary"] += f"- {issue.issue}\n"
        END FOR
    END IF
    
    IF LENGTH(diagnosis.warnings) > 0:
        output["summary"] += "\n### ğŸ’¡ å»ºè®®ä¼˜åŒ–\n"
        FOR warning IN diagnosis.warnings:
            output["summary"] += f"- {warning.issue}\n"
        END FOR
    END IF
    
    # ========== è¾“å‡º4ï¼šæ–°å¢Factæ¸…å• ==========
    output["new_facts"] = FORMAT_FACTS_LIST(new_facts)
    
    # ========== è¾“å‡º5ï¼šè¯¦ç»†è¯Šæ–­ï¼ˆå¯æŠ˜å ï¼‰==========
    output["details"] = f"""
<details>
<summary>ğŸ“Š è¯¦ç»†ç»Ÿè®¡ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

- æ€»å­—æ•°: {diagnosis.details.word_count}
- å¯¹è¯å æ¯”: {diagnosis.details.dialogue_ratio * 100:.1f}%
- å†…å¿ƒæˆå æ¯”: {diagnosis.details.inner_ratio * 100:.1f}%
- æ®µè½æ•°: {diagnosis.details.paragraph_count}
- å¹³å‡æ®µè½é•¿åº¦: {diagnosis.details.avg_para_length:.0f}å­—
- ä¿¡æ¯å¯†åº¦: {diagnosis.details.info_density:.3f}
- Claudeç—…ç—‡æ£€å‡º: {diagnosis.details.disease_count}ä¸ª

### åœºæ™¯æ£€æŸ¥è®°å½•
"""
    FOR check IN monitors.scene_checks:
        status_icon = "âœ…" IF check.severity == "OK" ELSE "âš ï¸"
        output["details"] += f"- åœºæ™¯{check.scene_idx}: {status_icon} {check.word_count}å­—\n"
    END FOR
    
    output["details"] += "</details>"
    
    RETURN output
END FUNCTION
```

---

## ã€é™„å½•ã€‘è¾…åŠ©å‡½æ•°è¯´æ˜ï¼ˆç²¾ç®€ç‰ˆï¼‰

```python
# ==================== æ ¸å¿ƒè¾…åŠ©å‡½æ•° ====================

FUNCTION PARSE_CAPSULE(CAPSULE):
    """è§£æèƒ¶å›Šï¼Œæå–æ‰€æœ‰æ¨¡å—æ•°æ®"""
    # ä»Â§1-Â§19æå–ç»“æ„åŒ–æ•°æ®
    PASS
END FUNCTION

FUNCTION GET_SCENE_COUNT_FROM_CAPSULE(parsed_data):
    """
    ä»Capsuleæ¨æ–­åœºæ™¯æ•°é‡
    è§„åˆ™ï¼š3000-4000å­— â†’ 4-5ä¸ªåœºæ™¯
    """
    word_target = parsed_data.meta.word_count_target OR WORD_COUNT_TARGET
    RETURN ROUND(word_target / 800)  # å¹³å‡æ¯åœºæ™¯800å­—
END FUNCTION

FUNCTION CHECK_REDLINE_VIOLATION(text, redline):
    """æ£€æŸ¥æ˜¯å¦è¿åçº¢çº¿ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰"""
    keywords = EXTRACT_KEYWORDS(redline)
    IF ALL(keyword IN text FOR keyword IN keywords):
        RETURN SEMANTIC_CHECK(text, redline)  # è¿›ä¸€æ­¥è¯­ä¹‰éªŒè¯
    END IF
    RETURN FALSE
END FUNCTION

FUNCTION COUNT_NEW_INFO(text):
    """ç»Ÿè®¡æ–°ä¿¡æ¯ç‚¹æ•°é‡"""
    # ç®€åŒ–å®ç°ï¼šæ£€æµ‹å…³é”®è¯ã€å¯¹è¯ã€æ–°æ¦‚å¿µç­‰
    info_indicators = [
        COUNT_PROPER_NOUNS(text),           # ä¸“æœ‰åè¯
        COUNT_PLOT_VERBS(text),             # å‰§æƒ…åŠ¨è¯ï¼ˆå‘ç°/å¾—åˆ°/å¤±å»ï¼‰
        COUNT_DIALOGUE_EXCHANGES(text) / 2  # å¯¹è¯å›åˆ
    ]
    RETURN SUM(info_indicators)
END FUNCTION

FUNCTION CALCULATE_DIALOGUE_RATIO(text):
    """è®¡ç®—å¯¹è¯å æ¯”"""
    dialogue_chars = LENGTH(EXTRACT_ALL_DIALOGUES(text))
    total_chars = LENGTH(text)
    RETURN dialogue_chars / total_chars
END FUNCTION

FUNCTION CHECK_MISSION_COMPLETED(text, mission):
    """æ£€æŸ¥æ ¸å¿ƒä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰"""
    mission_keywords = EXTRACT_KEYWORDS(mission)
    IF ANY(keyword NOT IN text FOR keyword IN mission_keywords):
        RETURN FALSE
    END IF
    # è¿›ä¸€æ­¥è¯­ä¹‰æ£€æŸ¥
    RETURN SEMANTIC_VERIFY_MISSION(text, mission)
END FUNCTION

FUNCTION CHECK_HOOK_PLANTED(text, hook):
    """æ£€æŸ¥é’©å­æ˜¯å¦åŸ‹ä¸‹"""
    hook_keywords = EXTRACT_KEYWORDS(hook)
    RETURN ALL(keyword IN text FOR keyword IN hook_keywords)
END FUNCTION

FUNCTION WRITE_FORESHADOW(foreshadow, parsed_data):
    """
    å†™ä¼ç¬”ï¼ˆæ ¹æ®æ˜¾çœ¼åº¦ï¼‰
    æ˜¾çœ¼åº¦ç›´æ¥ä»CapsuleÂ§16è¯»å–
    """
    visibility = foreshadow.visibility
    content = foreshadow.content
    
    SWITCH visibility:
        CASE "æåº¦éšè”½":
            # æ··åœ¨5ä¸ªå…¶ä»–ç»†èŠ‚ä¸­ï¼Œä¸€ç¬”å¸¦è¿‡
            other_details = GENERATE_RANDOM_DETAILS(5, parsed_data)
            all_details = SHUFFLE([content] + other_details)
            RETURN JOIN(all_details, "ï¼Œ") + "ã€‚"
        
        CASE "éšè”½":
            # æ­£å¸¸æå†™ï¼Œä¸ç‰¹åˆ«å¼ºè°ƒ
            RETURN f"{content}ã€‚"
        
        CASE "å¾®æ˜¾çœ¼":
            # ç•¥å¾®å¤šå†™å‡ ä¸ªå­—
            detail = ADD_MINOR_DETAIL(content)
            RETURN f"{content}ï¼Œ{detail}ã€‚"
        
        CASE "æ˜¾çœ¼":
            # å•ç‹¬æˆå¥
            RETURN f"\n\n{content}ã€‚\n\n"
        
        CASE "æåº¦æ˜¾çœ¼":
            # å¼ºè°ƒ + ä¸»è§’æ³¨æ„åˆ°
            reaction = GENERATE_PROTAGONIST_REACTION(content, parsed_data)
            RETURN f"\n\n{content}ã€‚{reaction}\n\n"
    END SWITCH
END FUNCTION

FUNCTION SELECT_COOL_POINT_TYPE_FOR_SCENE(scene_idx, parsed_data):
    """ä¸ºåœºæ™¯é€‰æ‹©åˆé€‚çš„çˆ½ç‚¹ç±»å‹"""
    # ä»Capsuleè¯»å–åœºæ™¯ä¿¡æ¯
    scene_info = GET_SCENE_INFO(scene_idx, parsed_data)
    
    IF scene_info.has_conflict:
        RETURN RANDOM_CHOICE(["æ‰“è„¸çˆ½", "å¤ä»‡çˆ½"])
    ELSE IF scene_info.has_breakthrough:
        RETURN "å‡çº§çˆ½"
    ELSE IF scene_info.has_crisis:
        RETURN "åæ€çˆ½"
    ELSE:
        RETURN "è£…é€¼çˆ½"
    END IF
END FUNCTION

FUNCTION GENERATE_COOL_POINT(cool_type, parsed_data):
    """ç”Ÿæˆçˆ½ç‚¹æ–‡æœ¬"""
    cool_config = COOL_POINT_TYPES[cool_type]
    structure = cool_config.ç»“æ„
    
    # æ ¹æ®ç»“æ„æ‹†è§£æˆ3ä¸ªé˜¶æ®µ
    phases = SPLIT(structure, " â†’ ")
    
    cool_text = ""
    FOR phase IN phases:
        cool_text += WRITE_COOL_PHASE(phase, parsed_data) + "\n\n"
    END FOR
    
    RETURN cool_text
END FUNCTION

FUNCTION DECOMPOSE_SCENE_TO_UNITS(scene_idx, parsed_data):
    """
    å°†åœºæ™¯æ‹†è§£ä¸ºå†™ä½œå•å…ƒ
    ä»Capsuleæ¨æ–­åœºæ™¯åº”è¯¥åŒ…å«ä»€ä¹ˆ
    """
    units = []
    
    # åŸºç¡€å•å…ƒï¼šå¼€åœºç¯å¢ƒï¼ˆé‡è¦åº¦5ï¼‰
    units.APPEND({
        "type": "æå†™",
        "importance": 5,
        "content": GET_SCENE_ENVIRONMENT(scene_idx, parsed_data)
    })
    
    # ä»Â§3æ ¸å¿ƒä»»åŠ¡æ¨æ–­éœ€è¦çš„å•å…ƒ
    mission = parsed_data.goals.core_mission
    mission_units = INFER_UNITS_FROM_MISSION(mission, scene_idx)
    units.EXTEND(mission_units)
    
    # ä»Â§5æƒ…ç»ªæ•°æ®æ¨æ–­æƒ…ç»ªå•å…ƒ
    IF scene_idx == 1:  # å¼€ç¯‡åœºæ™¯
        units.APPEND({
            "type": "æƒ…ç»ª",
            "importance": 7,
            "emotion_type": parsed_data.emotions.start_emotion,
            "intensity": 60
        })
    END IF
    
    RETURN units
END FUNCTION



FUNCTION COUNT_CLAUDE_DISEASES_V3(text):
    """
    æ£€æµ‹Claudeç—…ç—‡ï¼ˆå®Œæ•´ç‰ˆï¼š20æ¡ï¼‰
    """
    disease_count = 0
    
    # ========== å‰10æ¡ï¼ˆv3.0å·²æœ‰ï¼‰==========
    
    # D12: å£æ°´è¯
    filler_words = ["ä¼¼ä¹", "ä»¿ä½›", "æˆ–è®¸", "å¯èƒ½", "å¤§æ¦‚", "å¥½åƒ"]
    FOR word IN filler_words:
        disease_count += COUNT_OCCURRENCES(text, word)
    END FOR
    
    # D13: å¥—è·¯è½¬æŠ˜
    cliche_words = ["ç„¶è€Œ", "ä½†æ˜¯", "å°±åœ¨è¿™æ—¶", "çªç„¶", "å¿½ç„¶"]
    FOR word IN cliche_words:
        disease_count += COUNT_OCCURRENCES(text, word)
    END FOR
    
    # D18: æƒ…ç»ªè¯ç›´å†™
    emotion_words = ["éœ‡æƒŠ", "ææƒ§", "æ„¤æ€’", "ç„¦è™‘"]
    FOR word IN emotion_words:
        disease_count += COUNT_OCCURRENCES(text, word)
    END FOR
    
    # D15: ç»“å°¾æ€»ç»“å¥
    paragraphs = SPLIT_PARAGRAPHS(text)
    FOR para IN paragraphs:
        last_sent = GET_LAST_SENTENCE(para)
        IF CONTAINS_ANY(last_sent, ["ä»–æ˜ç™½äº†", "ä»–å†³å®šäº†", "ä»–æ„è¯†åˆ°"]):
            disease_count += 1
        END IF
    END FOR
    
    # D19: åŠ¨ä½œé“¾
    action_chains = DETECT_ACTION_CHAINS(text)
    disease_count += COUNT(chain FOR chain IN action_chains IF LENGTH(chain) > 4)
    
    # D1: è¿‡åº¦å±•å¼€ï¼ˆæ£€æµ‹å•ä¸ªä¿¡æ¯ç‚¹å±•å¼€>150å­—ï¼‰
    disease_count += COUNT_OVER_EXPANDED_INFO(text, threshold=150)
    
    # D2: è¿‡åº¦è§£é‡Šï¼ˆæ£€æµ‹"è¿™æ˜¯å› ä¸º""ä¹‹æ‰€ä»¥"ç­‰ï¼‰
    explain_patterns = ["è¿™æ˜¯å› ä¸º", "ä¹‹æ‰€ä»¥", "åŸå› åœ¨äº"]
    FOR pattern IN explain_patterns:
        disease_count += COUNT_OCCURRENCES(text, pattern)
    END FOR
    
    # D3: è¿‡åº¦å†…å¿ƒæˆï¼ˆæ£€æµ‹å†…å¿ƒæˆå æ¯”>15%ï¼‰
    inner_ratio = CALCULATE_INNER_MONOLOGUE_RATIO(text)
    IF inner_ratio > 0.15:
        disease_count += ROUND((inner_ratio - 0.15) * 100)  # æ¯è¶…1%åŠ 1åˆ†
    END IF
    
    # D16: ä¸æ•¢ç•™ç™½ï¼ˆæ£€æµ‹è¿‡åº¦æš—ç¤ºï¼‰
    hint_patterns = ["ä¼¼ä¹é¢„ç¤ºç€", "å¯èƒ½ä¼š", "æˆ–è®¸æ„å‘³ç€"]
    FOR pattern IN hint_patterns:
        disease_count += COUNT_OCCURRENCES(text, pattern)
    END FOR
    
    # D20: è§†è§’æ¼‚ç§»ï¼ˆæ£€æµ‹"XXæƒ³""XXè§‰å¾—"å‡ºç°åœ¨éä¸»è§’èº«ä¸Šï¼‰
    disease_count += COUNT_POV_SHIFTS(text, parsed_data.characters.protagonist.name)
    
    # ========== å10æ¡ï¼ˆæ–°å¢æ£€æµ‹ï¼‰==========
    
    # D4: è¿‡åº¦å®‰å…¨
    hedging = ["å¯ä»¥è¯´", "åŸºæœ¬ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š", "æŸç§ç¨‹åº¦ä¸Š"]
    FOR phrase IN hedging:
        disease_count += COUNT_OCCURRENCES(text, phrase)
    END FOR
    
    # D5: è¿‡åº¦å†—ä½™
    disease_count += DETECT_REPETITION_COUNT(text)
    
    # D14: è¿‡åº¦é“ºå«
    disease_count += DETECT_OVER_SETUP_COUNT(text)
    
    # D17: ä¸æ•¢è·³è·ƒ
    transition_phrases = ["è¿‡äº†", "æ¥åˆ°", "èµ°åˆ°", "ç‰‡åˆ»ä¹‹å"]
    FOR phrase IN transition_phrases:
        disease_count += COUNT_OCCURRENCES(text, phrase)
    END FOR
    
    # D7: è¿‡åº¦é€»è¾‘
    causality = ["å› ä¸º", "æ‰€ä»¥", "ç”±äº", "å¯¼è‡´"]
    FOR word IN causality:
        disease_count += COUNT_OCCURRENCES(text, word) * 0.5  # å› æœè¯é€‚å½“æ‰“æŠ˜
    END FOR
    
    # D8: è¿‡åº¦å®Œæ•´
    disease_count += COUNT_COMPLETE_STRUCTURES(text)
    
    # D11: ä¿¡æ¯å¯†åº¦ä¸è¶³
    low_density_paras = FIND_LOW_DENSITY_PARAGRAPHS(text, threshold=0.005)
    disease_count += LENGTH(low_density_paras)
    
    # D6, D9, D10: åœ¨å†™ä½œé˜¶æ®µå·²é€šè¿‡æ‹ŸäººåŒ–ä»»åŠ¡å¤„ç†ï¼Œä¸é‡å¤æ£€æµ‹
    
    RETURN disease_count
END FUNCTION

# ==================== æ–°å¢æ£€æµ‹è¾…åŠ©å‡½æ•° ====================

FUNCTION COUNT_OVER_EXPANDED_INFO(text, threshold):
    """æ£€æµ‹è¿‡åº¦å±•å¼€çš„ä¿¡æ¯ç‚¹"""
    info_points = DETECT_INFO_POINTS(text)
    over_expanded = 0
    
    FOR point IN info_points:
        IF point.word_count > threshold:
            over_expanded += 1
        END IF
    END FOR
    
    RETURN over_expanded
END FUNCTION

FUNCTION COUNT_POV_SHIFTS(text, protagonist_name):
    """æ£€æµ‹è§†è§’æ¼‚ç§»"""
    shift_count = 0
    thought_patterns = [r"(\w+)(æƒ³|è§‰å¾—|è®¤ä¸º|å¿ƒæƒ³)", r"(\w+)çš„(å†…å¿ƒ|å¿ƒé‡Œ)"]
    
    FOR pattern IN thought_patterns:
        matches = FIND_ALL(text, pattern)
        FOR match IN matches:
            character_name = EXTRACT_CHARACTER(match)
            IF character_name != protagonist_name:
                shift_count += 1
            END IF
        END FOR
    END FOR
    
    RETURN shift_count
END FUNCTION

FUNCTION DETECT_REPETITION_COUNT(text):
    """æ£€æµ‹é‡å¤è¡¨è¾¾çš„æ¬¡æ•°"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    repetition_count = 0
    
    FOR para IN paragraphs:
        sentences = SPLIT_SENTENCES(para)
        FOR i = 0 TO LENGTH(sentences) - 3:
            IF ARE_SEMANTICALLY_SIMILAR(sentences[i], sentences[i+1], sentences[i+2]):
                repetition_count += 1
            END IF
        END FOR
    END FOR
    
    RETURN repetition_count
END FUNCTION

FUNCTION DETECT_OVER_SETUP_COUNT(text):
    """æ£€æµ‹è¿‡åº¦é“ºå«çš„æ¬¡æ•°"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    over_setup_count = 0
    
    FOR i = 0 TO LENGTH(paragraphs) - 2:
        IF LENGTH(paragraphs[i]) > 100 AND CONTAINS_ACTION_TRIGGER(paragraphs[i+1]):
            over_setup_count += 1
        END IF
    END FOR
    
    RETURN over_setup_count
END FUNCTION

FUNCTION COUNT_COMPLETE_STRUCTURES(text):
    """æ£€æµ‹å®Œæ•´çš„èµ·æ‰¿è½¬åˆç»“æ„"""
    scenes = DETECT_SCENES(text)
    complete_count = 0
    
    FOR scene IN scenes:
        structure = ANALYZE_SCENE_STRUCTURE(scene)
        IF structure.has_all_four_parts:
            complete_count += 1
        END IF
    END FOR
    
    RETURN complete_count
END FUNCTION

FUNCTION FIND_LOW_DENSITY_PARAGRAPHS(text, threshold):
    """æ‰¾å‡ºä½å¯†åº¦æ®µè½"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    low_density = []
    
    FOR para IN paragraphs:
        IF LENGTH(para) > 100:  # åªæ£€æŸ¥é•¿æ®µè½
            density = CALCULATE_INFO_DENSITY(para)
            IF density < threshold:
                low_density.APPEND(para)
            END IF
        END IF
    END FOR
    
    RETURN low_density
END FUNCTION

```

---

## ã€æ‰§è¡Œç¤ºä¾‹ã€‘

```python
# ==================== ç¤ºä¾‹æ‰§è¡Œæµç¨‹ ====================

# 1. è¯»å–èƒ¶å›Š
CAPSULE = READ_FILE("Capsule.md")

# 2. æ‰§è¡Œä¸»æµç¨‹
PRINT "==================== å¼€å§‹æ‰§è¡Œ ===================="
output = MAIN_EXECUTION(CAPSULE)

# 3. è¾“å‡ºç»“æœ

# 3.1 å¿«é€Ÿæ‘˜è¦
PRINT output.summary

# 3.2 ç« èŠ‚æ­£æ–‡
PRINT "\n==================== ç« èŠ‚æ­£æ–‡ ====================\n"
PRINT output.chapter_content

# 3.3 æ–°å¢Fact
PRINT "\n==================== æ–°å¢Fact ====================\n"
PRINT output.new_facts

# 3.4 è¯¦ç»†è¯Šæ–­ï¼ˆå¯é€‰ï¼‰
IF EXISTS(output, "details"):
    PRINT "\n"
    PRINT output.details
END IF

PRINT "\n==================== æ‰§è¡Œå®Œæˆ ===================="
```

---


### ğŸ“‹ æ‰§è¡Œè¦ç‚¹

1. **ä¸¥æ ¼æ‰§è¡Œä»»åŠ¡æ¸…å•**ï¼šClaudeå¿…é¡»å®Œæˆhumanizer.task_checklistä¸­çš„æ‰€æœ‰MUSTä»»åŠ¡

2. **å­—æ•°é¢„ç®—ç®¡ç†**ï¼šæ¯ä¸ªåœºæ™¯æœ‰æ˜ç¡®å­—æ•°é¢„ç®—ï¼Œä¸è¶…æ ‡

3. **é›†ä¸­æ¶¦è‰²**ï¼šå†™ä½œæ—¶ä¸å¿…æ‹…å¿ƒç—…ç—‡ï¼Œæ¶¦è‰²é˜¶æ®µç»Ÿä¸€å¤„ç†

4. **å®æ—¶æ£€æŸ¥**ï¼šä»…æ£€æŸ¥çº¢çº¿è¿åå’Œä¸¥é‡è¶…æ ‡ï¼Œå…¶ä»–é—®é¢˜ç•™ç»™å…¨å±€è¯Šæ–­

5. **é—®é¢˜åˆ†æ**ï¼šé‡å†™å‰å¿…é¡»å…ˆåˆ†æé—®é¢˜æ ¹æºï¼Œé¿å…ç›²ç›®é‡å†™

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **Capsuleä¼˜å…ˆ**ï¼šå¦‚æœCapsuleä¸­æ˜ç¡®æŒ‡å®šäº†æŸäº›å†…å®¹ï¼ˆå¦‚çˆ½ç‚¹ã€ä¼ç¬”æ˜¾çœ¼åº¦ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸è‡ªåŠ¨ç”Ÿæˆ

2. **é‡å†™æ¬¡æ•°é™åˆ¶**ï¼šæœ€å¤šé‡å†™2æ¬¡ï¼Œè¶…è¿‡åˆ™å¼ºåˆ¶äº¤ä»˜å¹¶æ ‡è®°é—®é¢˜

3. **ç´§æ€¥å¹²é¢„é—¨æ§›**ï¼šåªæœ‰CRITICALé—®é¢˜æ‰è§¦å‘åœºæ™¯é‡å†™ï¼ŒWARNINGä»…è®°å½•

4. **è¾“å‡ºåˆ†å±‚**ï¼šå¿«é€Ÿæ‘˜è¦â†’é—®é¢˜æ¸…å•â†’æ­£æ–‡â†’è¯¦ç»†æ•°æ®ï¼Œç”¨æˆ·å¯æŒ‰éœ€æŸ¥çœ‹

---

**END OF SOP v3.0**

---
