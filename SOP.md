# ğŸ“‹ Claudeå†™ä½œæ‰§è¡ŒSOP v3.1 

---

## ã€æ¨¡å—0ã€‘GLOBAL_CONFIG - å…¨å±€é…ç½®

```python
# ==================== å…¨å±€å¸¸é‡ ====================
CONST WORD_COUNT_TARGET = 4000  # ç›®æ ‡å­—æ•°
CONST WORD_COUNT_MIN = 3000     # æœ€å°å­—æ•°ï¼ˆç¡¬çº¦æŸï¼‰
CONST WORD_COUNT_MAX = 6000     # æœ€å¤§å­—æ•°ï¼ˆç¡¬çº¦æŸï¼‰
CONST MAX_REWRITE_ATTEMPTS = 2  # æœ€å¤§é‡å†™æ¬¡æ•°

CONST SCENE_WORD_MIN = 300      # å•åœºæ™¯æœ€å°å­—æ•°
CONST SCENE_WORD_MAX = 1200     # å•åœºæ™¯æœ€å¤§å­—æ•°
CONST SCENE_WORD_AVG = 800      # å•åœºæ™¯å¹³å‡å­—æ•°

# ==================== ç•ªèŒ„é£æ ¼æ ¸å¿ƒè§„åˆ™ ====================
CONST TOMATO_CORE_RULES = {
    "pace": "å¿«èŠ‚å¥ï¼Œä¸æ‹–æ³¥å¸¦æ°´",
    "hook_frequency": 600,           # æ¯600å­—ä¸€ä¸ªå°é’©å­
    "dialogue_ratio": [0.35, 0.45],  # å¯¹è¯å æ¯”35%-45%
    "inner_monologue_max": 0.15,     # å†…å¿ƒæˆä¸è¶…è¿‡15%
    "paragraph_max_length": 150,     # å•æ®µä¸è¶…è¿‡150å­—
    "info_density_min": 0.01,        # æ¯100å­—è‡³å°‘1ä¸ªä¿¡æ¯ç‚¹
    "conflict_intensity_floor": 30,  # å†²çªå¼ºåº¦åº•çº¿
    "conflict_interval_max": 600,    # å†²çªæœ€å¤§é—´éš”600å­—
    "boredom_threshold": 60          # æ— èŠåº¦é˜ˆå€¼
}

# ==================== Claudeç—…ç—‡å®Œæ•´æ¸…å•ï¼ˆ20æ¡ï¼‰====================
CONST CLAUDE_ALL20_DISEASES = {
    "D1_è¿‡åº¦å±•å¼€": "æ¯ä¸ªä¿¡æ¯ç‚¹éƒ½å±•å¼€200å­—",
    "D2_è¿‡åº¦è§£é‡Š": "æ¯ä¸ªé€»è¾‘éƒ½è¦ç»™è¯»è€…è§£é‡Šæ¸…æ¥š",
    "D3_è¿‡åº¦å†…å¿ƒæˆ": "ä¸»è§’ä¸€ä¸ªå°åŠ¨ä½œé…500å­—å¿ƒç†æ´»åŠ¨",
    "D4_è¿‡åº¦å®‰å…¨": "é£è¯é€ å¥è°¨æ…å¾—åƒåœ¨å†™æ³•å¾‹æ–‡ä¹¦",
    "D5_è¿‡åº¦å†—ä½™": "åŒä¸€ä¸ªæ„æ€ç”¨3ç§æ–¹å¼é‡å¤è¡¨è¾¾",
    "D6_è¿‡åº¦å¹³å‡": "æ¯ä¸ªåœºæ™¯ã€æ¯ä¸ªè§’è‰²çš„ç¬”å¢¨åˆ†é…éƒ½å¾ˆå‡åŒ€",
    "D7_è¿‡åº¦é€»è¾‘": "æ‰€æœ‰è¡Œä¸ºéƒ½è¦æœ‰æ˜ç¡®çš„å› æœé“¾",
    "D8_è¿‡åº¦å®Œæ•´": "æ¯ä¸ªåœºæ™¯éƒ½è¦æœ‰'èµ·æ‰¿è½¬åˆ'",
    "D9_è¿‡åº¦ç¤¼è²Œ": "è§’è‰²è¯´è¯éƒ½å¾ˆå®¢æ°”ï¼Œæ²¡æœ‰ç²—é²/æš´èº/çœç•¥",
    "D10_è¿‡åº¦å¯¹ç§°": "ä¸»è§’åšAï¼Œé…è§’å°±ä¼šå›åº”Bï¼ŒèŠ‚å¥å‘†æ¿",
    "D11_ä¿¡æ¯å¯†åº¦ä¸è¶³": "ç”¨200å­—è®²ä¸€ä¸ªåªéœ€è¦20å­—çš„äº‹",
    "D12_å£æ°´è¯æ»¥ç”¨": "å¤§é‡çš„'ä¼¼ä¹''ä»¿ä½›''æˆ–è®¸''å¯èƒ½'ç­‰æ¨¡ç³Šè¯",
    "D13_å¥—è·¯åŒ–è½¬æŠ˜": "æ€»æ˜¯ç”¨'ç„¶è€Œ''ä½†æ˜¯''å°±åœ¨è¿™æ—¶'ç­‰è€å¥—è½¬æŠ˜è¯",
    "D14_è¿‡åº¦é“ºå«": "ä¸€ä¸ªå°äº‹ä»¶å‰é¢é“ºå«300å­—",
    "D15_ç»“å°¾æ€»ç»“ç™–": "æ¯ä¸ªåœºæ™¯ç»“å°¾éƒ½è¦æ¥ä¸€å¥æ€»ç»“æ€§çš„è¯",
    "D16_ä¸æ•¢ç•™ç™½": "æ‰€æœ‰ä¼ç¬”éƒ½è¦æš—ç¤ºï¼Œç”Ÿæ€•è¯»è€…çœ‹ä¸æ‡‚",
    "D17_ä¸æ•¢è·³è·ƒ": "æ—¶é—´/ç©ºé—´åˆ‡æ¢éƒ½è¦è¯¦ç»†äº¤ä»£è¿‡æ¸¡",
    "D18_æƒ…ç»ªè¯æ»¥ç”¨": "åŠ¨ä¸åŠ¨å°±'éœ‡æƒŠ''ææƒ§''æ„¤æ€’'ç­‰å¼ºæƒ…ç»ªè¯",
    "D19_åŠ¨ä½œé“¾å®Œæ•´ç™–": "'ä»–ç«™èµ·æ¥ï¼Œèµ°åˆ°é—¨å£ï¼Œä¼¸æ‰‹æ¨å¼€é—¨ï¼Œè¿ˆæ­¥èµ°å‡ºå»'",
    "D20_è§†è§’æ¼‚ç§»": "æ˜æ˜æ˜¯ä¸»è§’è§†è§’ï¼Œå´çªç„¶çŸ¥é“äº†é…è§’çš„æƒ³æ³•"
}

# ==================== ç—…ç—‡æ£€æµ‹æƒé‡ ====================
CONST DISEASE_WEIGHTS = {
    "D1_è¿‡åº¦å±•å¼€": 1.0,
    "D2_è¿‡åº¦è§£é‡Š": 1.0,
    "D3_è¿‡åº¦å†…å¿ƒæˆ": 1.5,      # ä¸¥é‡è¿åï¼ŒåŠ æƒ
    "D4_è¿‡åº¦å®‰å…¨": 0.8,
    "D5_è¿‡åº¦å†—ä½™": 1.2,
    "D6_è¿‡åº¦å¹³å‡": 0.0,         # å†™ä½œé˜¶æ®µå¤„ç†ï¼Œä¸è®¡å…¥
    "D7_è¿‡åº¦é€»è¾‘": 0.5,         # å› æœè¯é«˜é¢‘ï¼Œå‡æƒ
    "D8_è¿‡åº¦å®Œæ•´": 1.0,
    "D9_è¿‡åº¦ç¤¼è²Œ": 0.0,         # å†™ä½œé˜¶æ®µå¤„ç†ï¼Œä¸è®¡å…¥
    "D10_è¿‡åº¦å¯¹ç§°": 0.0,        # å†™ä½œé˜¶æ®µå¤„ç†ï¼Œä¸è®¡å…¥
    "D11_ä¿¡æ¯å¯†åº¦ä¸è¶³": 1.5,    # ä¸¥é‡è¿åï¼ŒåŠ æƒ
    "D12_å£æ°´è¯æ»¥ç”¨": 1.0,
    "D13_å¥—è·¯åŒ–è½¬æŠ˜": 1.0,
    "D14_è¿‡åº¦é“ºå«": 1.2,
    "D15_ç»“å°¾æ€»ç»“ç™–": 1.0,
    "D16_ä¸æ•¢ç•™ç™½": 0.8,
    "D17_ä¸æ•¢è·³è·ƒ": 0.8,
    "D18_æƒ…ç»ªè¯æ»¥ç”¨": 1.5,      # ä¸¥é‡è¿åShowåŸåˆ™ï¼ŒåŠ æƒ
    "D19_åŠ¨ä½œé“¾å®Œæ•´ç™–": 1.0,
    "D20_è§†è§’æ¼‚ç§»": 1.5         # ä¸¥é‡é”™è¯¯ï¼ŒåŠ æƒ
}

# ==================== å¸¸ç”¨çˆ½ç‚¹ç±»å‹ï¼ˆç²¾ç®€åˆ°5ç§ï¼‰====================
CONST COOL_POINT_TYPES = {
    "æ‰“è„¸çˆ½": {
        "ç»“æ„": "è¢«è´¨ç–‘ â†’ ç¬é—´åè½¬ â†’ å¯¹æ–¹éœ‡æƒŠ",
        "å¼ºåº¦": 80,
        "ä½¿ç”¨åœºæ™¯": ["å±•ç¤ºå®åŠ›", "æ‹†ç©¿ä¼ªè£…", "åé©³è´¨ç–‘"],
        "æ’å…¥ä½ç½®": "å†²çªå"
    },
    "è£…é€¼çˆ½": {
        "ç»“æ„": "ä½è°ƒéšè—å®åŠ› â†’ å…³é”®æ—¶åˆ»å±•ç° â†’ ä¼—äººéœ‡æ’¼",
        "å¼ºåº¦": 85,
        "ä½¿ç”¨åœºæ™¯": ["å±æœºæ—¶åˆ»", "è¢«è½»è§†å", "ä¿æŠ¤ä»–äºº"],
        "æ’å…¥ä½ç½®": "åœºæ™¯é«˜æ½®"
    },
    "å¤ä»‡çˆ½": {
        "ç»“æ„": "è¢«æ¬ºè´Ÿ â†’ è®°ä¸‹ä»‡æ¨ â†’ å½“åœºæˆ–å»¶åæŠ¥å¤",
        "å¼ºåº¦": 90,
        "ä½¿ç”¨åœºæ™¯": ["è¢«ç¾è¾±", "è¢«é™·å®³", "è¢«èƒŒå›"],
        "æ’å…¥ä½ç½®": "å†²çªå"
    },
    "å‡çº§çˆ½": {
        "ç»“æ„": "çªç ´å¢ƒç•Œ â†’ å®åŠ›æš´æ¶¨ â†’ ç«‹åˆ»éªŒè¯",
        "å¼ºåº¦": 95,
        "ä½¿ç”¨åœºæ™¯": ["ä¿®ç‚¼", "é¡¿æ‚Ÿ", "è·å¾—ä¼ æ‰¿"],
        "æ’å…¥ä½ç½®": "åœºæ™¯ä¸­æ®µ"
    },
    "åæ€çˆ½": {
        "ç»“æ„": "æ¿’æ­»ç»å¢ƒ â†’ çªç„¶ç¿»ç›˜ â†’ åæ€æ•Œäºº",
        "å¼ºåº¦": 100,
        "ä½¿ç”¨åœºæ™¯": ["ç”Ÿæ­»å±æœº", "ç»å¢ƒ", "èƒŒæ°´ä¸€æˆ˜"],
        "æ’å…¥ä½ç½®": "åœºæ™¯æœ«å°¾"
    }
}

# ==================== å…·ä½“æ€§ç­‰çº§è¡¨ï¼ˆç®€åŒ–ï¼‰====================
CONST SPECIFICITY_RULES = {
    "importance >= 8": "å…·ä½“",    # "ä»–ä¼¸å‡ºå³æ‰‹ï¼Œäº”æŒ‡å¾®æ›²ï¼Œæ¡ä½é—¨æŠŠæ‰‹"
    "importance 5-7": "ä¸­ç­‰",     # "ä»–æ¡ä½é—¨æŠŠæ‰‹"
    "importance < 5": "æŠ½è±¡"      # "ä»–å¼€é—¨"
}

# ==================== èŠ‚å¥æ§åˆ¶è§„åˆ™ï¼ˆç”¨å¥å¼ä»£æ›¿æ³¢å½¢ï¼‰====================
CONST RHYTHM_RULES = {
    "å¿«èŠ‚å¥": {
        "sentence_length": [5, 15],      # å¥é•¿5-15å­—
        "paragraph_length": [30, 80],    # æ®µé•¿30-80å­—
        "pattern": "çŸ­å¥+çŸ­å¥+çŸ­å¥"
    },
    "ä¸­èŠ‚å¥": {
        "sentence_length": [15, 25],
        "paragraph_length": [80, 120],
        "pattern": "é•¿çŸ­å¥äº¤é”™"
    },
    "æ…¢èŠ‚å¥": {
        "sentence_length": [20, 40],
        "paragraph_length": [100, 150],
        "pattern": "é•¿å¥ï¼Œè¯¦ç»†æå†™"
    }
}

# ==================== Show vs Tellè§„åˆ™ï¼ˆåŠ¨æ€ï¼‰====================
CONST SHOW_TELL_RULES = {
    "æƒ…ç»ª": {
        "importance >= 7": "SHOW_DETAILED",  # è¯¦å†™ç”Ÿç†ååº”
        "importance 4-6": "SHOW_BRIEF",      # ç®€å†™ç”Ÿç†ååº”
        "importance < 4": "TELL"              # ç›´æ¥è¯´
    },
    "åŠ¨ä½œ": "SHOW_MOSTLY",     # åŠ¨ä½œä»¥Showä¸ºä¸»
    "ä¿¡æ¯": "TELL_OK",         # ä¿¡æ¯å¯ä»¥Tell
    "ç¯å¢ƒ": "SHOW_SELECTIVELY" # ç¯å¢ƒé€‰æ‹©æ€§Show
}

# ==================== è¯­æ³•è§„åˆ™ï¼ˆç»Ÿä¸€æ ‡å‡†ï¼‰====================
CONST SYNTAX_RULES = {
    "å˜é‡å­˜åœ¨æ£€æŸ¥": "ä½¿ç”¨ 'key' IN dict",
    "å­—ç¬¦ä¸²åŒ…å«æ£€æŸ¥": "ä½¿ç”¨ keyword IN text",
    "é»˜è®¤å€¼è·å–": "ä½¿ç”¨ dict.GET(key, default)",
    "æ•°ç»„è¿‡æ»¤": "ä½¿ç”¨ FILTER(array, condition)",
    "æ•°ç»„æ˜ å°„": "ä½¿ç”¨ MAP(array, function)"
}
```

---

## ã€æ¨¡å—1ã€‘MAIN_EXECUTION_LOOP - ä¸»æ‰§è¡Œæµç¨‹

```python
FUNCTION MAIN_EXECUTION(CAPSULE, existing_humanizer=NULL):
    """
    ä¸»æ‰§è¡Œæµç¨‹ï¼šé›†æˆå®æ—¶è½»é‡æ£€æŸ¥ + æœ€ç»ˆå…¨å±€è¯Šæ–­
    
    å‚æ•°:
        CAPSULE: ä¿¡æ¯èƒ¶å›Šå†…å®¹
        existing_humanizer: é‡å†™æ—¶ä¼ å…¥çš„humanizerï¼ˆä¿ç•™çŠ¶æ€ï¼‰
    """
    
    # ========== STEP 1: è§£æèƒ¶å›Š ==========
    PRINT "[SYSTEM] å¼€å§‹è§£æä¿¡æ¯èƒ¶å›Š..."
    TRY:
        parsed_data = PARSE_CAPSULE(CAPSULE)
        VALIDATE_CAPSULE_INTEGRITY(parsed_data)
    CATCH CapsuleParseError AS e:
        PRINT "[ERROR] èƒ¶å›Šè§£æå¤±è´¥: {e.message}"
        RETURN ERROR_REPORT(e)
    END TRY
    
    # ========== STEP 2: ç”Ÿæˆæˆ–æ¢å¤ä»»åŠ¡æ¸…å• ==========
    IF existing_humanizer IS NULL:
        PRINT "[SYSTEM] ç”Ÿæˆæ‹ŸäººåŒ–ä»»åŠ¡æ¸…å•..."
        humanizer = INIT_HUMANIZE_ENGINE_V3(parsed_data)
        humanizer.rewrite_count = 0
    ELSE:
        PRINT "[SYSTEM] æ¢å¤humanizerçŠ¶æ€ï¼ˆç¬¬{existing_humanizer.rewrite_count}æ¬¡é‡å†™ï¼‰..."
        humanizer = existing_humanizer
    END IF
    
    # ========== STEP 3: åˆå§‹åŒ–å®æ—¶ç›‘æ§å™¨ ==========
    monitors = {
        "word_count": 0,
        "last_info_point": 0,
        "last_conflict": 0,
        "scene_checks": []  # æ¯ä¸ªåœºæ™¯çš„è½»é‡æ£€æŸ¥ç»“æœ
    }
    
    # ========== STEP 4: åˆ†åœºæ™¯å†™ä½œï¼ˆé›†æˆè½»é‡æ£€æŸ¥ï¼‰==========
    chapter_content = ""
    scene_count = GET_SCENE_COUNT_FROM_CAPSULE(parsed_data)
    
    FOR scene_idx = 1 TO scene_count:
        PRINT "[WRITING] æ­£åœ¨å†™ä½œåœºæ™¯ {scene_idx}/{scene_count}..."
        
        # 4.1 æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶æ”¶å°¾
        IF monitors.word_count > WORD_COUNT_MAX * 0.85:
            PRINT "[WARNING] å­—æ•°æ¥è¿‘ä¸Šé™({monitors.word_count}/{WORD_COUNT_MAX})ï¼Œå‡†å¤‡æ”¶å°¾..."
            humanizer.force_ending = TRUE
            # è·³è¿‡å‰©ä½™åœºæ™¯ï¼Œç›´æ¥å†™ç»“å°¾
            IF scene_idx < scene_count:
                scene_count = scene_idx  # æˆªæ–­åœºæ™¯æ•°
            END IF
        END IF
        
        # 4.2 å†™ä½œåœºæ™¯
        TRY:
            scene_text = WRITE_SCENE_V3(
                scene_idx, 
                parsed_data, 
                humanizer, 
                chapter_content
            )
        CATCH SceneWriteError AS e:
            PRINT "[ERROR] åœºæ™¯{scene_idx}å†™ä½œå¤±è´¥: {e.message}"
            scene_text = WRITE_FALLBACK_SCENE(scene_idx, parsed_data)
        END TRY
        
        chapter_content += scene_text
        monitors.word_count += LENGTH(scene_text)
        
        # 4.3 å®æ—¶è½»é‡æ£€æŸ¥
        check_result = LIGHTWEIGHT_SCENE_CHECK(
            scene_text, 
            scene_idx, 
            parsed_data, 
            monitors
        )
        
        monitors.scene_checks.APPEND(check_result)
        
        # 4.4 ç´§æ€¥å¹²é¢„ï¼ˆä»…å¤„ç†CRITICALé—®é¢˜ï¼‰
        IF check_result.severity == "CRITICAL":
            PRINT "[EMERGENCY] åœºæ™¯{scene_idx}å‘ç°ä¸¥é‡é—®é¢˜ï¼Œç«‹å³é‡å†™..."
            
            TRY:
                scene_text = REWRITE_SCENE_WITH_FIX(
                    scene_idx, 
                    parsed_data, 
                    humanizer, 
                    check_result.issue
                )
                chapter_content = REPLACE_LAST_SCENE(chapter_content, scene_text)
                monitors.word_count = LENGTH(chapter_content)  # é‡æ–°è®¡ç®—
            CATCH RewriteError AS e:
                PRINT "[ERROR] é‡å†™å¤±è´¥ï¼Œä¿ç•™åŸåœºæ™¯: {e.message}"
            END TRY
        END IF
    END FOR
    
    # ========== STEP 5: é›†ä¸­åClaudeç—…æ¶¦è‰² ==========
    PRINT "[SYSTEM] æ‰§è¡Œé›†ä¸­åClaudeç—…æ¶¦è‰²..."
    TRY:
        chapter_content = APPLY_ALL_ANTI_DISEASE_RULES(chapter_content, parsed_data)
    CATCH PolishError AS e:
        PRINT "[WARNING] æ¶¦è‰²éƒ¨åˆ†å¤±è´¥: {e.message}"
        # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­
    END TRY
    
    # ========== STEP 6: å…¨å±€è¯Šæ–­ ==========
    PRINT "[SYSTEM] å¼€å§‹å…¨å±€è¯Šæ–­..."
    diagnosis = DIAGNOSE_CHAPTER_V3(chapter_content, parsed_data, monitors)
    
    # ========== STEP 7: ä¿®æ­£æˆ–é‡å†™ ==========
    IF diagnosis.has_critical_issues:
        IF humanizer.rewrite_count < MAX_REWRITE_ATTEMPTS:
            # é—®é¢˜åˆ†æ
            PRINT "[SYSTEM] åˆ†æé—®é¢˜åŸå› ..."
            problem_analysis = ANALYZE_WHAT_WENT_WRONG(diagnosis, parsed_data, chapter_content)
            
            PRINT "[SYSTEM] ç”Ÿæˆä¿®æ­£æŒ‡ä»¤..."
            fix_instruction = GENERATE_FIX_INSTRUCTION(problem_analysis)
            
            humanizer.rewrite_count += 1
            humanizer.fix_instruction = fix_instruction
            
            PRINT "[SYSTEM] ç¬¬{humanizer.rewrite_count}æ¬¡é‡å†™ï¼ˆé’ˆå¯¹æ€§ä¿®æ­£ï¼‰..."
            
            # é€’å½’é‡å†™ï¼ˆä¼ é€’humanizerçŠ¶æ€ï¼‰
            RETURN MAIN_EXECUTION(CAPSULE, humanizer)
        ELSE:
            PRINT "[ERROR] å·²è¾¾æœ€å¤§é‡å†™æ¬¡æ•°({MAX_REWRITE_ATTEMPTS})ï¼Œå¼ºåˆ¶äº¤ä»˜"
            diagnosis.forced_delivery = TRUE
        END IF
    END IF
    
    # ========== STEP 8: æå–Fact ==========
    PRINT "[SYSTEM] æå–æ–°å¢Fact..."
    new_facts = EXTRACT_FACTS(chapter_content, parsed_data)
    
    # ========== STEP 9: äº¤ä»˜ ==========
    PRINT "[SYSTEM] ç”Ÿæˆäº¤ä»˜è¾“å‡º..."
    RETURN DELIVER_OUTPUT_V3(chapter_content, new_facts, diagnosis, monitors)
END FUNCTION
```

---

## ã€æ¨¡å—2ã€‘HUMANIZE_ENGINE_V3 - æ‹ŸäººåŒ–å¼•æ“

```python
FUNCTION INIT_HUMANIZE_ENGINE_V3(parsed_data):
    """
    æ‹ŸäººåŒ–å¼•æ“v3ï¼šä»æ¦‚ç‡æœºåˆ¶æ”¹ä¸ºä»»åŠ¡æ¸…å•æœºåˆ¶
    æ ¸å¿ƒæ”¹è¿›ï¼šæ˜ç¡®å‘Šè¯‰Claude"å“ªä¸ªåœºæ™¯å¿…é¡»åšä»€ä¹ˆ"
    """
    
    humanizer = {
        "task_checklist": {},    # {scene_idx: [task1, task2, ...]}
        "scene_budget": {},      # {scene_idx: word_count}
        "cool_point_plan": {},   # {scene_idx: cool_type}
        "foreshadow_plan": {},   # {scene_idx: {content, visibility}}
        "rewrite_count": 0,
        "force_ending": FALSE,
        "fix_instruction": NULL  # é‡å†™æ—¶çš„ä¿®æ­£æŒ‡ä»¤
    }
    
    # ========== ä¼°ç®—åœºæ™¯æ•°é‡ ==========
    scene_count = ESTIMATE_SCENE_COUNT(parsed_data)
    PRINT "[HUMANIZE] ä¼°ç®—åœºæ™¯æ•°é‡: {scene_count}"
    
    # ========== ç”Ÿæˆä»»åŠ¡æ¸…å• ==========
    FOR scene_idx = 1 TO scene_count:
        tasks = []
        
        # ä»»åŠ¡1ï¼šå¼€ç¯‡æ–¹å¼ï¼ˆä»…ç¬¬ä¸€åœºæ™¯ï¼‰
        IF scene_idx == 1:
            opening_style = RANDOM_CHOICE([
                "åŠ¨ä½œç›´å…¥",   # ç›´æ¥åŠ¨ä½œï¼Œæ— é“ºå«
                "å¯¹è¯ç›´å…¥",   # ç›´æ¥å¯¹è¯ï¼Œæ— èƒŒæ™¯
                "æ„Ÿå®˜ç›´å…¥",   # æ„Ÿå®˜ç»†èŠ‚ï¼Œå¿«é€Ÿå¸¦å…¥
                "å†²çªç›´å…¥"    # ç›´æ¥å†²çªï¼Œä¸è§£é‡ŠåŸå› 
            ])
            tasks.APPEND({
                "type": "å¼€ç¯‡",
                "action": opening_style,
                "priority": "MUST",
                "description": f"ä½¿ç”¨{opening_style}ï¼Œä¸è¶…è¿‡3å¥è¯è¿›å…¥åœºæ™¯"
            })
        END IF
        
        # ä»»åŠ¡2ï¼šè·³è·ƒå¼åˆ‡å…¥ï¼ˆ30%åœºæ™¯ï¼‰
        IF scene_idx > 1 AND RANDOM() < 0.3:
            tasks.APPEND({
                "type": "è·³è·ƒåˆ‡å…¥",
                "action": "åˆ é™¤åœºæ™¯å‰3å¥é“ºå«ï¼Œç›´æ¥ä»åŠ¨ä½œ/å¯¹è¯å¼€å§‹",
                "priority": "MUST",
                "description": "ä¸å†™è¿‡æ¸¡å¥ï¼Œç›´æ¥è¿›å…¥å…³é”®åŠ¨ä½œ"
            })
        END IF
        
        # ä»»åŠ¡3ï¼šæ•…æ„çœç•¥ï¼ˆé€‰æ‹©2-3ä¸ªåœºæ™¯ï¼‰
        # ä¿®å¤ï¼šæ˜ç¡®åˆ†é…åˆ°å…·ä½“åœºæ™¯
        IF scene_idx IN RANDOM_SAMPLE(RANGE(2, scene_count+1), MIN(2, scene_count-1)):
            tasks.APPEND({
                "type": "æ•…æ„çœç•¥",
                "action": "çœç•¥1ä¸ªåŠ¨æœºè§£é‡Š",
                "priority": "SHOULD",
                "description": "æ‰¾åˆ°åŒ…å«'å› ä¸º/ä¸ºäº†/æƒ³è¦'çš„å¥å­ï¼Œåˆ é™¤1å¥",
                "target_keywords": ["å› ä¸º", "ä¸ºäº†", "æƒ³è¦", "ä¹‹æ‰€ä»¥"]
            })
        END IF
        
        # ä»»åŠ¡4ï¼šç¬”å¢¨å¤±è¡¡ï¼ˆé€‰æ‹©1-2ä¸ªåœºæ™¯ï¼‰
        IF scene_idx IN RANDOM_SAMPLE(RANGE(2, scene_count+1), MIN(2, scene_count-1)):
            tasks.APPEND({
                "type": "ç¬”å¢¨å¤±è¡¡",
                "action": "æŸä¸ªæ¬¡è¦å…ƒç´ çªç„¶å¤šå†™50å­—",
                "priority": "SHOULD",
                "description": "é€‰æ‹©å‡ºç°æ¬¡æ•°<=2çš„ç‰©å“æˆ–é…è§’ï¼Œæ‰©å±•æå†™",
                "target_length": 50
            })
        END IF
        
        # ä»»åŠ¡5ï¼šå¯¹è¯ä¸å›åº”ï¼ˆ25%åœºæ™¯ï¼‰
        IF RANDOM() < 0.25:
            tasks.APPEND({
                "type": "å¯¹è¯ä¸å›åº”",
                "action": "è‡³å°‘æœ‰1å¤„ï¼šAè¯´è¯ï¼ŒBä¸æ¥èŒ¬ï¼Œç›´æ¥åšåˆ«çš„",
                "priority": "SHOULD",
                "description": "å¯¹è¯ä¸­æ’å…¥1æ¬¡ä¸å¯¹ç§°å›åº”"
            })
        END IF
        
        humanizer.task_checklist[scene_idx] = tasks
    END FOR
    
    # ========== åˆ†é…åœºæ™¯å­—æ•°é¢„ç®— ==========
    total_budget = parsed_data.meta.GET("word_count_target", WORD_COUNT_TARGET)
    scene_importances = GET_SCENE_IMPORTANCES(parsed_data, scene_count)
    importance_sum = SUM(scene_importances.VALUES())
    
    FOR scene_idx = 1 TO scene_count:
        importance = scene_importances.GET(scene_idx, 5)  # é»˜è®¤é‡è¦æ€§5
        
        # æŒ‰é‡è¦æ€§åˆ†é…é¢„ç®—
        budget = total_budget * (importance / importance_sum)
        
        # çº¦æŸåœ¨åˆç†èŒƒå›´
        budget = CLAMP(budget, SCENE_WORD_MIN, SCENE_WORD_MAX)
        
        humanizer.scene_budget[scene_idx] = budget
        PRINT "[HUMANIZE] åœºæ™¯{scene_idx}é¢„ç®—: {budget:.0f}å­—ï¼ˆé‡è¦æ€§{importance}ï¼‰"
    END FOR
    
    # ========== çˆ½ç‚¹è®¡åˆ’ ==========
    IF "cool_point_plan" IN parsed_data.goals:
        # ä»Capsuleç›´æ¥è¯»å–
        humanizer.cool_point_plan = parsed_data.goals.cool_point_plan
        PRINT "[HUMANIZE] ä½¿ç”¨CapsuleæŒ‡å®šçš„çˆ½ç‚¹è®¡åˆ’"
    ELSE:
        # è‡ªåŠ¨è§„åˆ’ï¼šå¹³å‡æ¯1500å­—ä¸€ä¸ªçˆ½ç‚¹
        cool_point_count = MAX(1, ROUND(total_budget / 1500))
        cool_point_scenes = DISTRIBUTE_EVENLY(RANGE(1, scene_count+1), cool_point_count)
        
        FOR scene_idx IN cool_point_scenes:
            suitable_type = SELECT_COOL_POINT_TYPE_FOR_SCENE(scene_idx, parsed_data, scene_count)
            humanizer.cool_point_plan[scene_idx] = suitable_type
            PRINT "[HUMANIZE] åœºæ™¯{scene_idx}è®¡åˆ’çˆ½ç‚¹: {suitable_type}"
        END FOR
    END IF
    
    # ========== ä¼ç¬”è®¡åˆ’ ==========
    IF "foreshadowing" IN parsed_data.goals:
        FOR foreshadow IN parsed_data.goals.foreshadowing:
            scene_idx = CHOOSE_SCENE_FOR_FORESHADOW(foreshadow, scene_count, parsed_data)
            
            humanizer.foreshadow_plan[scene_idx] = {
                "content": foreshadow.GET("content", ""),
                "visibility": foreshadow.GET("visibility", "éšè”½")  # é»˜è®¤éšè”½
            }
            PRINT "[HUMANIZE] åœºæ™¯{scene_idx}è®¡åˆ’ä¼ç¬”: {foreshadow.content[:20]}..."
        END FOR
    END IF
    
    PRINT "[HUMANIZE] ä»»åŠ¡æ¸…å•ç”Ÿæˆå®Œæˆ"
    
    RETURN humanizer
END FUNCTION
```

---

## ã€æ¨¡å—3ã€‘LIGHTWEIGHT_SCENE_CHECK - å®æ—¶è½»é‡æ£€æŸ¥

```python
FUNCTION LIGHTWEIGHT_SCENE_CHECK(scene_text, scene_idx, parsed_data, monitors):
    """
    åœºæ™¯å†™å®Œåçš„è½»é‡æ£€æŸ¥
    åªæ£€æŸ¥æœ€å…³é”®çš„é—®é¢˜ï¼Œé¿å…è¿‡åº¦æ£€æŸ¥
    """
    
    check_result = {
        "scene_idx": scene_idx,
        "severity": "OK",  # OK / WARNING / CRITICAL
        "issue": NULL,
        "word_count": LENGTH(scene_text)
    }
    
    # ========== æ£€æŸ¥1ï¼šçº¢çº¿è¿åï¼ˆCRITICALï¼‰==========
    IF "redlines" IN parsed_data.constraints:
        FOR redline IN parsed_data.constraints.redlines:
            IF CHECK_REDLINE_VIOLATION(scene_text, redline):
                check_result.severity = "CRITICAL"
                check_result.issue = f"è¿åçº¢çº¿ï¼š{redline}"
                RETURN check_result
            END IF
        END FOR
    END IF
    
    # ========== æ£€æŸ¥2ï¼šå­—æ•°ä¸¥é‡è¶…æ ‡ï¼ˆCRITICALï¼‰==========
    IF LENGTH(scene_text) > SCENE_WORD_MAX:
        check_result.severity = "CRITICAL"
        check_result.issue = f"åœºæ™¯{scene_idx}å­—æ•°è¿‡å¤š({LENGTH(scene_text)}å­—ï¼Œä¸Šé™{SCENE_WORD_MAX})ï¼Œä¼šå¯¼è‡´å…¨ç« è¶…æ ‡"
        RETURN check_result
    END IF
    
    # ========== æ£€æŸ¥3ï¼šå­—æ•°ä¸¥é‡ä¸è¶³ï¼ˆWARNINGï¼‰==========
    IF LENGTH(scene_text) < SCENE_WORD_MIN:
        check_result.severity = "WARNING"
        check_result.issue = f"åœºæ™¯{scene_idx}å­—æ•°è¿‡å°‘({LENGTH(scene_text)}å­—ï¼Œä¸‹é™{SCENE_WORD_MIN})"
        # WARNINGä¸è§¦å‘é‡å†™ï¼Œä»…è®°å½•
    END IF
    
    # ========== æ£€æŸ¥4ï¼šä¿¡æ¯å¯†åº¦è¿‡ä½ï¼ˆWARNINGï¼‰==========
    info_density = COUNT_NEW_INFO(scene_text) / MAX(LENGTH(scene_text), 1)
    IF info_density < TOMATO_CORE_RULES.info_density_min:
        check_result.severity = "WARNING"
        check_result.issue = f"åœºæ™¯{scene_idx}ä¿¡æ¯å¯†åº¦è¿‡ä½({info_density:.3f}ï¼Œæœ€ä½{TOMATO_CORE_RULES.info_density_min})"
    END IF
    
    # ========== æ£€æŸ¥5ï¼šå†²çªç¼ºå¤±ï¼ˆWARNINGï¼‰==========
    words_since_conflict = monitors.word_count - monitors.last_conflict
    IF words_since_conflict > TOMATO_CORE_RULES.conflict_interval_max:
        check_result.severity = "WARNING"
        check_result.issue = f"å·²è¿ç»­{words_since_conflict}å­—æ— å†²çªï¼ˆä¸Šé™{TOMATO_CORE_RULES.conflict_interval_max}ï¼‰"
        
        # æ£€æŸ¥å½“å‰åœºæ™¯æ˜¯å¦æœ‰å†²çª
        IF HAS_CONFLICT(scene_text):
            monitors.last_conflict = monitors.word_count  # æ›´æ–°å†²çªä½ç½®
        END IF
    END IF
    
    # ========== æ£€æŸ¥6ï¼šå¯¹è¯å æ¯”å¼‚å¸¸ï¼ˆWARNINGï¼‰==========
    dialogue_ratio = CALCULATE_DIALOGUE_RATIO(scene_text)
    target_range = TOMATO_CORE_RULES.dialogue_ratio
    
    IF dialogue_ratio < target_range[0] * 0.5 OR dialogue_ratio > target_range[1] * 1.5:
        check_result.severity = "WARNING"
        check_result.issue = f"åœºæ™¯{scene_idx}å¯¹è¯å æ¯”å¼‚å¸¸({dialogue_ratio*100:.1f}%ï¼Œç›®æ ‡{target_range[0]*100}-{target_range[1]*100}%)"
    END IF
    
    RETURN check_result
END FUNCTION
```

---

## ã€æ¨¡å—4ã€‘WRITE_SCENE_V3 - åœºæ™¯å†™ä½œå™¨

```python
FUNCTION WRITE_SCENE_V3(scene_idx, parsed_data, humanizer, previous_content):
    """
    å†™ä½œå•ä¸ªåœºæ™¯ï¼ˆv3ä¿®å¤ç‰ˆï¼‰
    æ ¸å¿ƒæ”¹è¿›ï¼šæ‰§è¡Œä»»åŠ¡æ¸…å•ï¼Œè€Œéæ¦‚ç‡å†³ç­–
    """
    
    scene_text = ""
    scene_budget = humanizer.scene_budget.GET(scene_idx, SCENE_WORD_AVG)
    tasks = humanizer.task_checklist.GET(scene_idx, [])
    
    PRINT "[SCENE] åœºæ™¯{scene_idx}å¼€å§‹ï¼Œé¢„ç®—{scene_budget:.0f}å­—ï¼Œä»»åŠ¡{LENGTH(tasks)}é¡¹"
    
    # ========== STEP 1: æ‰§è¡Œå¼€ç¯‡ä»»åŠ¡ ==========
    opening_task = FIND_TASK_BY_TYPE(tasks, "å¼€ç¯‡")
    
    IF opening_task IS NOT NULL:
        scene_text = EXECUTE_OPENING_TASK(opening_task, parsed_data, scene_idx)
    ELSE:
        scene_text = WRITE_DEFAULT_OPENING(scene_idx, parsed_data, previous_content)
    END IF
    
    # ========== STEP 2: æ£€æŸ¥æ˜¯å¦éœ€è¦åº”ç”¨ä¿®æ­£æŒ‡ä»¤ ==========
    IF humanizer.fix_instruction IS NOT NULL:
        IF scene_idx IN humanizer.fix_instruction.GET("focus_scenes", []):
            PRINT "[FIX] åœºæ™¯{scene_idx}åº”ç”¨ä¿®æ­£æŒ‡ä»¤"
            scene_text = APPLY_FIX_TO_OPENING(scene_text, humanizer.fix_instruction)
        END IF
    END IF
    
    # ========== STEP 3: å†™ä½œåœºæ™¯ä¸»ä½“ ==========
    units = DECOMPOSE_SCENE_TO_UNITS(scene_idx, parsed_data, humanizer)
    
    FOR unit IN units:
        # 3.1 å­—æ•°æ§åˆ¶ï¼ˆç¡¬åœæ­¢ï¼‰
        IF LENGTH(scene_text) > scene_budget * 1.1:
            PRINT "[BUDGET] åœºæ™¯{scene_idx}è¾¾åˆ°é¢„ç®—ä¸Šé™({LENGTH(scene_text)}å­—)ï¼Œå¼ºåˆ¶ç»“æŸ"
            BREAK
        END IF
        
        # 3.2 å¼ºåˆ¶æ”¶å°¾æ£€æŸ¥
        IF humanizer.force_ending AND LENGTH(scene_text) > scene_budget * 0.8:
            PRINT "[ENDING] è§¦å‘å¼ºåˆ¶æ”¶å°¾ï¼Œè·³è¿‡å‰©ä½™å•å…ƒ"
            BREAK
        END IF
        
        # 3.3 æ ¹æ®é‡è¦æ€§å†³å®šå±•å¼€ç¨‹åº¦
        expansion_level = DECIDE_EXPANSION_BY_IMPORTANCE(unit.GET("importance", 5))
        
        # 3.4 å†™ä½œå•å…ƒ
		unit_text = WRITE_UNIT_BY_TYPE(
            unit, 
            expansion_level, 
            parsed_data
        )
        
        scene_text += unit_text
    END FOR
    
    # ========== STEP 4: æ‰§è¡Œæ‹ŸäººåŒ–ä»»åŠ¡ ==========
    FOR task IN tasks:
        IF task.type != "å¼€ç¯‡":  # å¼€ç¯‡ä»»åŠ¡å·²æ‰§è¡Œ
            scene_text = EXECUTE_HUMANIZE_TASK(scene_text, task, parsed_data)
        END IF
    END FOR
    
    # ========== STEP 5: æ³¨å…¥çˆ½ç‚¹ï¼ˆå¦‚æœè®¡åˆ’ä¸­æœ‰ï¼‰==========
    IF scene_idx IN humanizer.cool_point_plan:
        cool_type = humanizer.cool_point_plan[scene_idx]
        PRINT "[COOL] åœºæ™¯{scene_idx}æ³¨å…¥çˆ½ç‚¹: {cool_type}"
        
        cool_text = GENERATE_COOL_POINT(cool_type, parsed_data, scene_idx)
        scene_text = INSERT_COOL_POINT(scene_text, cool_text, cool_type)
    END IF
    
    # ========== STEP 6: åŸ‹ä¼ç¬”ï¼ˆå¦‚æœè®¡åˆ’ä¸­æœ‰ï¼‰==========
    IF scene_idx IN humanizer.foreshadow_plan:
        foreshadow = humanizer.foreshadow_plan[scene_idx]
        PRINT "[FORESHADOW] åœºæ™¯{scene_idx}åŸ‹ä¼ç¬”: {foreshadow.content[:20]}..."
        
        foreshadow_text = WRITE_FORESHADOW(foreshadow, parsed_data)
        scene_text = INSERT_FORESHADOW(scene_text, foreshadow_text, foreshadow.visibility)
    END IF
    
    # ========== STEP 7: åœºæ™¯ç»“å°¾ ==========
    ending = WRITE_SCENE_ENDING(scene_idx, parsed_data, humanizer, scene_text)
    scene_text += ending
    
    PRINT "[SCENE] åœºæ™¯{scene_idx}å®Œæˆï¼Œå®é™…{LENGTH(scene_text)}å­—"
    
    RETURN scene_text
END FUNCTION

# ==================== è¾…åŠ©å‡½æ•° ====================

FUNCTION DECIDE_EXPANSION_BY_IMPORTANCE(importance):
    """æ ¹æ®é‡è¦æ€§å†³å®šå±•å¼€ç¨‹åº¦"""
    IF importance >= 8:
        RETURN "EXPAND"      # å±•å¼€50-100å­—
    ELSE IF importance >= 5:
        RETURN "BRIEF"       # ç®€å†™20-50å­—
    ELSE:
        RETURN "SKIP"        # ä¸€å¥è¯å¸¦è¿‡æˆ–è·³è¿‡
    END IF
END FUNCTION

FUNCTION WRITE_UNIT_BY_TYPE(unit, expansion_level, parsed_data):
    """æ ¹æ®å•å…ƒç±»å‹å†™ä½œ"""
    unit_type = unit.GET("type", "æå†™")
    
    SWITCH unit_type:
        CASE "åŠ¨ä½œ":
            RETURN WRITE_ACTION(unit, expansion_level, parsed_data)
        
        CASE "å¯¹è¯":
            RETURN WRITE_DIALOGUE(unit, expansion_level, parsed_data)
        
        CASE "æƒ…ç»ª":
            RETURN WRITE_EMOTION(unit, expansion_level, parsed_data)
        
        CASE "æå†™":
            RETURN WRITE_DESCRIPTION(unit, expansion_level, parsed_data)
        
        CASE "å†…å¿ƒæˆ":
            IF expansion_level == "EXPAND":
                RETURN WRITE_INNER_MONOLOGUE(unit, parsed_data)
            ELSE:
                RETURN ""  # ä½é‡è¦åº¦çš„å†…å¿ƒæˆç›´æ¥è·³è¿‡
            END IF
        
        DEFAULT:
            RETURN ""
    END SWITCH
END FUNCTION

FUNCTION EXECUTE_HUMANIZE_TASK(scene_text, task, parsed_data):
    """æ‰§è¡Œæ‹ŸäººåŒ–ä»»åŠ¡ï¼ˆä¿®å¤ç‰ˆï¼šè¾¹ç•Œæ¡ä»¶å¤„ç†ï¼‰"""
    
    SWITCH task.type:
        CASE "è·³è·ƒåˆ‡å…¥":
            RETURN REMOVE_FIRST_N_SENTENCES_SAFE(scene_text, 3, parsed_data)
        
        CASE "æ•…æ„çœç•¥":
            RETURN REMOVE_ONE_MOTIVATION(scene_text, task.GET("target_keywords", []))
        
        CASE "ç¬”å¢¨å¤±è¡¡":
            RETURN EXPAND_MINOR_ELEMENT(scene_text, task.GET("target_length", 50), parsed_data)
        
        CASE "å¯¹è¯ä¸å›åº”":
            # å·²åœ¨å¯¹è¯å†™ä½œæ—¶å¤„ç†
            RETURN scene_text
        
        DEFAULT:
            RETURN scene_text
    END SWITCH
END FUNCTION

FUNCTION REMOVE_FIRST_N_SENTENCES_SAFE(scene_text, n, parsed_data):
    """
    å®‰å…¨åœ°åˆ é™¤å‰Nå¥ï¼ˆä¿®å¤ç‰ˆï¼šæ£€æŸ¥å…³é”®ä¿¡æ¯ï¼‰
    """
    sentences = SPLIT_SENTENCES(scene_text)
    
    IF LENGTH(sentences) <= n:
        PRINT "[SKIP] åœºæ™¯å¥å­æ•°ä¸è¶³{n}å¥ï¼Œè·³è¿‡è·³è·ƒåˆ‡å…¥"
        RETURN scene_text
    END IF
    
    first_n = sentences[0:n]
    first_n_text = JOIN(first_n, "")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
    has_critical = (
        CONTAINS_REDLINE_KEYWORDS(first_n_text, parsed_data) OR
        CONTAINS_CORE_MISSION_KEYWORDS(first_n_text, parsed_data) OR
        CONTAINS_CHARACTER_INTRO(first_n_text, parsed_data)
    )
    
    IF has_critical:
        PRINT "[SKIP] å‰{n}å¥åŒ…å«å…³é”®ä¿¡æ¯ï¼Œè·³è¿‡è·³è·ƒåˆ‡å…¥"
        RETURN scene_text
    END IF
    
    RETURN JOIN(sentences[n:], "")
END FUNCTION

FUNCTION REMOVE_ONE_MOTIVATION(scene_text, target_keywords):
    """
    åˆ é™¤ä¸€ä¸ªåŠ¨æœºè§£é‡Šï¼ˆä¿®å¤ç‰ˆï¼šæ˜ç¡®å®šä½ï¼‰
    """
    IF LENGTH(target_keywords) == 0:
        target_keywords = ["å› ä¸º", "ä¸ºäº†", "æƒ³è¦", "ä¹‹æ‰€ä»¥"]
    END IF
    
    sentences = SPLIT_SENTENCES(scene_text)
    motivation_indices = []
    
    # æ‰¾åˆ°æ‰€æœ‰åŒ…å«åŠ¨æœºå…³é”®è¯çš„å¥å­
    FOR i, sent IN ENUMERATE(sentences):
        FOR keyword IN target_keywords:
            IF keyword IN sent AND LENGTH(sent) > 10:
                motivation_indices.APPEND(i)
                BREAK
            END IF
        END FOR
    END FOR
    
    IF LENGTH(motivation_indices) == 0:
        PRINT "[SKIP] æœªæ‰¾åˆ°åŠ¨æœºå¥ï¼Œè·³è¿‡æ•…æ„çœç•¥"
        RETURN scene_text
    END IF
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªåˆ é™¤
    target_idx = RANDOM_CHOICE(motivation_indices)
    sentences[target_idx] = ""  # åˆ é™¤è¯¥å¥
    
    RETURN JOIN(FILTER(sentences, lambda s: LENGTH(s) > 0), "")
END FUNCTION

FUNCTION EXPAND_MINOR_ELEMENT(scene_text, target_length, parsed_data):
    """
    æ‰©å±•æ¬¡è¦å…ƒç´ æå†™ï¼ˆä¿®å¤ç‰ˆï¼šæ˜ç¡®è¯†åˆ«ï¼‰
    """
    # æå–æ‰€æœ‰åè¯
    nouns = EXTRACT_NOUNS(scene_text)
    
    # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
    noun_counts = COUNT_OCCURRENCES_EACH(nouns)
    
    # ç­›é€‰å‡ºç°æ¬¡æ•°<=2çš„æ¬¡è¦å…ƒç´ 
    minor_elements = FILTER(noun_counts.ITEMS(), lambda item: item[1] <= 2)
    
    IF LENGTH(minor_elements) == 0:
        PRINT "[SKIP] æœªæ‰¾åˆ°æ¬¡è¦å…ƒç´ ï¼Œè·³è¿‡ç¬”å¢¨å¤±è¡¡"
        RETURN scene_text
    END IF
    
    # éšæœºé€‰æ‹©ä¸€ä¸ª
    target_noun, count = RANDOM_CHOICE(minor_elements)
    
    # ç”Ÿæˆæ‰©å±•æå†™
    expansion = GENERATE_MINOR_ELEMENT_DESCRIPTION(target_noun, target_length, parsed_data)
    
    # åœ¨ç¬¬ä¸€æ¬¡å‡ºç°è¯¥åè¯çš„ä½ç½®åæ’å…¥
    first_occurrence_pos = FIND_FIRST(scene_text, target_noun)
    
    IF first_occurrence_pos == -1:
        RETURN scene_text
    END IF
    
    # æ‰¾åˆ°è¯¥å¥çš„ç»“æŸä½ç½®
    sentence_end = FIND_NEXT(scene_text, "ã€‚", first_occurrence_pos)
    
    IF sentence_end == -1:
        RETURN scene_text
    END IF
    
    # æ’å…¥æ‰©å±•æå†™
    RETURN scene_text[:sentence_end+1] + expansion + scene_text[sentence_end+1:]
END FUNCTION
```

---

## ã€æ¨¡å—5ã€‘ANTI_DISEASE_PROTOCOL_V3 - é›†ä¸­åClaudeç—…

```python
FUNCTION APPLY_ALL_ANTI_DISEASE_RULES(chapter_content, parsed_data):
    """
    é›†ä¸­åº”ç”¨æ‰€æœ‰åClaudeç—…è§„åˆ™ï¼ˆä¿®å¤ç‰ˆï¼šåŠ æƒå¤„ç†ï¼‰
    åœ¨æ¶¦è‰²é˜¶æ®µç»Ÿä¸€å¤„ç†ï¼Œè€Œéåˆ†æ•£åœ¨å†™ä½œè¿‡ç¨‹ä¸­
    """
    
    PRINT "[POLISH] å¼€å§‹é›†ä¸­åClaudeç—…æ¶¦è‰²ï¼ˆ20æ¡è§„åˆ™ï¼‰..."
    
    polished = chapter_content
    disease_log = {}  # è®°å½•æ¯ç§ç—…ç—‡çš„å¤„ç†æ¬¡æ•°
    
    # ========== ç—…ç—‡1ï¼šåˆ é™¤å£æ°´è¯ï¼ˆD12ï¼‰==========
    filler_words = ["ä¼¼ä¹", "ä»¿ä½›", "æˆ–è®¸", "å¯èƒ½", "å¤§æ¦‚", "å¥½åƒ", "æŸç§ç¨‹åº¦ä¸Š"]
    count = 0
    FOR word IN filler_words:
        occurrences = COUNT_OCCURRENCES(polished, word)
        polished = REPLACE_ALL(polished, word, "")
        count += occurrences
    END FOR
    disease_log["D12_å£æ°´è¯"] = count * DISEASE_WEIGHTS["D12_å£æ°´è¯æ»¥ç”¨"]
    
    # ========== ç—…ç—‡2ï¼šåˆ é™¤å¥—è·¯è½¬æŠ˜è¯ï¼ˆD13ï¼‰==========
    cliche_words = ["ç„¶è€Œ", "ä½†æ˜¯", "å°±åœ¨è¿™æ—¶", "çªç„¶", "å¿½ç„¶", "éœæ—¶é—´"]
    count = 0
    FOR word IN cliche_words:
        occurrences = COUNT_OCCURRENCES(polished, word)
        polished = REPLACE_ALL(polished, word, "")
        count += occurrences
    END FOR
    disease_log["D13_å¥—è·¯è½¬æŠ˜"] = count * DISEASE_WEIGHTS["D13_å¥—è·¯åŒ–è½¬æŠ˜"]
    
    # ========== ç—…ç—‡3ï¼šæ›¿æ¢æƒ…ç»ªè¯ä¸ºç”Ÿç†ååº”ï¼ˆD18ï¼‰==========
    polished, emotion_count = REPLACE_EMOTION_WORDS_WITH_PHYSIOLOGY(polished)
    disease_log["D18_æƒ…ç»ªè¯"] = emotion_count * DISEASE_WEIGHTS["D18_æƒ…ç»ªè¯æ»¥ç”¨"]
    
    # ========== ç—…ç—‡4ï¼šç®€åŒ–åŠ¨ä½œé“¾ï¼ˆD19ï¼‰==========
    polished, action_count = SIMPLIFY_ACTION_CHAINS(polished)
    disease_log["D19_åŠ¨ä½œé“¾"] = action_count * DISEASE_WEIGHTS["D19_åŠ¨ä½œé“¾å®Œæ•´ç™–"]
    
    # ========== ç—…ç—‡5ï¼šåˆ é™¤æ®µè½æ€»ç»“å¥ï¼ˆD15ï¼‰==========
    polished, summary_count = REMOVE_PARAGRAPH_SUMMARIES(polished)
    disease_log["D15_æ€»ç»“ç™–"] = summary_count * DISEASE_WEIGHTS["D15_ç»“å°¾æ€»ç»“ç™–"]
    
    # ========== ç—…ç—‡6ï¼šå‹ç¼©è¿‡é•¿æ®µè½ï¼ˆé€šç”¨ï¼‰==========
    polished, compress_count = COMPRESS_LONG_PARAGRAPHS(polished, TOMATO_CORE_RULES.paragraph_max_length)
    disease_log["æ®µè½è¿‡é•¿"] = compress_count
    
    # ========== ç—…ç—‡7ï¼šå‡å°‘å†…å¿ƒæˆï¼ˆD3ï¼‰==========
    polished, inner_count = REDUCE_INNER_MONOLOGUE(polished, TOMATO_CORE_RULES.inner_monologue_max)
    disease_log["D3_å†…å¿ƒæˆ"] = inner_count * DISEASE_WEIGHTS["D3_è¿‡åº¦å†…å¿ƒæˆ"]
    
    # ========== ç—…ç—‡8ï¼šåˆ é™¤å†—ä½™ï¼ˆD5ï¼‰==========
    polished, redundancy_count = REMOVE_REDUNDANCY(polished)
    disease_log["D5_å†—ä½™"] = redundancy_count * DISEASE_WEIGHTS["D5_è¿‡åº¦å†—ä½™"]
    
    # ========== ç—…ç—‡9ï¼šä¼˜åŒ–å¥é•¿ï¼ˆé€šç”¨ï¼‰==========
    polished = OPTIMIZE_SENTENCE_LENGTH(polished)
    
    # ========== ç—…ç—‡10ï¼šåˆ é™¤è§£é‡Šæ€§å†…å¿ƒæˆï¼ˆD2ï¼‰==========
    polished, explain_count = REMOVE_EXPLANATORY_THOUGHTS(polished)
    disease_log["D2_è§£é‡Š"] = explain_count * DISEASE_WEIGHTS["D2_è¿‡åº¦è§£é‡Š"]
    
    # ========== ç—…ç—‡11ï¼šè¿‡åº¦å®‰å…¨ï¼ˆD4ï¼‰==========
    hedging_phrases = ["å¯ä»¥è¯´", "åŸºæœ¬ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š", "æŸç§è§’åº¦", "ç›¸å¯¹è€Œè¨€", "æ€»çš„æ¥è¯´"]
    count = 0
    FOR phrase IN hedging_phrases:
        occurrences = COUNT_OCCURRENCES(polished, phrase)
        polished = REPLACE_ALL(polished, phrase, "")
        count += occurrences
    END FOR
    disease_log["D4_å®‰å…¨"] = count * DISEASE_WEIGHTS["D4_è¿‡åº¦å®‰å…¨"]
    
    # ========== ç—…ç—‡12ï¼šè¿‡åº¦é“ºå«ï¼ˆD14ï¼‰==========
    polished, setup_count = COMPRESS_SETUP(polished)
    disease_log["D14_é“ºå«"] = setup_count * DISEASE_WEIGHTS["D14_è¿‡åº¦é“ºå«"]
    
    # ========== ç—…ç—‡13ï¼šä¸æ•¢è·³è·ƒï¼ˆD17ï¼‰==========
    polished, transition_count = REMOVE_TRANSITIONS(polished)
    disease_log["D17_è·³è·ƒ"] = transition_count * DISEASE_WEIGHTS["D17_ä¸æ•¢è·³è·ƒ"]
    
    # ========== ç—…ç—‡14ï¼šè¿‡åº¦é€»è¾‘ï¼ˆD7ï¼‰==========
    polished, causality_count = REMOVE_CAUSALITY_EXPLANATION(polished)
    disease_log["D7_é€»è¾‘"] = causality_count * DISEASE_WEIGHTS["D7_è¿‡åº¦é€»è¾‘"]
    
    # ========== ç—…ç—‡15ï¼šè¿‡åº¦å®Œæ•´ï¼ˆD8ï¼‰==========
    polished, structure_count = BREAK_SCENE_STRUCTURE(polished)
    disease_log["D8_å®Œæ•´"] = structure_count * DISEASE_WEIGHTS["D8_è¿‡åº¦å®Œæ•´"]
    
    # ========== ç—…ç—‡16ï¼šè¿‡åº¦ç¤¼è²Œï¼ˆD9ï¼‰==========
    polished, rough_count = ADD_CONVERSATIONAL_ROUGHNESS(polished)
    disease_log["D9_ç¤¼è²Œ"] = rough_count  # æƒé‡ä¸º0ï¼Œä»…è®°å½•
    
    # ========== ç—…ç—‡17ï¼šä¿¡æ¯å¯†åº¦ä¸è¶³ï¼ˆD11/D20ï¼‰==========
    polished, density_count = COMPRESS_LOW_DENSITY_PARAGRAPHS(polished)
    disease_log["D11_å¯†åº¦"] = density_count * DISEASE_WEIGHTS["D11_ä¿¡æ¯å¯†åº¦ä¸è¶³"]
    
    # ========== ç—…ç—‡18ï¼šä¸æ•¢ç•™ç™½ï¼ˆD16ï¼‰==========
    hint_patterns = ["ä¼¼ä¹é¢„ç¤ºç€", "å¯èƒ½ä¼š", "æˆ–è®¸æ„å‘³ç€", "æš—ç¤ºç€"]
    count = 0
    FOR pattern IN hint_patterns:
        occurrences = COUNT_OCCURRENCES(polished, pattern)
        polished = REPLACE_ALL(polished, pattern, "")
        count += occurrences
    END FOR
    disease_log["D16_ç•™ç™½"] = count * DISEASE_WEIGHTS["D16_ä¸æ•¢ç•™ç™½"]
    
    # ========== ç—…ç—‡19ï¼šè§†è§’æ¼‚ç§»ï¼ˆD20ï¼‰==========
    protagonist_name = parsed_data.characters.protagonist.GET("name", "ä¸»è§’")
    polished, pov_count = FIX_POV_SHIFTS(polished, protagonist_name)
    disease_log["D20_è§†è§’"] = pov_count * DISEASE_WEIGHTS["D20_è§†è§’æ¼‚ç§»"]
    
    # ========== ç—…ç—‡20ï¼šè¿‡åº¦å±•å¼€ï¼ˆD1ï¼‰==========
    polished, expand_count = COMPRESS_OVER_EXPANDED_INFO(polished)
    disease_log["D1_å±•å¼€"] = expand_count * DISEASE_WEIGHTS["D1_è¿‡åº¦å±•å¼€"]
    
    # ========== è¾“å‡ºç—…ç—‡å¤„ç†æŠ¥å‘Š ==========
    total_weighted_score = SUM(disease_log.VALUES())
    PRINT "[POLISH] åClaudeç—…æ¶¦è‰²å®Œæˆï¼ŒåŠ æƒç—…ç—‡åˆ†: {total_weighted_score:.1f}"
    
    FOR disease, score IN disease_log.ITEMS():
        IF score > 0:
            PRINT "[POLISH]   - {disease}: {score:.1f}åˆ†"
        END IF
    END FOR
    
    RETURN polished
END FUNCTION

# ==================== å…·ä½“å®ç°å‡½æ•°====================

FUNCTION REPLACE_EMOTION_WORDS_WITH_PHYSIOLOGY(text):
    """æ›¿æ¢æƒ…ç»ªè¯ä¸ºç”Ÿç†ååº”ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    emotion_map = {
        "éœ‡æƒŠ": ["ç³å­”éª¤ç„¶æ”¶ç¼©", "å‘¼å¸ä¸€æ»", "åƒµåœ¨åŸåœ°"],
        "ææƒ§": ["åèƒŒå‘å‡‰", "è…¿å‘è½¯", "å†·æ±—æµ¸é€åèƒŒ"],
        "æ„¤æ€’": ["å¤ªé˜³ç©´è·³åŠ¨", "æ‹³å¤´æ”¥ç´§", "è„¸é¢Šå‘çƒ«"],
        "ç„¦è™‘": ["èƒƒéƒ¨æ”¶ç´§", "æ‰‹å¿ƒå†’æ±—", "å‘¼å¸å˜æµ…"],
        "ç´§å¼ ": ["å–‰å’™å‘å¹²", "å¿ƒè·³åŠ é€Ÿ", "è‚Œè‚‰ç´§ç»·"],
        "å…´å¥‹": ["å¿ƒè·³å¦‚é¼“", "å‘¼å¸æ€¥ä¿ƒ", "çœ¼ç›å‘äº®"],
        "æ‚²ä¼¤": ["é¼»è…”å‘é…¸", "çœ¼çœ¶å‘çƒ­", "å–‰å¤´å‘ç´§"]
    }
    
    replace_count = 0
    
    FOR emotion_word, reactions IN emotion_map.ITEMS():
        # åŒ¹é…æ¨¡å¼ï¼š"ä»–/å¥¹ + æ„Ÿåˆ°/å……æ»¡/æ»¡æ˜¯ + æƒ…ç»ªè¯"
        patterns = [
            f"(ä»–|å¥¹|\\w{{2,4}}).*æ„Ÿåˆ°{emotion_word}",
            f"(ä»–|å¥¹|\\w{{2,4}}).*å……æ»¡{emotion_word}",
            f"(ä»–|å¥¹|\\w{{2,4}}).*æ»¡æ˜¯{emotion_word}",
            f"{emotion_word}(æ¶Œä¸Š|æ¶Œèµ·|è¢­æ¥)"
        ]
        
        FOR pattern IN patterns:
            matches = FIND_ALL_WITH_CONTEXT(text, pattern)
            
            FOR match IN matches:
                # æå–ä¸»è¯­
                subject = EXTRACT_SUBJECT(match)
                
                # éšæœºé€‰æ‹©ç”Ÿç†ååº”
                reaction = RANDOM_CHOICE(reactions)
                
                # ç”Ÿæˆæ›¿æ¢æ–‡æœ¬
                IF subject:
                    replacement = f"{subject}çš„{reaction}"
                ELSE:
                    replacement = reaction
                END IF
                
                text = REPLACE_FIRST(text, match.full_text, replacement)
                replace_count += 1
            END FOR
        END FOR
    END FOR
    
    RETURN text, replace_count
END FUNCTION

FUNCTION SIMPLIFY_ACTION_CHAINS(text):
    """ç®€åŒ–åŠ¨ä½œé“¾ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    # åŒ¹é…æ¨¡å¼ï¼šè¿ç»­4ä¸ªä»¥ä¸ŠåŠ¨ä½œï¼Œç”¨"ç„¶å/æ¥ç€/ï¼Œ"è¿æ¥
    pattern = r"(ä»–|å¥¹|\\w{2,4})(\\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\\w{2,5})(ï¼Œ|ç„¶å|æ¥ç€)(\\w{2,5})"
    
    matches = FIND_ALL_WITH_CONTEXT(text, pattern)
    simplify_count = 0
    
    FOR match IN matches:
        actions = EXTRACT_ACTIONS(match)  # æå–æ‰€æœ‰åŠ¨ä½œè¯
        
        IF LENGTH(actions) < 4:
            CONTINUE
        END IF
        
        # é€‰æ‹©å…³é”®åŠ¨ä½œï¼ˆé€šå¸¸æ˜¯æœ€åä¸€ä¸ªï¼‰
        key_action = actions[-1]
        
        # æå–ä¸»è¯­
        subject = EXTRACT_SUBJECT(match)
        
        # ç®€åŒ–ä¸ºä¸€ä¸ªåŠ¨ä½œ
        simplified = f"{subject}{key_action}"
        
        text = REPLACE_FIRST(text, match.full_text, simplified)
        simplify_count += 1
    END FOR
    
    RETURN text, simplify_count
END FUNCTION

FUNCTION REMOVE_PARAGRAPH_SUMMARIES(text):
    """åˆ é™¤æ®µè½æ€»ç»“å¥ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    remove_count = 0
    
    summary_patterns = [
        "ä»–æ˜ç™½äº†",
        "ä»–æ„è¯†åˆ°",
        "ä»–å†³å®šäº†",
        "ä»–çŸ¥é“",
        "ä»–æƒ³é€šäº†",
        "ä»–æç„¶å¤§æ‚Ÿ",
        "åŸæ¥å¦‚æ­¤"
    ]
    
    FOR i, para IN ENUMERATE(paragraphs):
        sentences = SPLIT_SENTENCES(para)
        
        IF LENGTH(sentences) == 0:
            CONTINUE
        END IF
        
        last_sent = sentences[-1]
        
        # æ£€æŸ¥æœ€åä¸€å¥æ˜¯å¦åŒ…å«æ€»ç»“æ¨¡å¼
        has_summary = FALSE
        FOR pattern IN summary_patterns:
            IF pattern IN last_sent:
                has_summary = TRUE
                BREAK
            END IF
        END FOR
        
        IF has_summary:
            # åˆ é™¤æœ€åä¸€å¥
            sentences = sentences[:-1]
            paragraphs[i] = JOIN(sentences, "")
            remove_count += 1
        END IF
    END FOR
    
    RETURN JOIN(FILTER(paragraphs, lambda p: LENGTH(p) > 0), "\n\n"), remove_count
END FUNCTION

FUNCTION COMPRESS_LONG_PARAGRAPHS(text, max_length):
    """å‹ç¼©è¿‡é•¿æ®µè½ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    compress_count = 0
    
    FOR i, para IN ENUMERATE(paragraphs):
        IF LENGTH(para) > max_length:
            # æå–æ ¸å¿ƒä¿¡æ¯
            core_info = EXTRACT_CORE_INFO(para)
            
            # é‡å†™ä¸ºç®€çŸ­ç‰ˆæœ¬
            compressed = REWRITE_BRIEFLY(core_info, max_length)
            
            paragraphs[i] = compressed
            compress_count += 1
        END IF
    END FOR
    
    RETURN JOIN(paragraphs, "\n\n"), compress_count
END FUNCTION

FUNCTION REDUCE_INNER_MONOLOGUE(text, max_ratio):
    """å‡å°‘å†…å¿ƒæˆï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    inner_ratio = CALCULATE_INNER_MONOLOGUE_RATIO(text)
    
    IF inner_ratio <= max_ratio:
        RETURN text, 0
    END IF
    
    # æå–æ‰€æœ‰å†…å¿ƒæˆç‰‡æ®µ
    inner_segments = EXTRACT_INNER_MONOLOGUE(text)
    
    # è®¡ç®—éœ€è¦åˆ å‡çš„æ¯”ä¾‹
    reduce_ratio = 1 - (max_ratio / inner_ratio)
    
    # æŒ‰é‡è¦æ€§æ’åºï¼Œåˆ é™¤ä¸é‡è¦çš„
    inner_segments_sorted = SORT_BY_IMPORTANCE(inner_segments)
    segments_to_remove = inner_segments_sorted[:ROUND(LENGTH(inner_segments) * reduce_ratio)]
    
    FOR segment IN segments_to_remove:
        text = REPLACE_FIRST(text, segment.full_text, "")
    END FOR
    
    RETURN CLEAN_WHITESPACE(text), LENGTH(segments_to_remove)
END FUNCTION

FUNCTION REMOVE_REDUNDANCY(text):
    """åˆ é™¤åŒä¸€æ„æ€çš„é‡å¤è¡¨è¾¾ï¼ˆä¿®å¤ç‰ˆï¼šè¯­ä¹‰åˆ¤æ–­ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    remove_count = 0
    
    FOR i, para IN ENUMERATE(paragraphs):
        sentences = SPLIT_SENTENCES(para)
        
        IF LENGTH(sentences) < 3:
            CONTINUE
        END IF
        
        # æ£€æµ‹ç›¸é‚»3å¥æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼
        j = 0
        WHILE j <= LENGTH(sentences) - 3:
            IF ARE_SEMANTICALLY_SIMILAR(sentences[j], sentences[j+1], sentences[j+2]):
                # ä¿ç•™ç¬¬ä¸€å¥ï¼Œåˆ é™¤åä¸¤å¥
                sentences = sentences[:j+1] + sentences[j+3:]
                remove_count += 1
            ELSE:
                j += 1
            END IF
        END WHILE
        
        paragraphs[i] = JOIN(sentences, "")
    END FOR
    
    RETURN JOIN(FILTER(paragraphs, lambda p: LENGTH(p) > 0), "\n\n"), remove_count
END FUNCTION

FUNCTION OPTIMIZE_SENTENCE_LENGTH(text):
    """ä¼˜åŒ–å¥é•¿ï¼ˆé•¿çŸ­å¥äº¤é”™ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    
    FOR i, para IN ENUMERATE(paragraphs):
        sentences = SPLIT_SENTENCES(para)
        
        IF LENGTH(sentences) < 5:
            CONTINUE
        END IF
        
        # æ£€æµ‹è¿ç»­5å¥æ˜¯å¦éƒ½æ˜¯é•¿å¥ï¼ˆ>25å­—ï¼‰æˆ–éƒ½æ˜¯çŸ­å¥ï¼ˆ<10å­—ï¼‰
        FOR j = 0 TO LENGTH(sentences) - 5:
            segment = sentences[j:j+5]
            lengths = MAP(segment, LENGTH)
            
            all_long = ALL(length > 25 FOR length IN lengths)
            all_short = ALL(length < 10 FOR length IN lengths)
            
            IF all_long:
                # å°†ä¸­é—´ä¸€å¥æ‹†åˆ†ä¸ºä¸¤å¥
                middle_idx = j + 2
                sentences[middle_idx] = SPLIT_SENTENCE_AT_COMMA(sentences[middle_idx])
            ELSE IF all_short:
                # å°†ä¸­é—´ä¸¤å¥åˆå¹¶
                sentences[j+2] = sentences[j+2] + sentences[j+3]
                sentences = sentences[:j+3] + sentences[j+4:]
            END IF
        END FOR
        
        paragraphs[i] = JOIN(sentences, "")
    END FOR
    
    RETURN JOIN(paragraphs, "\n\n")
END FUNCTION

FUNCTION REMOVE_EXPLANATORY_THOUGHTS(text):
    """åˆ é™¤è§£é‡Šæ€§å†…å¿ƒæˆï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    explanatory_patterns = [
        "ä»–è¿™æ ·åšæ˜¯å› ä¸º",
        "å¥¹ä¹‹æ‰€ä»¥",
        "è¿™æ˜¯ä¸ºäº†",
        "ç›®çš„æ˜¯",
        "åŸå› åœ¨äº"
    ]
    
    remove_count = 0
    
    FOR pattern IN explanatory_patterns:
        # æ‰¾åˆ°åŒ…å«è¯¥æ¨¡å¼çš„å¥å­
        sentences_with_pattern = FIND_SENTENCES_CONTAINING(text, pattern)
        
        FOR sentence IN sentences_with_pattern:
            text = REPLACE_FIRST(text, sentence, "")
            remove_count += 1
        END FOR
    END FOR
    
    RETURN CLEAN_WHITESPACE(text), remove_count
END FUNCTION

FUNCTION COMPRESS_SETUP(text):
    """å‹ç¼©è¿‡åº¦é“ºå«ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    compress_count = 0
    
    FOR i = 0 TO LENGTH(paragraphs) - 2:
        current = paragraphs[i]
        next = paragraphs[i+1]
        
        # æ£€æµ‹é“ºå«æ¨¡å¼ï¼šå½“å‰æ®µè½>100å­— + ä¸‹ä¸€æ®µè½åŒ…å«åŠ¨ä½œè§¦å‘è¯
        IF LENGTH(current) > 100 AND CONTAINS_ACTION_TRIGGER(next):
            # æå–æ ¸å¿ƒä¿¡æ¯
            core = EXTRACT_CORE_INFO(current)
            
            # å‹ç¼©åˆ°30-50å­—
            compressed = REWRITE_BRIEFLY(core, 50)
            
            paragraphs[i] = compressed
            compress_count += 1
        END IF
    END FOR
    
    RETURN JOIN(paragraphs, "\n\n"), compress_count
END FUNCTION

FUNCTION REMOVE_TRANSITIONS(text):
    """åˆ é™¤è¿‡æ¸¡å¥ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    transition_patterns = [
        r"è¿‡äº†.{1,5}(æ—¶é—´|åˆ†é’Ÿ|å°æ—¶|å¤©|æœˆ)",
        r"(ä»–|å¥¹|\\w{2,4}).{0,5}(èµ°åˆ°|æ¥åˆ°|åˆ°äº†).{2,10}",
        r"ç‰‡åˆ»ä¹‹å",
        r"ä¸å¤šæ—¶",
        r"é¡»è‡¾",
        r"ä¸ä¸€ä¼šå„¿"
    ]
    
    remove_count = 0
    
    FOR pattern IN transition_patterns:
        matches = FIND_ALL_WITH_CONTEXT(text, pattern)
        
        FOR match IN matches:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹çš„è¿‡æ¸¡å¥ï¼ˆå‰åéƒ½æœ‰å¥å·æˆ–æ¢è¡Œï¼‰
            IF IS_STANDALONE_SENTENCE(text, match):
                text = REPLACE_FIRST(text, match.full_sentence, "")
                remove_count += 1
            END IF
        END FOR
    END FOR
    
    RETURN CLEAN_WHITESPACE(text), remove_count
END FUNCTION

FUNCTION REMOVE_CAUSALITY_EXPLANATION(text):
    """åˆ é™¤å› æœè§£é‡Šï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    causality_patterns = [
        r"å› ä¸º.{5,30}æ‰€ä»¥",
        r"ä¹‹æ‰€ä»¥.{5,30}æ˜¯å› ä¸º",
        r"è¿™(æ˜¯|æ ·åš)(æ˜¯)?å› ä¸º",
        r"æ­£(æ˜¯)?ç”±äº"
    ]
    
    remove_count = 0
    
    FOR pattern IN causality_patterns:
        matches = FIND_ALL_WITH_CONTEXT(text, pattern)
        
        FOR match IN matches:
            # ä¿ç•™"æ‰€ä»¥"åé¢çš„ç»“æœéƒ¨åˆ†ï¼Œåˆ é™¤"å› ä¸º"éƒ¨åˆ†
            result_part = EXTRACT_RESULT_CLAUSE(match)
            
            IF result_part:
                text = REPLACE_FIRST(text, match.full_text, result_part)
                remove_count += 1
            END IF
        END FOR
    END FOR
    
    RETURN text, remove_count
END FUNCTION

FUNCTION BREAK_SCENE_STRUCTURE(text):
    """æ‰“ç ´åœºæ™¯å®Œæ•´ç»“æ„ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    scenes = DETECT_SCENES(text)
    break_count = 0
    
    FOR scene IN scenes:
        structure = ANALYZE_SCENE_STRUCTURE(scene)
        
        # å¦‚æœåœºæ™¯æœ‰å®Œæ•´çš„"èµ·æ‰¿è½¬åˆ"
        IF structure.has_all_four_parts AND RANDOM()< 0.3:
            # éšæœºåˆ é™¤"æ‰¿"æˆ–"è½¬"
            part_to_remove = RANDOM_CHOICE(["æ‰¿", "è½¬"])
            
            scene.content = REMOVE_STRUCTURE_PART(scene.content, part_to_remove)
            text = REPLACE_SCENE_IN_TEXT(text, scene)
            break_count += 1
        END IF
    END FOR
    
    RETURN text, break_count
END FUNCTION

FUNCTION ADD_CONVERSATIONAL_ROUGHNESS(text):
    """å¢åŠ å¯¹è¯çš„"ç²—ç³™æ„Ÿ"ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    dialogues = EXTRACT_ALL_DIALOGUES(text)
    modify_count = 0
    
    FOR dialogue IN dialogues:
        # 30%æ¦‚ç‡åšå¤„ç†
        IF RANDOM() < 0.3:
            modification = RANDOM_CHOICE([
                "TRUNCATE",      # è¯´åŠå¥è¯
                "SINGLE_CHAR",   # æ”¹æˆ"å—¯""å•Š""å“¦"
                "ADD_PAUSE"      # æ·»åŠ "â€¦â€¦"
            ])
            
            SWITCH modification:
                CASE "TRUNCATE":
                    # "ä½ è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ" â†’ "ä½ è¿™æ˜¯ä»€ä¹ˆâ€”â€”"
                    IF LENGTH(dialogue.content) > 10:
                        dialogue.content = TRUNCATE_AT_RANDOM(dialogue.content) + "â€”â€”"
                        modify_count += 1
                    END IF
                
                CASE "SINGLE_CHAR":
                    # å®Œæ•´å›ç­” â†’ "å—¯"ï¼ˆä»…å¯¹å›åº”ç±»å¯¹è¯ï¼‰
                    IF LENGTH(dialogue.content) > 10 AND IS_RESPONSE_DIALOGUE(dialogue):
                        dialogue.content = RANDOM_CHOICE(["å—¯", "å•Š", "å“¦", "å‘µ"])
                        modify_count += 1
                    END IF
                
                CASE "ADD_PAUSE":
                    # æ·»åŠ çŠ¹è±«åœé¡¿
                    IF LENGTH(dialogue.content) > 15:
                        dialogue.content = INSERT_PAUSE_RANDOMLY(dialogue.content)
                        modify_count += 1
                    END IF
            END SWITCH
            
            text = REPLACE_DIALOGUE_IN_TEXT(text, dialogue)
        END IF
    END FOR
    
    RETURN text, modify_count
END FUNCTION

FUNCTION COMPRESS_LOW_DENSITY_PARAGRAPHS(text):
    """å‹ç¼©ä½ä¿¡æ¯å¯†åº¦æ®µè½ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    compress_count = 0
    
    FOR i, para IN ENUMERATE(paragraphs):
        IF LENGTH(para) < 100:
            CONTINUE
        END IF
        
        density = CALCULATE_INFO_DENSITY(para)
        
        # ä¿¡æ¯å¯†åº¦<0.005ï¼ˆæ¯200å­—ä¸åˆ°1ä¸ªä¿¡æ¯ç‚¹ï¼‰
        IF density < 0.005:
            # æå–æ ¸å¿ƒä¿¡æ¯
            core = EXTRACT_CORE_INFO(para)
            
            IF LENGTH(core) < 20:
                # æ ¸å¿ƒä¿¡æ¯å¤ªå°‘ï¼Œç›´æ¥åˆ é™¤
                paragraphs[i] = ""
            ELSE:
                # å‹ç¼©åˆ°30-50å­—
                paragraphs[i] = REWRITE_BRIEFLY(core, 50)
            END IF
            
            compress_count += 1
        END IF
    END FOR
    
    RETURN JOIN(FILTER(paragraphs, lambda p: LENGTH(p) > 0), "\n\n"), compress_count
END FUNCTION

FUNCTION FIX_POV_SHIFTS(text, protagonist_name):
    """ä¿®å¤è§†è§’æ¼‚ç§»ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    thought_patterns = [
        r"(\\w{2,4})(æƒ³|è§‰å¾—|è®¤ä¸º|å¿ƒæƒ³)",
        r"(\\w{2,4})çš„(å†…å¿ƒ|å¿ƒé‡Œ)"
    ]
    
    fix_count = 0
    
    FOR pattern IN thought_patterns:
        matches = FIND_ALL_WITH_CONTEXT(text, pattern)
        
        FOR match IN matches:
            character_name = EXTRACT_CHARACTER_NAME(match)
            
            # å¦‚æœä¸æ˜¯ä¸»è§’çš„æƒ³æ³•ï¼Œåˆ é™¤
            IF character_name != protagonist_name AND character_name IS NOT NULL:
                text = REPLACE_FIRST(text, match.full_sentence, "")
                fix_count += 1
            END IF
        END FOR
    END FOR
    
    RETURN CLEAN_WHITESPACE(text), fix_count
END FUNCTION

FUNCTION COMPRESS_OVER_EXPANDED_INFO(text):
    """å‹ç¼©è¿‡åº¦å±•å¼€çš„ä¿¡æ¯ç‚¹ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›è®¡æ•°ï¼‰"""
    info_points = DETECT_INFO_POINTS(text)
    compress_count = 0
    
    FOR point IN info_points:
        IF point.word_count > 150:
            # æå–æ ¸å¿ƒ
            core = EXTRACT_CORE_INFO(point.content)
            
            # å‹ç¼©åˆ°50-80å­—
            compressed = REWRITE_BRIEFLY(core, 80)
            
            text = REPLACE_FIRST(text, point.content, compressed)
            compress_count += 1
        END IF
    END FOR
    
    RETURN text, compress_count
END FUNCTION
```

---

## ã€æ¨¡å—6ã€‘DIAGNOSE_CHAPTER_V3 - å…¨å±€è¯Šæ–­

```python
FUNCTION DIAGNOSE_CHAPTER_V3(chapter_content, parsed_data, monitors):
    """
    å…¨å±€è¯Šæ–­ï¼ˆv3ä¿®å¤ç‰ˆï¼‰
    è¾“å‡ºåˆ†å±‚æŠ¥å‘Šï¼Œå¿«é€Ÿå®šä½é—®é¢˜
    """
    
    diagnosis = {
        "passed": TRUE,
        "has_critical_issues": FALSE,
        "summary": {},      # å¿«é€Ÿæ‘˜è¦
        "critical": [],     # å¿…é¡»ä¿®å¤
        "warnings": [],     # å»ºè®®ä¼˜åŒ–
        "details": {},      # è¯¦ç»†æ•°æ®
        "forced_delivery": FALSE
    }
    
    # ========== è®¡ç®—ç»Ÿè®¡æ•°æ® ==========
    stats = {
        "word_count": LENGTH(chapter_content),
        "dialogue_ratio": CALCULATE_DIALOGUE_RATIO(chapter_content),
        "inner_ratio": CALCULATE_INNER_MONOLOGUE_RATIO(chapter_content),
        "paragraph_count": COUNT_PARAGRAPHS(chapter_content),
        "avg_para_length": AVG_PARAGRAPH_LENGTH(chapter_content),
        "info_density": CALCULATE_INFO_DENSITY(chapter_content),
        "disease_score": CALCULATE_DISEASE_SCORE(chapter_content, parsed_data),
        "scene_checks": monitors.scene_checks
    }
    
    # ========== å¿«é€Ÿæ‘˜è¦ ==========
    quality_score = CALCULATE_QUALITY_SCORE(stats)
    
    diagnosis.summary = {
        "status": "âœ… é€šè¿‡" IF diagnosis.passed ELSE "âŒ æœªé€šè¿‡",
        "word_count": f"{stats.word_count}å­—",
        "quality_score": f"{quality_score}/100"
    }
    
    # ========== å…³é”®æ£€æŸ¥ ==========
    
    # æ£€æŸ¥1ï¼šå­—æ•°èŒƒå›´ï¼ˆç¡¬çº¦æŸï¼‰
    IF stats.word_count < WORD_COUNT_MIN:
        diagnosis.critical.APPEND({
            "type": "WORD_COUNT",
            "issue": f"å­—æ•°ä¸è¶³ï¼ˆ{stats.word_count}å­—ï¼Œæœ€ä½{WORD_COUNT_MIN}å­—ï¼‰",
            "fix": "éœ€è¦æ‰©å……å…³é”®æƒ…èŠ‚æˆ–å¢åŠ å†²çª",
            "severity": 10
        })
        diagnosis.passed = FALSE
        diagnosis.has_critical_issues = TRUE
    
    ELSE IF stats.word_count > WORD_COUNT_MAX:
        diagnosis.critical.APPEND({
            "type": "WORD_COUNT",
            "issue": f"å­—æ•°è¶…æ ‡ï¼ˆ{stats.word_count}å­—ï¼Œä¸Šé™{WORD_COUNT_MAX}å­—ï¼‰",
            "fix": "éœ€è¦åˆ å‡æ¬¡è¦æƒ…èŠ‚æˆ–å‹ç¼©æå†™",
            "severity": 10
        })
        diagnosis.passed = FALSE
        diagnosis.has_critical_issues = TRUE
    END IF
    
    # æ£€æŸ¥2ï¼šæ ¸å¿ƒä»»åŠ¡ï¼ˆå¿…é¡»å®Œæˆï¼‰
    IF "core_mission" IN parsed_data.goals:
        core_mission = parsed_data.goals.core_mission
        
        IF NOT CHECK_MISSION_COMPLETED(chapter_content, core_mission, parsed_data):
            diagnosis.critical.APPEND({
                "type": "MISSION",
                "issue": f"æ ¸å¿ƒä»»åŠ¡æœªå®Œæˆï¼š{core_mission}",
                "fix": "å¿…é¡»é‡å†™æˆ–è¡¥å……å…³é”®æƒ…èŠ‚",
                "severity": 10
            })
            diagnosis.passed = FALSE
            diagnosis.has_critical_issues = TRUE
        END IF
    END IF
    
    # æ£€æŸ¥3ï¼šçº¢çº¿è¿åï¼ˆé›¶å®¹å¿ï¼‰
    IF "redlines" IN parsed_data.constraints:
        FOR redline IN parsed_data.constraints.redlines:
            IF CHECK_REDLINE_VIOLATION(chapter_content, redline):
                diagnosis.critical.APPEND({
                    "type": "REDLINE",
                    "issue": f"è¿åçº¢çº¿ï¼š{redline}",
                    "fix": "å¿…é¡»åˆ é™¤è¿è§„å†…å®¹",
                    "severity": 15  # æœ€é«˜ä¸¥é‡åº¦
                })
                diagnosis.passed = FALSE
                diagnosis.has_critical_issues = TRUE
            END IF
        END FOR
    END IF
    
    # æ£€æŸ¥4ï¼šå¿…é¡»åŸ‹çš„é’©å­ï¼ˆé‡è¦ä½†éå…³é”®ï¼‰
    IF "hooks_to_plant" IN parsed_data.goals:
        FOR hook IN parsed_data.goals.hooks_to_plant:
            IF NOT CHECK_HOOK_PLANTED(chapter_content, hook):
                diagnosis.warnings.APPEND({
                    "type": "HOOK",
                    "issue": f"é’©å­æœªåŸ‹ï¼š{hook}",
                    "fix": "å»ºè®®è¡¥å……ä¼ç¬”"
                })
            END IF
        END FOR
    END IF
    
    # ========== è­¦å‘Šé¡¹ ==========
    
    # è­¦å‘Š1ï¼šå¯¹è¯å æ¯”
    dialogue_ratio = stats.dialogue_ratio
    target_range = TOMATO_CORE_RULES.dialogue_ratio
    
    IF dialogue_ratio < target_range[0] * 0.8 OR dialogue_ratio > target_range[1] * 1.2:
        diagnosis.warnings.APPEND({
            "type": "DIALOGUE_RATIO",
            "issue": f"å¯¹è¯å æ¯”åç¦»ç›®æ ‡ï¼ˆå½“å‰{dialogue_ratio*100:.1f}%ï¼Œç›®æ ‡{target_range[0]*100}-{target_range[1]*100}%ï¼‰",
            "fix": "è°ƒæ•´å¯¹è¯/æå†™æ¯”ä¾‹"
        })
    END IF
    
    # è­¦å‘Š2ï¼šClaudeç—…ç—‡
    IF stats.disease_score > 30:  # åŠ æƒåˆ†æ•°é˜ˆå€¼
        diagnosis.warnings.APPEND({
            "type": "DISEASE",
            "issue": f"Claudeç—…ç—‡åŠ æƒåˆ†æ•°è¿‡é«˜ï¼ˆ{stats.disease_score:.1f}åˆ†ï¼‰",
            "fix": "å·²è‡ªåŠ¨æ¶¦è‰²ï¼Œä½†å»ºè®®äººå·¥å¤æŸ¥"
        })
    END IF
    
    # è­¦å‘Š3ï¼šä¿¡æ¯å¯†åº¦
    IF stats.info_density < TOMATO_CORE_RULES.info_density_min:
        diagnosis.warnings.APPEND({
            "type": "INFO_DENSITY",
            "issue": f"ä¿¡æ¯å¯†åº¦è¿‡ä½ï¼ˆ{stats.info_density:.3f}ï¼Œæœ€ä½{TOMATO_CORE_RULES.info_density_min}ï¼‰",
            "fix": "å‹ç¼©æå†™æˆ–å¢åŠ ä¿¡æ¯ç‚¹"
        })
    END IF
    
    # è­¦å‘Š4ï¼šå†…å¿ƒæˆè¿‡å¤š
    IF stats.inner_ratio > TOMATO_CORE_RULES.inner_monologue_max * 1.2:
        diagnosis.warnings.APPEND({
            "type": "INNER_MONOLOGUE",
            "issue": f"å†…å¿ƒæˆå æ¯”è¿‡é«˜ï¼ˆ{stats.inner_ratio*100:.1f}%ï¼Œä¸Šé™{TOMATO_CORE_RULES.inner_monologue_max*100}%ï¼‰",
            "fix": "å·²è‡ªåŠ¨å‹ç¼©ï¼Œè¯·å¤æŸ¥"
        })
    END IF
    
    # è­¦å‘Š5ï¼šæ®µè½è¿‡é•¿
    IF stats.avg_para_length > TOMATO_CORE_RULES.paragraph_max_length * 1.2:
        diagnosis.warnings.APPEND({
            "type": "PARAGRAPH_LENGTH",
            "issue": f"å¹³å‡æ®µè½é•¿åº¦è¿‡é•¿ï¼ˆ{stats.avg_para_length:.0f}å­—ï¼Œä¸Šé™{TOMATO_CORE_RULES.paragraph_max_length}å­—ï¼‰",
            "fix": "å·²è‡ªåŠ¨å‹ç¼©ï¼Œè¯·å¤æŸ¥"
        })
    END IF
    
    # ========== è¯¦ç»†æ•°æ® ==========
    diagnosis.details = stats
    
    RETURN diagnosis
END FUNCTION

FUNCTION CALCULATE_QUALITY_SCORE(stats):
    """
    è®¡ç®—ç»¼åˆè´¨é‡åˆ†ï¼ˆ0-100ï¼‰
    """
    score = 100
    
    # å­—æ•°ï¼šåœ¨ç›®æ ‡èŒƒå›´å†…+10ï¼Œä¸¥é‡åç¦»-20
    word_count = stats.word_count
    IF word_count < WORD_COUNT_MIN OR word_count > WORD_COUNT_MAX:
        score -= 20
    ELSE IF ABS(word_count - WORD_COUNT_TARGET) < 500:
        score += 10
    ELSE:
        score -= 5
    END IF
    
    # å¯¹è¯å æ¯”ï¼šåœ¨ç›®æ ‡èŒƒå›´å†…+15
    dialogue_ratio = stats.dialogue_ratio
    target_range = TOMATO_CORE_RULES.dialogue_ratio
    
    IF dialogue_ratio >= target_range[0] AND dialogue_ratio <= target_range[1]:
        score += 15
    ELSE IF dialogue_ratio >= target_range[0] * 0.8 AND dialogue_ratio <= target_range[1] * 1.2:
        score += 5
    ELSE:
        score -= 10
    END IF
    
    # ä¿¡æ¯å¯†åº¦ï¼šè¾¾æ ‡+15
    IF stats.info_density >= TOMATO_CORE_RULES.info_density_min:
        score += 15
    ELSE:
        score -= 10
    END IF
    
    # Claudeç—…ç—‡ï¼šæ¯åˆ†-0.5åˆ†ï¼ˆåŠ æƒåï¼‰
    score -= stats.disease_score * 0.5
    
    # æ®µè½é•¿åº¦ï¼šå¹³å‡<150å­—+10åˆ†
    IF stats.avg_para_length <= TOMATO_CORE_RULES.paragraph_max_length:
        score += 10
    ELSE:
        score -= 5
    END IF
    
    # å†…å¿ƒæˆï¼šä¸è¶…æ ‡+10åˆ†
    IF stats.inner_ratio <= TOMATO_CORE_RULES.inner_monologue_max:
        score += 10
    ELSE:
        score -= 10
    END IF
    
    RETURN CLAMP(score, 0, 100)
END FUNCTION

FUNCTION CALCULATE_DISEASE_SCORE(text, parsed_data):
    """
    è®¡ç®—Claudeç—…ç—‡åŠ æƒåˆ†æ•°
    """
    disease_score = 0
    
    # D1: è¿‡åº¦å±•å¼€
    over_expanded = COUNT_OVER_EXPANDED_INFO(text, 150)
    disease_score += over_expanded * DISEASE_WEIGHTS["D1_è¿‡åº¦å±•å¼€"]
    
    # D2: è¿‡åº¦è§£é‡Š
    explain_patterns = ["è¿™æ˜¯å› ä¸º", "ä¹‹æ‰€ä»¥", "åŸå› åœ¨äº"]
    FOR pattern IN explain_patterns:
        disease_score += COUNT_OCCURRENCES(text, pattern) * DISEASE_WEIGHTS["D2_è¿‡åº¦è§£é‡Š"]
    END FOR
    
    # D3: è¿‡åº¦å†…å¿ƒæˆ
    inner_ratio = CALCULATE_INNER_MONOLOGUE_RATIO(text)
    IF inner_ratio > TOMATO_CORE_RULES.inner_monologue_max:
        excess = (inner_ratio - TOMATO_CORE_RULES.inner_monologue_max) * 100
        disease_score += excess * DISEASE_WEIGHTS["D3_è¿‡åº¦å†…å¿ƒæˆ"]
    END IF
    
    # D4: è¿‡åº¦å®‰å…¨
    hedging = ["å¯ä»¥è¯´", "åŸºæœ¬ä¸Š", "åœ¨æŸç§æ„ä¹‰ä¸Š"]
    FOR phrase IN hedging:
        disease_score += COUNT_OCCURRENCES(text, phrase) * DISEASE_WEIGHTS["D4_è¿‡åº¦å®‰å…¨"]
    END FOR
    
    # D5: è¿‡åº¦å†—ä½™
    disease_score += DETECT_REPETITION_COUNT(text) * DISEASE_WEIGHTS["D5_è¿‡åº¦å†—ä½™"]
    
    # D7: è¿‡åº¦é€»è¾‘
    causality = ["å› ä¸º", "æ‰€ä»¥", "ç”±äº", "å¯¼è‡´"]
    FOR word IN causality:
        disease_score += COUNT_OCCURRENCES(text, word) * DISEASE_WEIGHTS["D7_è¿‡åº¦é€»è¾‘"]
    END FOR
    
    # D8: è¿‡åº¦å®Œæ•´
    disease_score += COUNT_COMPLETE_STRUCTURES(text) * DISEASE_WEIGHTS["D8_è¿‡åº¦å®Œæ•´"]
    
    # D11: ä¿¡æ¯å¯†åº¦ä¸è¶³
    low_density_paras = FIND_LOW_DENSITY_PARAGRAPHS(text, 0.005)
    disease_score += LENGTH(low_density_paras) * DISEASE_WEIGHTS["D11_ä¿¡æ¯å¯†åº¦ä¸è¶³"]
    
    # D12: å£æ°´è¯
    filler_words = ["ä¼¼ä¹", "ä»¿ä½›", "æˆ–è®¸", "å¯èƒ½", "å¤§æ¦‚", "å¥½åƒ"]
    FOR word IN filler_words:
        disease_score += COUNT_OCCURRENCES(text, word) * DISEASE_WEIGHTS["D12_å£æ°´è¯æ»¥ç”¨"]
    END FOR
    
    # D13: å¥—è·¯è½¬æŠ˜
    cliche_words = ["ç„¶è€Œ", "ä½†æ˜¯", "å°±åœ¨è¿™æ—¶", "çªç„¶", "å¿½ç„¶"]
    FOR word IN cliche_words:
        disease_score += COUNT_OCCURRENCES(text, word) * DISEASE_WEIGHTS["D13_å¥—è·¯åŒ–è½¬æŠ˜"]
    END FOR
    
    # D14: è¿‡åº¦é“ºå«
    disease_score += DETECT_OVER_SETUP_COUNT(text) * DISEASE_WEIGHTS["D14_è¿‡åº¦é“ºå«"]
    
    # D15: ç»“å°¾æ€»ç»“ç™–
    summary_patterns = ["ä»–æ˜ç™½äº†", "ä»–æ„è¯†åˆ°", "ä»–å†³å®šäº†"]
    FOR pattern IN summary_patterns:
        disease_score += COUNT_OCCURRENCES(text, pattern) * DISEASE_WEIGHTS["D15_ç»“å°¾æ€»ç»“ç™–"]
    END FOR
    
    # D16: ä¸æ•¢ç•™ç™½
    hint_patterns = ["ä¼¼ä¹é¢„ç¤ºç€", "å¯èƒ½ä¼š", "æˆ–è®¸æ„å‘³ç€"]
    FOR pattern IN hint_patterns:
        disease_score += COUNT_OCCURRENCES(text, pattern) * DISEASE_WEIGHTS["D16_ä¸æ•¢ç•™ç™½"]
    END FOR
    
    # D17: ä¸æ•¢è·³è·ƒ
    transition_phrases = ["è¿‡äº†", "æ¥åˆ°", "èµ°åˆ°", "ç‰‡åˆ»ä¹‹å"]
    FOR phrase IN transition_phrases:
        disease_score += COUNT_OCCURRENCES(text, phrase) * DISEASE_WEIGHTS["D17_ä¸æ•¢è·³è·ƒ"]
    END FOR
    
    # D18: æƒ…ç»ªè¯æ»¥ç”¨
    emotion_words = ["éœ‡æƒŠ", "ææƒ§", "æ„¤æ€’", "ç„¦è™‘", "ç´§å¼ "]
    FOR word IN emotion_words:
        disease_score += COUNT_OCCURRENCES(text, word) * DISEASE_WEIGHTS["D18_æƒ…ç»ªè¯æ»¥ç”¨"]
    END FOR
    
    # D19: åŠ¨ä½œé“¾
    action_chains = DETECT_ACTION_CHAINS(text)
    long_chains = FILTER(action_chains, lambda chain: LENGTH(chain) > 4)
    disease_score += LENGTH(long_chains) * DISEASE_WEIGHTS["D19_åŠ¨ä½œé“¾å®Œæ•´ç™–"]
    
    # D20: è§†è§’æ¼‚ç§»
    protagonist_name = parsed_data.characters.protagonist.GET("name", "ä¸»è§’")
    disease_score += COUNT_POV_SHIFTS(text, protagonist_name) * DISEASE_WEIGHTS["D20_è§†è§’æ¼‚ç§»"]
    
    RETURN disease_score
END FUNCTION
```

---

## ã€æ¨¡å—7ã€‘PROBLEM_ANALYSIS - é—®é¢˜åˆ†æ

```python
FUNCTION ANALYZE_WHAT_WENT_WRONG(diagnosis, parsed_data, chapter_content):
    """
    åˆ†æè¯Šæ–­å¤±è´¥çš„æ ¹æœ¬åŸå› ï¼ˆä¿®å¤ç‰ˆï¼šå¢åŠ ä¸Šä¸‹æ–‡ï¼‰
    é¿å…ç›²ç›®é‡å†™
    """
    
    analysis = {
        "root_cause": [],
        "affected_scenes": [],
        "fix_strategy": []
    }
    
    # ========== åˆ†æCriticalé—®é¢˜ ==========
    FOR issue IN diagnosis.critical:
        SWITCH issue.type:
            CASE "WORD_COUNT":
                IF "ä¸è¶³" IN issue.issue:
                    analysis.root_cause.APPEND("å†…å®¹é‡ä¸è¶³")
                    analysis.fix_strategy.APPEND("å±•å¼€é‡è¦æƒ…èŠ‚è‡³200-300å­—/åœºæ™¯")
                    analysis.fix_strategy.APPEND("å¢åŠ 1-2ä¸ªå†²çªåœºæ™¯")
                ELSE:
                    analysis.root_cause.APPEND("å†…å®¹è¿‡åº¦å±•å¼€")
                    analysis.fix_strategy.APPEND("å‹ç¼©æ¬¡è¦åœºæ™¯è‡³300-500å­—")
                    analysis.fix_strategy.APPEND("åˆ é™¤ä¸å¿…è¦çš„æå†™")
                END IF
            
            CASE "MISSION":
                analysis.root_cause.APPEND("é—æ¼å…³é”®æƒ…èŠ‚")
                
                # æ‰¾åˆ°æ ¸å¿ƒä»»åŠ¡ç›¸å…³çš„å…³é”®è¯
                mission = parsed_data.goals.core_mission
                mission_keywords = EXTRACT_KEYWORDS(mission)
                
                # æ£€æŸ¥å“ªäº›åœºæ™¯ç¼ºå°‘è¿™äº›å…³é”®è¯
                missing_scenes = FIND_SCENES_MISSING_KEYWORDS(chapter_content, mission_keywords)
                analysis.affected_scenes.EXTEND(missing_scenes)
                
                analysis.fix_strategy.APPEND(f"åœ¨åœºæ™¯{missing_scenes}è¡¥å……æ ¸å¿ƒä»»åŠ¡ç›¸å…³æƒ…èŠ‚")
            
            CASE "REDLINE":
                analysis.root_cause.APPEND("é€»è¾‘é”™è¯¯æˆ–ç†è§£åå·®")
                
                # ä»ç« èŠ‚å†…å®¹ä¸­æ‰¾åˆ°è¿è§„ä½ç½®
                redline_text = issue.issue.REPLACE("è¿åçº¢çº¿ï¼š", "")
                violation_location = FIND_VIOLATION_LOCATION(chapter_content, redline_text)
                
                IF violation_location:
                    analysis.affected_scenes.APPEND(violation_location.scene_idx)
                    analysis.fix_strategy.APPEND(f"åˆ é™¤åœºæ™¯{violation_location.scene_idx}ç¬¬{violation_location.para_idx}æ®µçš„è¿è§„å†…å®¹")
                ELSE:
                    analysis.fix_strategy.APPEND("å…¨æ–‡æ£€æŸ¥å¹¶åˆ é™¤è¿è§„å†…å®¹")
                END IF
            
            CASE "HOOK":
                analysis.root_cause.APPEND("é—æ¼ä¼ç¬”")
                analysis.fix_strategy.APPEND("åœ¨é€‚å½“åœºæ™¯è¡¥å……ä¼ç¬”")
        END SWITCH
    END FOR
    
    # ========== åˆ†æWarningï¼ˆè¾…åŠ©åˆ¤æ–­ï¼‰==========
    warning_types = MAP(diagnosis.warnings, lambda w: w.type)
    
    IF "DIALOGUE_RATIO" IN warning_types:
        analysis.root_cause.APPEND("å¯¹è¯å æ¯”å¤±è¡¡")
        analysis.fix_strategy.APPEND("å¢åŠ /å‡å°‘å¯¹è¯è‡³35%-45%")
    END IF
    
    IF "DISEASE" IN warning_types:
        analysis.root_cause.APPEND("Claudeç—…ç—‡ä¸¥é‡")
        analysis.fix_strategy.APPEND("å†™ä½œæ—¶æ›´æ³¨é‡Showè€ŒéTell")
    END IF
    
    IF "INFO_DENSITY" IN warning_types:
        analysis.root_cause.APPEND("ä¿¡æ¯å¯†åº¦ä¸è¶³")
        analysis.fix_strategy.APPEND("å‹ç¼©æå†™æˆ–å¢åŠ ä¿¡æ¯ç‚¹")
    END IF
    
    # ========== å»é‡ ==========
    analysis.root_cause = UNIQUE(analysis.root_cause)
    analysis.affected_scenes = UNIQUE(analysis.affected_scenes)
    analysis.fix_strategy = UNIQUE(analysis.fix_strategy)
    
    PRINT "[ANALYSIS] é—®é¢˜æ ¹æº: {JOIN(analysis.root_cause, ', ')}"
    PRINT "[ANALYSIS] å—å½±å“åœºæ™¯: {analysis.affected_scenes}"
    PRINT "[ANALYSIS] ä¿®å¤ç­–ç•¥: {LENGTH(analysis.fix_strategy)}é¡¹"
    
    RETURN analysis
END FUNCTION

FUNCTION GENERATE_FIX_INSTRUCTION(problem_analysis):
    """
    æ ¹æ®é—®é¢˜åˆ†æç”Ÿæˆä¿®æ­£æŒ‡ä»¤ï¼ˆä¿®å¤ç‰ˆï¼šç»“æ„åŒ–ï¼‰
    """
    
    fix_instruction = {
        "focus_scenes": problem_analysis.affected_scenes,
        "must_do": [],
        "must_avoid": [],
        "target_adjustments": {}
    }
    
    # ========== å¤„ç†æ ¹æœ¬åŸå›  ==========
    FOR cause IN problem_analysis.root_cause:
        SWITCH cause:
            CASE "å†…å®¹é‡ä¸è¶³":
                fix_instruction.must_do.APPEND("å±•å¼€é‡è¦æƒ…èŠ‚è‡³200-300å­—")
                fix_instruction.must_do.APPEND("å¢åŠ 1-2ä¸ªå†²çªåœºæ™¯")
                fix_instruction.target_adjustments["min_scene_words"] = 400
            
            CASE "å†…å®¹è¿‡åº¦å±•å¼€":
                fix_instruction.must_do.APPEND("å‹ç¼©æ¬¡è¦åœºæ™¯è‡³300-500å­—")
                fix_instruction.must_avoid.APPEND("è¿‡åº¦æå†™ç¯å¢ƒå’Œç»†èŠ‚")
                fix_instruction.target_adjustments["max_scene_words"] = 800
            
            CASE "é—æ¼å…³é”®æƒ…èŠ‚":
                fix_instruction.must_do.APPEND("è¡¥å……æ ¸å¿ƒä»»åŠ¡ç›¸å…³åœºæ™¯")
                fix_instruction.must_do.APPEND("ç¡®ä¿æ ¸å¿ƒä»»åŠ¡åœ¨æ–‡ä¸­æ˜ç¡®ä½“ç°")
            
            CASE "é€»è¾‘é”™è¯¯":
                fix_instruction.must_avoid.APPEND("è¿åçº¢çº¿çš„å†…å®¹")
                fix_instruction.must_do.APPEND("é‡æ–°æ£€æŸ¥é€»è¾‘åˆç†æ€§")
            
            CASE "å¯¹è¯å æ¯”å¤±è¡¡":
                fix_instruction.target_adjustments["dialogue_ratio"] = [0.35, 0.45]
                fix_instruction.must_do.APPEND("è°ƒæ•´å¯¹è¯/æå†™æ¯”ä¾‹")
            
            CASE "Claudeç—…ç—‡ä¸¥é‡":
                fix_instruction.must_avoid.APPEND("å£æ°´è¯ã€å¥—è·¯è½¬æŠ˜ã€æƒ…ç»ªè¯ç›´å†™")
                fix_instruction.must_do.APPEND("ç”¨Showä»£æ›¿Tell")
            
            CASE "ä¿¡æ¯å¯†åº¦ä¸è¶³":
                fix_instruction.must_do.APPEND("å¢åŠ ä¿¡æ¯ç‚¹å¯†åº¦è‡³0.01/100å­—")
                fix_instruction.must_avoid.APPEND("ç©ºæ´çš„æå†™å’Œé“ºå«")
        END SWITCH
    END FOR
    
    # ========== åº”ç”¨ä¿®å¤ç­–ç•¥ ==========
    FOR strategy IN problem_analysis.fix_strategy:
        IF strategy NOT IN fix_instruction.must_do:
            fix_instruction.must_do.APPEND(strategy)
        END IF
    END FOR
    
    PRINT "[FIX] ç”Ÿæˆä¿®æ­£æŒ‡ä»¤: {LENGTH(fix_instruction.must_do)}é¡¹å¿…åšï¼Œ{LENGTH(fix_instruction.must_avoid)}é¡¹ç¦æ­¢"
    
    RETURN fix_instruction
END FUNCTION
```

---

## ã€æ¨¡å—8ã€‘EMOTION_WRITER_V3 - æƒ…ç»ªå†™ä½œå™¨

```python
FUNCTION WRITE_EMOTION(unit, expansion_level, parsed_data):
    """
    å†™æƒ…ç»ªå•å…ƒï¼ˆv3ä¿®å¤ç‰ˆï¼‰
    æ ¹æ®expansion_levelåŠ¨æ€Show/Tell
    """
    
    emotion_text = ""
    emotion_type = unit.GET("emotion_type", "ç„¦è™‘")
    emotion_intensity = unit.GET("intensity", 60)
    
    # ========== æ ¹æ®expansion_levelå†³å®šShow/Tell ==========
    SWITCH expansion_level:
        CASE "EXPAND":
            # è¯¦å†™ï¼šç”Ÿç†ååº” + ç®€çŸ­å†…å¿ƒæˆ
            micro_expressions = GENERATE_MICRO_EXPRESSIONS(emotion_type, emotion_intensity)
            emotion_text = WRITE_MICRO_EXPRESSIONS(micro_expressions)
            
            # åŠ ä¸€å¥å†…å¿ƒæˆï¼ˆ<30å­—ï¼‰
            IF "active_thought" IN parsed_data.emotions.GET("protagonist", {}):
                inner = parsed_data.emotions.protagonist.active_thought
                IF LENGTH(inner) > 0:
                    emotion_text += f""{inner[:30]}""
                END IF
            END IF
        
        CASE "BRIEF":
            # ç®€å†™ï¼šåªå†™ç”Ÿç†ååº”ï¼ˆå–2ä¸ªï¼‰
            micro_expressions = GENERATE_MICRO_EXPRESSIONS(emotion_type, emotion_intensity)
            emotion_text = WRITE_MICRO_EXPRESSIONS(micro_expressions[:2])
        
        CASE "SKIP":
            # è·³è¿‡ï¼šä¸å†™æƒ…ç»ª
            emotion_text = ""
    END SWITCH
    
    RETURN emotion_text
END FUNCTION

FUNCTION GENERATE_MICRO_EXPRESSIONS(emotion_type, intensity):
    """ç”Ÿæˆç”Ÿç†ååº”ï¼ˆä¿®å¤ç‰ˆï¼šè¿”å›åˆ—è¡¨ï¼‰"""
    emotion_map = {
        "ç„¦è™‘": ["èƒƒéƒ¨æ”¶ç´§", "å‘¼å¸å˜æµ…", "æ‰‹å¿ƒå†’æ±—", "è‚©è†€ç´§ç»·"],
        "ææƒ§": ["åèƒŒå‘å‡‰", "è…¿å‘è½¯", "ç³å­”æ”¶ç¼©", "å¿ƒè·³éª¤åœ"],
        "æ„¤æ€’": ["å¤ªé˜³ç©´è·³åŠ¨", "æ‹³å¤´æ”¥ç´§", "è„¸é¢Šå‘çƒ«", "ç‰™å…³ç´§å’¬"],
        "å…´å¥‹": ["å¿ƒè·³åŠ å¿«", "å‘¼å¸æ€¥ä¿ƒ", "çœ¼ç›å‘äº®", "è¡€æ¶²ä¸Šæ¶Œ"],
        "æ‚²ä¼¤": ["é¼»è…”å‘é…¸", "çœ¼çœ¶å‘çƒ­", "å–‰å¤´å‘ç´§", "èƒ¸å£å‘é—·"],
        "ç´§å¼ ": ["å–‰å’™å‘å¹²", "å¿ƒè·³åŠ é€Ÿ", "è‚Œè‚‰ç´§ç»·", "æ‰‹æŒ‡é¢¤æŠ–"],
        "å›°æƒ‘": ["çœ‰å¤´ç´§é”", "ç›®å…‰æ¸¸ç§»", "å˜´å”‡å¾®å¼ ", "èº«ä½“åƒµä½"],
        "æƒŠè®¶": ["ç³å­”æ”¾å¤§", "å‘¼å¸ä¸€çª’", "èº«ä½“åä»°", "çœ¼ç›çªåœ†"]
    }
    
    reactions = emotion_map.GET(emotion_type, ["èº«ä½“ç´§ç»·", "å‘¼å¸ä¸ç¨³"])
    
    # æ ¹æ®å¼ºåº¦é€‰æ‹©æ•°é‡
    IF intensity > 70:
        RETURN reactions  # å…¨éƒ¨
    ELSE IF intensity > 40:
        RETURN reactions[:3]  # å‰3ä¸ª
    ELSE:
        RETURN reactions[:1]  # å‰1ä¸ª
    END IF
END FUNCTION

FUNCTION WRITE_MICRO_EXPRESSIONS(reactions):
    """å°†ç”Ÿç†ååº”å†™æˆæ–‡æœ¬"""
    IF LENGTH(reactions) == 0:
        RETURN ""
    END IF
    
    IF LENGTH(reactions) == 1:
        RETURN f"ä»–çš„{reactions[0]}ã€‚"
    ELSE IF LENGTH(reactions) == 2:
        RETURN f"ä»–çš„{reactions[0]}ï¼Œ{reactions[1]}ã€‚"
    ELSE:
        # 3ä¸ªä»¥ä¸Šï¼Œç”¨é¡¿å·è¿æ¥
        all_but_last = JOIN(reactions[:-1], "ã€")
        RETURN f"ä»–çš„{all_but_last}ï¼Œ{reactions[-1]}ã€‚"
    END IF
END FUNCTION
```

---

## ã€æ¨¡å—9ã€‘DELIVERY_V3 - äº¤ä»˜åè®®

```python
FUNCTION DELIVER_OUTPUT_V3(chapter_content, new_facts, diagnosis, monitors):
    """
    äº¤ä»˜è¾“å‡ºï¼ˆv3ä¿®å¤ç‰ˆï¼šç»“æ„åŒ–è¾“å‡ºï¼‰
    """
    
    output = {}
    
    # ========== è¾“å‡º1ï¼šç« èŠ‚æ­£æ–‡ ==========
    output["chapter_content"] = chapter_content
    
    # ========== è¾“å‡º2ï¼šå¿«é€Ÿæ‘˜è¦ ==========
    status_emoji = "âœ…" IF diagnosis.passed ELSE "âŒ"
    forced_note = " [å¼ºåˆ¶äº¤ä»˜]" IF diagnosis.forced_delivery ELSE ""
    
    output["summary"] = f"""
## å¿«é€Ÿæ‘˜è¦
{status_emoji} {diagnosis.summary.status}{forced_note}
- å­—æ•°: {diagnosis.summary.word_count}
- è´¨é‡åˆ†: {diagnosis.summary.quality_score}
"""
    
    # ========== è¾“å‡º3ï¼šé—®é¢˜æ¸…å•ï¼ˆå¦‚æœæœ‰ï¼‰==========
    IF LENGTH(diagnosis.critical) > 0:
        output["summary"] += "\n### âš ï¸ å¿…é¡»ä¿®å¤\n"
        FOR issue IN diagnosis.critical:
            output["summary"] += f"- **[{issue.type}]** {issue.issue}\n"
            output["summary"] += f"  ä¿®å¤: {issue.fix}\n"
        END FOR
    END IF
    
    IF LENGTH(diagnosis.warnings) > 0:
        output["summary"] += "\n### ğŸ’¡ å»ºè®®ä¼˜åŒ–\n"
        FOR warning IN diagnosis.warnings:
            output["summary"] += f"- **[{warning.type}]** {warning.issue}\n"
        END FOR
    END IF
    
    # ========== è¾“å‡º4ï¼šæ–°å¢Factæ¸…å• ==========
    output["new_facts"] = FORMAT_FACTS_LIST(new_facts)
    
    # ========== è¾“å‡º5ï¼šè¯¦ç»†è¯Šæ–­ï¼ˆå¯æŠ˜å ï¼‰==========
    details_content = f"""
- æ€»å­—æ•°: {diagnosis.details.word_count}
- å¯¹è¯å æ¯”: {diagnosis.details.dialogue_ratio * 100:.1f}% (ç›®æ ‡{TOMATO_CORE_RULES.dialogue_ratio[0]*100}-{TOMATO_CORE_RULES.dialogue_ratio[1]*100}%)
- å†…å¿ƒæˆå æ¯”: {diagnosis.details.inner_ratio * 100:.1f}% (ä¸Šé™{TOMATO_CORE_RULES.inner_monologue_max*100}%)
- æ®µè½æ•°: {diagnosis.details.paragraph_count}
- å¹³å‡æ®µè½é•¿åº¦: {diagnosis.details.avg_para_length:.0f}å­— (ä¸Šé™{TOMATO_CORE_RULES.paragraph_max_length}å­—)
- ä¿¡æ¯å¯†åº¦: {diagnosis.details.info_density:.3f} (æœ€ä½{TOMATO_CORE_RULES.info_density_min})
- Claudeç—…ç—‡åŠ æƒåˆ†: {diagnosis.details.disease_score:.1f}

### åœºæ™¯æ£€æŸ¥è®°å½•
"""
    
    FOR check IN monitors.scene_checks:
        status_icon = "âœ…" IF check.severity == "OK" ELSE ("âš ï¸" IF check.severity == "WARNING" ELSE "âŒ")
        details_content += f"- åœºæ™¯{check.scene_idx}: {status_icon} {check.word_count}å­—"
        
        IF check.issue:
            details_content += f" ({check.issue})"
        END IF
        
        details_content += "\n"
    END FOR
    
    output["details"] = f"""
<details>
<summary>ğŸ“Š è¯¦ç»†ç»Ÿè®¡ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

{details_content}
</details>
"""
    
    RETURN output
END FUNCTION
```

---

## ã€æ¨¡å—10ã€‘UTILITY_FUNCTIONS - å·¥å…·å‡½æ•°åº“ï¼ˆæ–°å¢å®Œæ•´å®ç°ï¼‰

```python
# ==================== èƒ¶å›Šè§£æ ====================

FUNCTION PARSE_CAPSULE(CAPSULE):
    """
    è§£æä¿¡æ¯èƒ¶å›Šï¼Œæå–æ‰€æœ‰æ¨¡å—çš„æ•°æ®
    è¿”å›ç»“æ„åŒ–çš„æ•°æ®å¯¹è±¡
    """
    
    parsed = {
        "meta": {},           # å…ƒä¿¡æ¯ï¼ˆç« èŠ‚å·ã€æ ‡é¢˜ç­‰ï¼‰
        "context": {},        # ä¸Šä¸‹æ–‡ï¼ˆæ—¶ç©ºã€ä¸»è§’å¿«ç…§ç­‰ï¼‰
        "goals": {},          # ç›®æ ‡ï¼ˆæ ¸å¿ƒä»»åŠ¡ã€é’©å­ç­‰ï¼‰
        "characters": {},     # è§’è‰²æ¡£æ¡ˆ
        "relationships": {},  # å…³ç³»æ¸©åº¦
        "emotions": {},       # æƒ…ç»ªæ•°æ®
        "constraints": {},    # çº¦æŸï¼ˆçº¢çº¿ã€ç¦æ­¢æ¸…å•ç­‰ï¼‰
        "world": {},          # ä¸–ç•Œè§‚
        "resources": {},      # èµ„æºæ¸…å•
        "style": {},          # è¯­è¨€é£æ ¼
        "sensors": {},        # æ„Ÿå®˜ç´ æ
        "risks": {},          # é«˜å±é£é™©
        "prev_chapter": "",   # ä¸Šç« æ­£æ–‡
        "info_points": [],    # å¿…é¡»åŸ‹çš„ä¿¡æ¯ç‚¹
        "hooks": []           # æœªå®Œæˆçš„é’©å­
    }
    
    # è§£æÂ§1 æ­¤æ—¶æ­¤åˆ»
    parsed.context.time = EXTRACT(CAPSULE, "Â§1", "æ—¶é—´")
    parsed.context.location = EXTRACT(CAPSULE, "Â§1", "åœ°ç‚¹")
    parsed.context.atmosphere = EXTRACT(CAPSULE, "Â§1", "å¤©æ°”/å…‰çº¿/å£°éŸ³")
    parsed.context.protagonist_posture = EXTRACT(CAPSULE, "Â§1", "å§¿æ€")
    parsed.context.protagonist_body = EXTRACT(CAPSULE, "Â§1", "èº«ä½“")
    parsed.context.protagonist_focus = EXTRACT(CAPSULE, "Â§1", "æ³¨æ„åŠ›ç„¦ç‚¹")
    parsed.context.protagonist_emotion = EXTRACT(CAPSULE, "Â§1", "ä¸»å¯¼æƒ…ç»ª")
    
    # è§£æÂ§2 ä¸Šç« æ¥å£
    parsed.prev_chapter = EXTRACT(CAPSULE, "Â§19", "ä¸Šç« å®Œæ•´æ­£æ–‡")
    parsed.context.connection_instruction = EXTRACT(CAPSULE, "Â§2.2", "æœ¬ç« è¡”æ¥è¯´æ˜")
    parsed.context.body_memory = EXTRACT(CAPSULE, "Â§2.3", "èº«ä½“è®°å¿†")
    parsed.context.emotion_residue = EXTRACT(CAPSULE, "Â§2.4", "æƒ…æ„Ÿä½™æ¸©")
    parsed.context.unfinished_thoughts = EXTRACT(CAPSULE, "Â§2.5", "æœªå®Œæˆçš„å¿µå¤´")
    parsed.context.avoid_breaks = EXTRACT(CAPSULE, "Â§2.6", "å¿…é¡»é¿å…çš„æ–­è£‚")
    
    # è§£æÂ§3 æœ¬ç« è¦å®Œæˆä»€ä¹ˆ
    parsed.goals.core_mission = EXTRACT(CAPSULE, "Â§3", "æ ¸å¿ƒä»»åŠ¡")
    parsed.goals.wildcard = EXTRACT(CAPSULE, "Â§3", "ç–¯ç‹‚åˆ†æ”¯")
    parsed.goals.hooks_to_plant = EXTRACT(CAPSULE, "Â§3", "æœ¬ç« å¿…é¡»åŸ‹ä¸‹çš„é’©å­")
    parsed.goals.cost_risk = EXTRACT(CAPSULE, "Â§3", "æœ¬ç« ä»£ä»·/é£é™©")
    parsed.goals.failure_consequence = EXTRACT(CAPSULE, "Â§3", "æœ¬ç« å¤±è´¥åæœ")
    parsed.goals.emotional_goal = EXTRACT(CAPSULE, "Â§3", "æƒ…æ„Ÿç›®æ ‡")
    parsed.goals.conflict_suggestions = EXTRACT(CAPSULE, "Â§3", "å†²çªå»ºè®®")
    parsed.goals.absolute_no = EXTRACT(CAPSULE, "Â§3", "ç»å¯¹ç¦æ­¢")
    
    # è§£æÂ§4 å…³ç³»æ¸©åº¦
    parsed.relationships = PARSE_RELATIONSHIP_TABLE(CAPSULE, "Â§4")
    
    # è§£æÂ§5 è§’è‰²å†…å¿ƒä¸–ç•Œ
    parsed.emotions.protagonist = {
        "micro_expressions": EXTRACT(CAPSULE, "Â§5", "æ­¤åˆ»çš„æ˜¾å¾®é•œç‰¹å†™"),
        "dominant_emotion": EXTRACT(CAPSULE, "Â§5", "æ­¤åˆ»çš„ä¸»å¯¼æƒ…ç»ª"),
        "active_thought": EXTRACT(CAPSULE, "Â§5", "æ­¤åˆ»è„‘å­é‡Œæœ€æ´»è·ƒçš„å¿µå¤´"),
        "core_conflict": EXTRACT(CAPSULE, "Â§5", "æ­¤åˆ»çš„æ ¸å¿ƒå†²çª"),
        "behavior_motivation": EXTRACT(CAPSULE, "Â§5", "è¡Œä¸ºçš„æƒ…æ„ŸåŠ¨æœº")
    }
    
    # è§£æÂ§6 æƒ…ç»ªçš„å¯èƒ½æ€§
    parsed.emotions.start_emotion = EXTRACT(CAPSULE, "Â§6", "å¼€ç¯‡æƒ…ç»ª")
    parsed.emotions.lower_bound = EXTRACT(CAPSULE, "Â§6", "æœ€ä½ä¸è·Œç ´")
    parsed.emotions.upper_bound = EXTRACT(CAPSULE, "Â§6", "æœ€é«˜å¯ä»¥åˆ°")
    parsed.emotions.fuel_elements = EXTRACT(CAPSULE, "Â§6", "æƒ…ç»ªç‡ƒæ–™")
    
    # è§£æÂ§7 ä¸»è§’æ¡£æ¡ˆ
    parsed.characters.protagonist = {
        "name": EXTRACT(CAPSULE, "Â§7", "å§“å"),
        "current_situation": EXTRACT(CAPSULE, "Â§7", "æœ¬ç« å¤„å¢ƒ"),
        "mbti": EXTRACT(CAPSULE, "Â§7", "MBTI"),
        "core_drive": EXTRACT(CAPSULE, "Â§7", "æ ¸å¿ƒé©±åŠ¨åŠ›"),
        "decision_instinct": EXTRACT(CAPSULE, "Â§7", "å†³ç­–æœ¬èƒ½"),
        "ooc_redlines": EXTRACT(CAPSULE, "Â§7", "ç»å¯¹ä¸ä¼šåšçš„äº‹"),
        "speech_style": EXTRACT(CAPSULE, "Â§7", "è¨€è¡Œæ ‡ç­¾"),
        "ability_boundary": EXTRACT(CAPSULE, "Â§7", "èƒ½åŠ›è¾¹ç•Œ")
    }
    
    # è§£æÂ§8 é…è§’æ¡£æ¡ˆ
    parsed.characters.supporting = PARSE_SUPPORTING_CHARACTERS(CAPSULE, "Â§8")
    
    # è§£æÂ§9 è®°å¿†åº“
    parsed.info_points = EXTRACT(CAPSULE, "Â§9", "æœ¬ç« å¿…é¡»åŸ‹ä¸‹çš„ä¿¡æ¯ç‚¹")
    parsed.context.relevant_facts = EXTRACT(CAPSULE, "Â§9", "æœ¬ç« ç›¸å…³çš„Fact")
    parsed.hooks = EXTRACT(CAPSULE, "Â§9", "æœªå®Œæˆçš„é’©å­")
    parsed.context.urgent_problems = EXTRACT(CAPSULE, "Â§9", "å½“å‰æœ€ç´§è¿«çš„é—®é¢˜")
    parsed.context.info_asymmetry = EXTRACT(CAPSULE, "Â§9", "ä¿¡æ¯ä¸å¯¹ç§°")
    
    # è§£æÂ§10 ä¸–ç•Œè§‚
    parsed.world = PARSE_WORLDVIEW_SETTINGS(CAPSULE, "Â§10")
    
    # è§£æÂ§11 èµ„æºæ¸…å•
    parsed.resources = PARSE_RESOURCES(CAPSULE, "Â§11")
    
    # è§£æÂ§12 è¯­è¨€é£æ ¼é”šç‚¹
    parsed.style = PARSE_STYLE_TABLE(CAPSULE, "Â§12")
    
    # è§£æÂ§13 æ„Ÿå®˜ç´ æåº“
    parsed.sensors = PARSE_SENSOR_MATERIALS(CAPSULE, "Â§13")
    
    # è§£æÂ§14-15 çº¢çº¿å’Œç¦æ­¢æ¸…å•
    parsed.constraints.redlines = EXTRACT(CAPSULE, "Â§14", "ç»å¯¹çº¢çº¿")
    parsed.constraints.prohibitions = EXTRACT(CAPSULE, "Â§15", "ç¦æ­¢æ¸…å•æ±‡æ€»")
    
    # è§£æÂ§16 é“ºå«ä»»åŠ¡
    parsed.goals.foreshadowing = EXTRACT(CAPSULE, "Â§16", "ä¸ºä¸‹ä¸€ç« åŸ‹é’‰å­")
    
    # è§£æÂ§17 è¡¥å……ä¿¡æ¯
    parsed.meta.additional_info = EXTRACT(CAPSULE, "Â§17", "è¡¥å……ä¿¡æ¯")
    
    # è§£æÂ§18 é«˜å±é£é™©é¢„è­¦
    parsed.risks = PARSE_HIGH_RISKS(CAPSULE, "Â§18")
    
    RETURN parsed
END FUNCTION

FUNCTION VALIDATE_CAPSULE_INTEGRITY(parsed_data):
    """
    æ ¡éªŒèƒ¶å›Šæ•°æ®å®Œæ•´æ€§
    """
    required_fields = [
        "context.time",
        "context.location",
        "goals.core_mission",
        "characters.protagonist",
        "constraints.redlines"
    ]
    
    FOR field IN required_fields:
        IF NOT EXISTS(parsed_data, field):
            THROW ERROR "èƒ¶å›Šæ•°æ®ç¼ºå¤±å…³é”®å­—æ®µ: {field}"
        END IF
    END FOR
    
    PRINT "[VALIDATION] èƒ¶å›Šæ•°æ®å®Œæ•´æ€§æ ¡éªŒé€šè¿‡"
END FUNCTION



# ==================== åœºæ™¯ä¼°ç®— ====================

FUNCTION ESTIMATE_SCENE_COUNT(parsed_data):
    """
    ä»Capsuleæ¨æ–­åœºæ™¯æ•°é‡
    è§„åˆ™ï¼š3000-4000å­— â†’ 4-5ä¸ªåœºæ™¯
    """
    word_target = parsed_data.meta.GET("word_count_target", WORD_COUNT_TARGET)
    scene_count = ROUND(word_target / SCENE_WORD_AVG)
    
    # çº¦æŸåœ¨åˆç†èŒƒå›´ï¼ˆæœ€å°‘3åœºæ™¯ï¼Œæœ€å¤š8åœºæ™¯ï¼‰
    scene_count = CLAMP(scene_count, 3, 8)
    
    RETURN scene_count
END FUNCTION

FUNCTION GET_SCENE_IMPORTANCES(parsed_data, scene_count):
    """
    è·å–æ¯ä¸ªåœºæ™¯çš„é‡è¦æ€§ï¼ˆ1-10åˆ†ï¼‰
    """
    importances = {}
    
    # å¦‚æœCapsuleä¸­æ˜ç¡®æŒ‡å®šäº†åœºæ™¯é‡è¦æ€§ï¼Œä½¿ç”¨æŒ‡å®šå€¼
    IF "scene_importances" IN parsed_data.meta:
        RETURN parsed_data.meta.scene_importances
    END IF
    
    # å¦åˆ™ä½¿ç”¨é»˜è®¤è§„åˆ™ï¼š
    # å¼€ç¯‡å’Œç»“å°¾æœ€é‡è¦(8åˆ†)ï¼Œä¸­é—´é€’å¢(5â†’7åˆ†)
    FOR i = 1 TO scene_count:
        IF i == 1 OR i == scene_count:
            importances[i] = 8  # å¼€ç¯‡å’Œç»“å°¾
        ELSE IF i <= scene_count / 2:
            importances[i] = 5  # å‰åŠéƒ¨åˆ†
        ELSE:
            importances[i] = 7  # ååŠéƒ¨åˆ†ï¼ˆé€’å¢ï¼‰
        END IF
    END FOR
    
    RETURN importances
END FUNCTION

# ==================== çˆ½ç‚¹ç›¸å…³ ====================

FUNCTION SELECT_COOL_POINT_TYPE_FOR_SCENE(scene_idx, parsed_data, scene_count):
    """ä¸ºåœºæ™¯é€‰æ‹©åˆé€‚çš„çˆ½ç‚¹ç±»å‹"""
    
    # æ ¹æ®åœºæ™¯ä½ç½®æ¨æ–­åˆé€‚çš„çˆ½ç‚¹
    position_ratio = scene_idx / scene_count
    
    IF position_ratio < 0.3:
        # å‰30%ï¼šè£…é€¼çˆ½æˆ–æ‰“è„¸çˆ½
        RETURN RANDOM_CHOICE(["è£…é€¼çˆ½", "æ‰“è„¸çˆ½"])
    
    ELSE IF position_ratio < 0.7:
        # ä¸­æ®µï¼šå¤ä»‡çˆ½æˆ–å‡çº§çˆ½
        RETURN RANDOM_CHOICE(["å¤ä»‡çˆ½", "å‡çº§çˆ½"])
    
    ELSE:
        # å30%ï¼šåæ€çˆ½
        RETURN "åæ€çˆ½"
    END IF
END FUNCTION

FUNCTION GENERATE_COOL_POINT(cool_type, parsed_data, scene_idx):
    """ç”Ÿæˆçˆ½ç‚¹æ–‡æœ¬"""
    cool_config = COOL_POINT_TYPES[cool_type]
    structure_parts = SPLIT(cool_config.ç»“æ„, " â†’ ")
    
    cool_text = ""
    
    FOR part IN structure_parts:
        phase_text = WRITE_COOL_PHASE(part, parsed_data, cool_type)
        cool_text += phase_text + "\n\n"
    END FOR
    
    RETURN cool_text
END FUNCTION

FUNCTION INSERT_COOL_POINT(scene_text, cool_text, cool_type):
    """
    æ’å…¥çˆ½ç‚¹åˆ°åœºæ™¯ä¸­ï¼ˆä¿®å¤ç‰ˆï¼šæ˜ç¡®ä½ç½®ï¼‰
    """
    insert_position = COOL_POINT_TYPES[cool_type].æ’å…¥ä½ç½®
    
    SWITCH insert_position:
        CASE "å†²çªå":
            # æ‰¾åˆ°å†²çªç»“æŸçš„ä½ç½®
            conflict_end = FIND_CONFLICT_END(scene_text)
            IF conflict_end > 0:
                RETURN scene_text[:conflict_end] + "\n\n" + cool_text + scene_text[conflict_end:]
            END IF
        
        CASE "åœºæ™¯é«˜æ½®":
            # æ‰¾åˆ°åœºæ™¯çš„é«˜æ½®ä½ç½®ï¼ˆ70%å¤„ï¼‰
            insert_pos = ROUND(LENGTH(scene_text) * 0.7)
            # æ‰¾åˆ°æœ€è¿‘çš„æ®µè½ç»“æŸ
            para_end = FIND_NEXT(scene_text, "\n\n", insert_pos)
            IF para_end > 0:
                RETURN scene_text[:para_end] + "\n\n" + cool_text + scene_text[para_end:]
            END IF
        
        CASE "åœºæ™¯ä¸­æ®µ":
            # 50%ä½ç½®æ’å…¥
            insert_pos = ROUND(LENGTH(scene_text) * 0.5)
            para_end = FIND_NEXT(scene_text, "\n\n", insert_pos)
            IF para_end > 0:
                RETURN scene_text[:para_end] + "\n\n" + cool_text + scene_text[para_end:]
            END IF
        
        CASE "åœºæ™¯æœ«å°¾":
            # ç›´æ¥è¿½åŠ åˆ°åœºæ™¯æœ«å°¾
            RETURN scene_text + "\n\n" + cool_text
    END SWITCH
    
    # é»˜è®¤ï¼šæ’å…¥åˆ°åœºæ™¯ä¸­æ®µ
    mid_pos = ROUND(LENGTH(scene_text) * 0.5)
    RETURN scene_text[:mid_pos] + "\n\n" + cool_text + scene_text[mid_pos:]
END FUNCTION

# ==================== ä¼ç¬”ç›¸å…³ ====================

FUNCTION CHOOSE_SCENE_FOR_FORESHADOW(foreshadow, scene_count, parsed_data):
    """é€‰æ‹©åŸ‹ä¼ç¬”çš„åœºæ™¯"""
    # å¦‚æœCapsuleä¸­æŒ‡å®šäº†åœºæ™¯ï¼Œä½¿ç”¨æŒ‡å®šå€¼
    IF "target_scene" IN foreshadow:
        RETURN foreshadow.target_scene
    END IF
    
    # å¦åˆ™æ ¹æ®ä¼ç¬”ç±»å‹é€‰æ‹©ï¼š
    # æ˜¾çœ¼åº¦é«˜çš„æ”¾åœ¨ååŠéƒ¨åˆ†ï¼Œéšè”½çš„æ”¾åœ¨å‰åŠéƒ¨åˆ†
    visibility = foreshadow.GET("visibility", "éšè”½")
    
    IF visibility IN ["æåº¦æ˜¾çœ¼", "æ˜¾çœ¼"]:
        # ååŠéƒ¨åˆ†ï¼ˆ60%-80%ï¼‰
        RETURN RANDOM_INT(CEIL(scene_count * 0.6), CEIL(scene_count * 0.8))
    ELSE:
        # å‰åŠéƒ¨åˆ†ï¼ˆ20%-50%ï¼‰
        RETURN RANDOM_INT(CEIL(scene_count * 0.2), CEIL(scene_count * 0.5))
    END IF
END FUNCTION

FUNCTION WRITE_FORESHADOW(foreshadow, parsed_data):
    """
    å†™ä¼ç¬”/é’‰å­/é“ºå«ï¼ˆæ ¹æ®æ˜¾çœ¼åº¦ï¼‰
    """
    visibility = foreshadow.GET("visibility", "éšè”½")
    content = foreshadow.GET("content", "")
    
    SWITCH visibility:
        CASE "æåº¦éšè”½":
            # æ··åœ¨5ä¸ªå…¶ä»–ç»†èŠ‚ä¸­ï¼Œä¸€ç¬”å¸¦è¿‡
            other_details = GENERATE_RANDOM_DETAILS(5, parsed_data)
            all_details = SHUFFLE([content] + other_details)
            RETURN JOIN(all_details, "ï¼Œ") + "ã€‚"
        
        CASE "éšè”½":
            # æ­£å¸¸æå†™ï¼Œä¸ç‰¹åˆ«å¼ºè°ƒ
            RETURN f"{content}ã€‚"
        
        CASE "å¾®æ˜¾çœ¼":
            # ç•¥å¾®å¤šå†™å‡ ä¸ªå­—
            detail = ADD_MINOR_DETAIL(content, parsed_data)
            RETURN f"{content}ï¼Œ{detail}ã€‚"
        
        CASE "æ˜¾çœ¼":
            # å•ç‹¬æˆæ®µ
            RETURN f"\n\n{content}ã€‚\n\n"
        
        CASE "æåº¦æ˜¾çœ¼":
            # å¼ºè°ƒ + ä¸»è§’æ³¨æ„åˆ°
            reaction = GENERATE_PROTAGONIST_REACTION(content, parsed_data)
            RETURN f"\n\n{content}ã€‚{reaction}\n\n"
    END SWITCH
END FUNCTION

FUNCTION INSERT_FORESHADOW(scene_text, foreshadow_text, visibility):
    """
    æ’å…¥ä¼ç¬”åˆ°åœºæ™¯ä¸­
    """
    IF visibility IN ["æåº¦éšè”½", "éšè”½"]:
        # æ’å…¥åˆ°åœºæ™¯å‰30%ï¼ˆä¸æ˜¾çœ¼ä½ç½®ï¼‰
        insert_pos = ROUND(LENGTH(scene_text) * 0.3)
    ELSE:
        # æ’å…¥åˆ°åœºæ™¯ä¸­æ®µæˆ–åæ®µï¼ˆæ˜¾çœ¼ä½ç½®ï¼‰
        insert_pos = ROUND(LENGTH(scene_text) * 0.6)
    END IF
    
    # æ‰¾åˆ°æœ€è¿‘çš„æ®µè½ç»“æŸ
    para_end = FIND_NEXT(scene_text, "\n\n", insert_pos)
    
    IF para_end > 0:
        RETURN scene_text[:para_end] + foreshadow_text + scene_text[para_end:]
    ELSE:
        # æ‰¾ä¸åˆ°æ®µè½ç»“æŸï¼Œæ’å…¥åˆ°ä¸­é—´ä½ç½®
        RETURN scene_text[:insert_pos] + foreshadow_text + scene_text[insert_pos:]
    END IF
END FUNCTION

# ==================== åœºæ™¯åˆ†è§£ ====================

FUNCTION DECOMPOSE_SCENE_TO_UNITS(scene_idx, parsed_data, humanizer):
    """
    å°†åœºæ™¯æ‹†è§£ä¸ºå†™ä½œå•å…ƒ
    """
    units = []
    
    # å•å…ƒ1ï¼šåœºæ™¯å¼€åœºï¼ˆç¯å¢ƒæˆ–åŠ¨ä½œï¼‰
    IF scene_idx == 1:
        # å¼€ç¯‡åœºæ™¯ï¼šæ ¹æ®å¼€ç¯‡ä»»åŠ¡å†³å®š
        opening_task = FIND_TASK_BY_TYPE(humanizer.task_checklist.GET(scene_idx, []), "å¼€ç¯‡")
        IF opening_task:
            # å¼€ç¯‡ä»»åŠ¡ä¼šå¤„ç†ï¼Œè¿™é‡Œä¸åŠ ç¯å¢ƒå•å…ƒ
            pass
        ELSE:
            units.APPEND({
                "type": "æå†™",
                "importance": 5,
                "content": "ç¯å¢ƒ"
            })
        END IF
    ELSE:
        # éå¼€ç¯‡ï¼šç®€çŸ­ç¯å¢ƒæå†™ï¼ˆä½é‡è¦æ€§ï¼‰
        units.APPEND({
            "type": "æå†™",
            "importance": 3,
            "content": "ç¯å¢ƒ"
        })
    END IF
    
    # å•å…ƒ2ï¼šä¸»è¦åŠ¨ä½œ/å¯¹è¯ï¼ˆä»æ ¸å¿ƒä»»åŠ¡æ¨æ–­ï¼‰
    IF "core_mission" IN parsed_data.goals:
        mission = parsed_data.goals.core_mission
        mission_units = INFER_UNITS_FROM_MISSION(mission, scene_idx, parsed_data)
        units.EXTEND(mission_units)
    END IF
    
    # å•å…ƒ3ï¼šæƒ…ç»ªååº”ï¼ˆå¦‚æœéœ€è¦ï¼‰
    IF scene_idx IN [1, ROUND(GET_SCENE_COUNT_FROM_CAPSULE(parsed_data) / 2)]:
        units.APPEND({
            "type": "æƒ…ç»ª",
            "importance": 7,
            "emotion_type": parsed_data.emotions.GET("start_emotion", "ç„¦è™‘"),
            "intensity": 60
        })
    END IF
    
    RETURN units
END FUNCTION

FUNCTION INFER_UNITS_FROM_MISSION(mission, scene_idx, parsed_data):
    """
    ä»æ ¸å¿ƒä»»åŠ¡æ¨æ–­åœºæ™¯éœ€è¦çš„å•å…ƒ
    """
    units = []
    
    # æå–ä»»åŠ¡ä¸­çš„åŠ¨ä½œå…³é”®è¯
    action_keywords = EXTRACT_ACTION_KEYWORDS(mission)
    
    FOR keyword IN action_keywords:
        units.APPEND({
            "type": "åŠ¨ä½œ",
            "importance": 8,
            "content": keyword
        })
    END FOR
    
    # å¦‚æœä»»åŠ¡ä¸­åŒ…å«å¯¹è¯å…³é”®è¯ï¼ŒåŠ å…¥å¯¹è¯å•å…ƒ
    IF CONTAINS_DIALOGUE_KEYWORDS(mission):
        units.APPEND({
            "type": "å¯¹è¯",
            "importance": 7,
            "content": "ä¸å…³é”®è§’è‰²å¯¹è¯"
        })
    END IF
    
    RETURN units
END FUNCTION

# ==================== åœºæ™¯æ£€æµ‹ ====================

FUNCTION DETECT_SCENES(text):
    """
    æ£€æµ‹æ–‡æœ¬ä¸­çš„åœºæ™¯åˆ†å‰²
    """
    # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
    paragraphs = SPLIT_PARAGRAPHS(text)
    
    scenes = []
    current_scene = {
        "start_idx": 0,
        "content": "",
        "paragraphs": []
    }
    
    FOR i, para IN ENUMERATE(paragraphs):
        current_scene.paragraphs.APPEND(para)
        current_scene.content += para + "\n\n"
        
        # æ£€æµ‹åœºæ™¯åˆ†å‰²ä¿¡å·ï¼š
        # 1. æ—¶é—´è·³è·ƒè¯
        # 2. ç©ºé—´è½¬æ¢è¯
        # 3. æ˜æ˜¾çš„ç»“æŸå¥
        IF HAS_SCENE_BREAK_SIGNAL(para) AND i < LENGTH(paragraphs) - 1:
            scenes.APPEND(current_scene)
            current_scene = {
                "start_idx": i + 1,
                "content": "",
                "paragraphs": []
            }
        END IF
    END FOR
    
    # æ·»åŠ æœ€åä¸€ä¸ªåœºæ™¯
    IF LENGTH(current_scene.paragraphs) > 0:
        scenes.APPEND(current_scene)
    END IF
    
    RETURN scenes
END FUNCTION

FUNCTION ANALYZE_SCENE_STRUCTURE(scene):
    """
    åˆ†æåœºæ™¯ç»“æ„ï¼ˆèµ·æ‰¿è½¬åˆï¼‰
    """
    paragraphs = scene.paragraphs
    para_count = LENGTH(paragraphs)
    
    structure = {
        "has_start": FALSE,      # èµ·
        "has_development": FALSE,  # æ‰¿
        "has_turn": FALSE,        # è½¬
        "has_conclusion": FALSE,  # åˆ
        "has_all_four_parts": FALSE
    }
    
    IF para_count < 4:
        RETURN structure
    END IF
    
    # æ£€æµ‹"èµ·"ï¼šç¬¬ä¸€æ®µæ˜¯å¦æœ‰ç¯å¢ƒ/åŠ¨ä½œå¼€åœº
    IF HAS_OPENING_PATTERN(paragraphs[0]):
        structure.has_start = TRUE
    END IF
    
    # æ£€æµ‹"æ‰¿"ï¼šä¸­é—´æ®µè½æ˜¯å¦æœ‰å‘å±•
    middle_paras = paragraphs[1:para_count-2]
    IF HAS_DEVELOPMENT_PATTERN(middle_paras):
        structure.has_development = TRUE
    END IF
    
    # æ£€æµ‹"è½¬"ï¼šæ˜¯å¦æœ‰è½¬æŠ˜è¯æˆ–å†²çª
    FOR para IN paragraphs:
        IF HAS_TURN_PATTERN(para):
            structure.has_turn = TRUE
            BREAK
        END IF
    END FOR
    
    # æ£€æµ‹"åˆ"ï¼šæœ€åä¸€æ®µæ˜¯å¦æœ‰æ€»ç»“
    IF HAS_CONCLUSION_PATTERN(paragraphs[-1]):
        structure.has_conclusion = TRUE
    END IF
    
    # åˆ¤æ–­æ˜¯å¦å®Œæ•´
    structure.has_all_four_parts = (
        structure.has_start AND
        structure.has_development AND
        structure.has_turn AND
        structure.has_conclusion
    )
    
    RETURN structure
END FUNCTION

# ==================== çº¢çº¿æ£€æŸ¥ ====================

FUNCTION CHECK_REDLINE_VIOLATION(text, redline):
    """æ£€æŸ¥æ˜¯å¦è¿åçº¢çº¿ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰"""
    # æå–çº¢çº¿ä¸­çš„å…³é”®è¯
    keywords = EXTRACT_KEYWORDS(redline)
    
    # å¿«é€Ÿç­›é€‰ï¼šæ‰€æœ‰å…³é”®è¯éƒ½å­˜åœ¨æ‰è¿›ä¸€æ­¥æ£€æŸ¥
    FOR keyword IN keywords:
        IF keyword NOT IN text:
            RETURN FALSE
        END IF
    END FOR
    
    # è¯­ä¹‰æ£€æŸ¥ï¼šæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„å¥å­ï¼Œåˆ¤æ–­æ˜¯å¦è¿è§„
    sentences_with_keywords = FIND_SENTENCES_CONTAINING_ALL(text, keywords)
    
    FOR sentence IN sentences_with_keywords:
        IF SEMANTIC_MATCH(sentence, redline):
            RETURN TRUE
        END IF
    END FOR
    
    RETURN FALSE
END FUNCTION

FUNCTION CONTAINS_REDLINE_KEYWORDS(text, parsed_data):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«çº¢çº¿å…³é”®è¯"""
    IF "redlines" NOT IN parsed_data.constraints:
        RETURN FALSE
    END IF
    
    FOR redline IN parsed_data.constraints.redlines:
        keywords = EXTRACT_KEYWORDS(redline)
        IF ALL(keyword IN text FOR keyword IN keywords):
            RETURN TRUE
        END IF
    END FOR
    
    RETURN FALSE
END FUNCTION

# ==================== ä»»åŠ¡æ£€æŸ¥ ====================

FUNCTION CHECK_MISSION_COMPLETED(text, mission, parsed_data):
    """æ£€æŸ¥æ ¸å¿ƒä»»åŠ¡æ˜¯å¦å®Œæˆ"""
    # æå–ä»»åŠ¡å…³é”®è¯
    mission_keywords = EXTRACT_KEYWORDS(mission)
    
    # æ‰€æœ‰å…³é”®è¯éƒ½å¿…é¡»å‡ºç°
    FOR keyword IN mission_keywords:
        IF keyword NOT IN text:
            RETURN FALSE
        END IF
    END FOR
    
    # è¯­ä¹‰éªŒè¯ï¼šä¸ä»…å…³é”®è¯å­˜åœ¨ï¼Œè¿˜è¦ç¡®ä¿å®Œæˆäº†ä»»åŠ¡
    RETURN SEMANTIC_VERIFY_MISSION(text, mission, mission_keywords)
END FUNCTION

FUNCTION CONTAINS_CORE_MISSION_KEYWORDS(text, parsed_data):
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ ¸å¿ƒä»»åŠ¡å…³é”®è¯"""
    IF "core_mission" NOT IN parsed_data.goals:
        RETURN FALSE
    END IF
    
    mission = parsed_data.goals.core_mission
    keywords = EXTRACT_KEYWORDS(mission)
    
    RETURN ALL(keyword IN text FOR keyword IN keywords)
END FUNCTION

FUNCTION CHECK_HOOK_PLANTED(text, hook):
    """æ£€æŸ¥é’©å­æ˜¯å¦åŸ‹ä¸‹"""
    hook_keywords = EXTRACT_KEYWORDS(hook)
    RETURN ALL(keyword IN text FOR keyword IN hook_keywords)
END FUNCTION

# ==================== ä¿¡æ¯å¯†åº¦è®¡ç®— ====================

FUNCTION COUNT_NEW_INFO(text):
    """ç»Ÿè®¡æ–°ä¿¡æ¯ç‚¹æ•°é‡"""
    info_count = 0
    
    # ä¿¡æ¯ç‚¹1ï¼šä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€ç‰©å“åï¼‰
    info_count += COUNT_PROPER_NOUNS(text)
    
    # ä¿¡æ¯ç‚¹2ï¼šå‰§æƒ…åŠ¨è¯ï¼ˆå‘ç°ã€å¾—åˆ°ã€å¤±å»ã€æ‰“è´¥ç­‰ï¼‰
    plot_verbs = ["å‘ç°", "å¾—åˆ°", "å¤±å»", "æ‰“è´¥", "é€ƒç¦»", "è¿›å…¥", "ç¦»å¼€", "é‡åˆ°"]
    FOR verb IN plot_verbs:
        info_count += COUNT_OCCURRENCES(text, verb)
    END FOR
    
    # ä¿¡æ¯ç‚¹3ï¼šå¯¹è¯å›åˆï¼ˆæ¯2å¥å¯¹è¯ç®—1ä¸ªä¿¡æ¯ç‚¹ï¼‰
    dialogue_exchanges = COUNT_DIALOGUE_EXCHANGES(text)
    info_count += ROUND(dialogue_exchanges / 2)
    
    # ä¿¡æ¯ç‚¹4ï¼šçŠ¶æ€å˜åŒ–ï¼ˆæƒ…ç»ªã€ä½ç½®ã€å…³ç³»ï¼‰
    info_count += COUNT_STATE_CHANGES(text)
    
    RETURN info_count
END FUNCTION

FUNCTION CALCULATE_INFO_DENSITY(text):
    """è®¡ç®—ä¿¡æ¯å¯†åº¦"""
    char_count = LENGTH(text)
    IF char_count == 0:
        RETURN 0
    END IF
    
    info_count = COUNT_NEW_INFO(text)
    RETURN info_count / char_count
END FUNCTION

# ==================== å¯¹è¯ç›¸å…³ ====================

FUNCTION CALCULATE_DIALOGUE_RATIO(text):
    """è®¡ç®—å¯¹è¯å æ¯”"""
    dialogues = EXTRACT_ALL_DIALOGUES(text)
    
    dialogue_chars = 0
    FOR dialogue IN dialogues:
        dialogue_chars += LENGTH(dialogue.content)
    END FOR
    
    total_chars = LENGTH(text)
    
    IF total_chars == 0:
        RETURN 0
    END IF
    
    RETURN dialogue_chars / total_chars
END FUNCTION

FUNCTION EXTRACT_ALL_DIALOGUES(text):
    """æå–æ‰€æœ‰å¯¹è¯"""
    # åŒ¹é… "..." æˆ– "..." æ ¼å¼
    pattern = r"[""]([^""]+)[""]"
    matches = FIND_ALL_WITH_CONTEXT(text, pattern)
    
    dialogues = []
    FOR match IN matches:
        dialogues.APPEND({
            "content": match.group(1),
            "full_text": match.group(0),
            "start_pos": match.start(),
            "end_pos": match.end()
        })
    END FOR
    
    RETURN dialogues
END FUNCTION

FUNCTION COUNT_DIALOGUE_EXCHANGES(text):
    """ç»Ÿè®¡å¯¹è¯å›åˆæ•°"""
    dialogues = EXTRACT_ALL_DIALOGUES(text)
    RETURN LENGTH(dialogues)
END FUNCTION

# ==================== å†…å¿ƒæˆç›¸å…³ ====================

FUNCTION CALCULATE_INNER_MONOLOGUE_RATIO(text):
    """è®¡ç®—å†…å¿ƒæˆå æ¯”"""
    inner_segments = EXTRACT_INNER_MONOLOGUE(text)
    
    inner_chars = 0
    FOR segment IN inner_segments:
        inner_chars += LENGTH(segment.content)
    END FOR
    
    total_chars = LENGTH(text)
    
    IF total_chars == 0:
        RETURN 0
    END IF
    
    RETURN inner_chars / total_chars
END FUNCTION

FUNCTION EXTRACT_INNER_MONOLOGUE(text):
    """æå–æ‰€æœ‰å†…å¿ƒæˆç‰‡æ®µ"""
    # å†…å¿ƒæˆç‰¹å¾ï¼š
    # 1. "ä»–æƒ³"ã€"ä»–å¿ƒæƒ³"ã€"ä»–è§‰å¾—"ç­‰
    # 2. "ä»–æš—è‡ª"ã€"ä»–å¿ƒé‡Œ"ç­‰
    # 3. é—®è‡ªå·±çš„é—®é¢˜
    
    inner_patterns = [
        r"(ä»–|å¥¹)(æƒ³|å¿ƒæƒ³|è§‰å¾—|è®¤ä¸º|æš—è‡ª).{5,100}",
        r"(ä»–|å¥¹)(çš„)?(å¿ƒé‡Œ|å†…å¿ƒ).{5,100}",
        r""[^""]{5,100}""(ä»–|å¥¹)(å¿ƒæƒ³|æš—æƒ³)"
    ]
    
    segments = []
    
    FOR pattern IN inner_patterns:
        matches = FIND_ALL_WITH_CONTEXT(text, pattern)
        FOR match IN matches:
            segments.APPEND({
                "content": match.group(0),
                "full_text": match.group(0),
                "start_pos": match.start(),
                "end_pos": match.end(),
                "importance": 5  # é»˜è®¤é‡è¦æ€§
            })
        END FOR
    END FOR
    
    RETURN segments
END FUNCTION

# ==================== è¾…åŠ©åˆ¤æ–­å‡½æ•° ====================

FUNCTION ARE_SEMANTICALLY_SIMILAR(sent1, sent2, sent3):
    """åˆ¤æ–­3å¥è¯æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼"""
    # æå–å…³é”®è¯
    keywords1 = EXTRACT_KEYWORDS(sent1)
    keywords2 = EXTRACT_KEYWORDS(sent2)
    keywords3 = EXTRACT_KEYWORDS(sent3)
    
    # è®¡ç®—é‡å åº¦
    overlap_12 = LENGTH(INTERSECT(keywords1, keywords2)) / MAX(LENGTH(UNION(keywords1, keywords2)), 1)
    overlap_23 = LENGTH(INTERSECT(keywords2, keywords3)) / MAX(LENGTH(UNION(keywords2, keywords3)), 1)
    
    # å¦‚æœä¸¤ä¸¤é‡å åº¦éƒ½>60%ï¼Œè®¤ä¸ºè¯­ä¹‰ç›¸ä¼¼
    RETURN overlap_12 > 0.6 AND overlap_23 > 0.6
END FUNCTION

FUNCTION CONTAINS_ACTION_TRIGGER(text):
    """æ£€æµ‹æ˜¯å¦åŒ…å«åŠ¨ä½œè§¦å‘è¯"""
    action_triggers = ["çªç„¶", "è¿™æ—¶", "å°±åœ¨", "åªè§", "å¿½ç„¶", "çŒ›åœ°", "ç¬é—´", "éœæ—¶"]
    RETURN ANY(trigger IN text FOR trigger IN action_triggers)
END FUNCTION

FUNCTION IS_STANDALONE_SENTENCE(text, match):
    """åˆ¤æ–­åŒ¹é…æ˜¯å¦æ˜¯ç‹¬ç«‹å¥å­"""
    start = match.start_pos
    end = match.end_pos
    
    # æ£€æŸ¥å‰åæ˜¯å¦æœ‰å¥å·æˆ–æ¢è¡Œ
    before_char = text[start-1] IF start > 0 ELSE "\n"
    after_char = text[end] IF end < LENGTH(text) ELSE "\n"
    
    RETURN before_char IN ["ã€‚", "\n", "ï¼", "ï¼Ÿ"] AND after_char IN ["ã€‚", "\n", "ï¼", "ï¼Ÿ"]
END FUNCTION

FUNCTION HAS_CONFLICT(text):
    """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«å†²çª"""
    conflict_keywords = [
        "åé©³", "æ‹’ç»", "è´¨ç–‘", "å¯¹æŠ—", "å†²çª",
        "ä¸æ»¡", "æ„¤æ€’", "äº‰æ‰§", "å¯¹å³™", "è¾ƒé‡"
    ]
    
    RETURN ANY(keyword IN text FOR keyword IN conflict_keywords)
END FUNCTION

FUNCTION CONTAINS_CHARACTER_INTRO(text, parsed_data):
    """æ£€æµ‹æ˜¯å¦åŒ…å«è§’è‰²ä»‹ç»"""
    IF "characters" NOT IN parsed_data:
        RETURN FALSE
    END IF
    
    FOR char_name, char_info IN parsed_data.characters.ITEMS():
        # å¦‚æœæ–‡æœ¬ä¸­åŒ…å«è§’è‰²åå­— + ç‰¹å¾æè¿°ï¼Œè®¤ä¸ºæ˜¯ä»‹ç»
        IF char_name IN text AND "ç‰¹å¾" IN char_info:
            FOR trait IN char_info.ç‰¹å¾:
                IF trait IN text:
                    RETURN TRUE
                END IF
            END IF
        END IF
    END FOR
    
    RETURN FALSE
END FUNCTION

# ==================== å·¥å…·å‡½æ•° ====================

FUNCTION HAS_NESTED_FIELD(obj, field_path):
    """æ£€æŸ¥åµŒå¥—å­—æ®µæ˜¯å¦å­˜åœ¨"""
    parts = SPLIT(field_path, ".")
    current = obj
    
    FOR part IN parts:
        IF part NOT IN current:
            RETURN FALSE
        END IF
        current = current[part]
    END FOR
    
    RETURN TRUE
END FUNCTION

FUNCTION FIND_TASK_BY_TYPE(tasks, task_type):
    """åœ¨ä»»åŠ¡åˆ—è¡¨ä¸­æŸ¥æ‰¾æŒ‡å®šç±»å‹çš„ä»»åŠ¡"""
    FOR task IN tasks:
        IF task.GET("type", "") == task_type:
            RETURN task
        END IF
    END FOR
    
    RETURN NULL
END FUNCTION

FUNCTION DISTRIBUTE_EVENLY(items, count):
    """å°†countä¸ªæ•°é‡å‡åŒ€åˆ†é…åˆ°itemsä¸­"""
    IF count >= LENGTH(items):
        RETURN items
    END IF
    
    step = LENGTH(items) / count
    result = []
    
    FOR i = 0 TO count-1:
        idx = ROUND(i * step)
        result.APPEND(items[idx])
    END FOR
    
    RETURN result
END FUNCTION

FUNCTION CLAMP(value, min_val, max_val):
    """å°†å€¼çº¦æŸåœ¨èŒƒå›´å†…"""
    IF value < min_val:
        RETURN min_val
    ELSE IF value > max_val:
        RETURN max_val
    ELSE:
        RETURN value
    END IF
END FUNCTION

FUNCTION CLEAN_WHITESPACE(text):
    """æ¸…ç†å¤šä½™ç©ºç™½"""
    # åˆ é™¤å¤šä½™ç©ºæ ¼
    text = REPLACE_PATTERN(text, r" +", " ")
    
    # åˆ é™¤å¤šä½™æ¢è¡Œï¼ˆè¶…è¿‡2ä¸ªè¿ç»­æ¢è¡Œï¼‰
    text = REPLACE_PATTERN(text, r"\n{3,}", "\n\n")
    
    # åˆ é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
    text = STRIP(text)
    
    RETURN text
END FUNCTION

# ==================== ç»Ÿè®¡å‡½æ•° ====================

FUNCTION COUNT_PARAGRAPHS(text):
    """ç»Ÿè®¡æ®µè½æ•°"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    RETURN LENGTH(FILTER(paragraphs, lambda p: LENGTH(p) > 0))
END FUNCTION

FUNCTION AVG_PARAGRAPH_LENGTH(text):
    """è®¡ç®—å¹³å‡æ®µè½é•¿åº¦"""
    paragraphs = SPLIT_PARAGRAPHS(text)
    paragraphs = FILTER(paragraphs, lambda p: LENGTH(p) > 0)
    
    IF LENGTH(paragraphs) == 0:
        RETURN 0
    END IF
    
    total_length = SUM(MAP(paragraphs, LENGTH))
    RETURN total_length / LENGTH(paragraphs)
END FUNCTION

FUNCTION SPLIT_PARAGRAPHS(text):
    """æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½"""
    RETURN SPLIT(text, "\n\n")
END FUNCTION

FUNCTION SPLIT_SENTENCES(text):
    """æŒ‰å¥å·åˆ†å‰²å¥å­"""
    # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
    RETURN SPLIT_PATTERN(text, r"[ã€‚ï¼ï¼Ÿ]")
END FUNCTION
```

---

## ã€æ¨¡å—11ã€‘ERROR_HANDLING - å¼‚å¸¸å¤„ç†ï¼ˆæ–°å¢ï¼‰

```python
# ==================== å¼‚å¸¸å®šä¹‰ ====================

CLASS CapsuleParseError(Exception):
    """èƒ¶å›Šè§£æå¼‚å¸¸"""
    PASS
END CLASS

CLASS SceneWriteError(Exception):
    """åœºæ™¯å†™ä½œå¼‚å¸¸"""
    PASS
END CLASS

CLASS PolishError(Exception):
    """æ¶¦è‰²å¼‚å¸¸"""
    PASS
END CLASS

CLASS RewriteError(Exception):
    """é‡å†™å¼‚å¸¸"""
    PASS
END CLASS

# ==================== å¼‚å¸¸å¤„ç†å‡½æ•° ====================

FUNCTION ERROR_REPORT(error):
    """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
    report = {
        "status": "ERROR",
        "error_type": TYPE(error),
        "error_message": error.message,
        "suggestion": GET_ERROR_SUGGESTION(error)
    }
    
    RETURN report
END FUNCTION

FUNCTION GET_ERROR_SUGGESTION(error):
    """æ ¹æ®é”™è¯¯ç±»å‹ç»™å‡ºå»ºè®®"""
    IF ISINSTANCE(error, CapsuleParseError):
        RETURN "è¯·æ£€æŸ¥ä¿¡æ¯èƒ¶å›Šæ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿åŒ…å«å¿…è¦çš„Â§1-Â§4æ¨¡å—"
    ELSE IF ISINSTANCE(error, SceneWriteError):
        RETURN "åœºæ™¯å†™ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥parsed_dataæ˜¯å¦å®Œæ•´"
    ELSE IF ISINSTANCE(error, PolishError):
        RETURN "æ¶¦è‰²å¤±è´¥ï¼Œä½†å¯ç»§ç»­ä½¿ç”¨æœªæ¶¦è‰²ç‰ˆæœ¬"
    ELSE IF ISINSTANCE(error, RewriteError):
        RETURN "é‡å†™å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¿®æ­£æŒ‡ä»¤æ˜¯å¦åˆç†"
    ELSE:
        RETURN "æœªçŸ¥é”™è¯¯ï¼Œè¯·è”ç³»å¼€å‘è€…"
    END IF
END FUNCTION

FUNCTION WRITE_FALLBACK_SCENE(scene_idx, parsed_data):
    """å†™ä½œå¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ"""
    fallback_text = f"""
[åœºæ™¯{scene_idx}å†™ä½œå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ]

{parsed_data.characters.protagonist.name}ç»§ç»­å‰è¡Œã€‚

"""
    
    PRINT "[FALLBACK] åœºæ™¯{scene_idx}ä½¿ç”¨é™çº§æ–¹æ¡ˆ"
    
    RETURN fallback_text
END FUNCTION
```

---

## ã€æ‰§è¡Œç¤ºä¾‹ã€‘

```python
# ==================== ç¤ºä¾‹æ‰§è¡Œæµç¨‹ ====================

FUNCTION MAIN():
    """ä¸»ç¨‹åºå…¥å£"""
    
    # 1. è¯»å–èƒ¶å›Š
    PRINT "==================== Claudeå†™ä½œç³»ç»Ÿ v3.1 ====================\n"
    
    TRY:
        CAPSULE = READ_FILE("Capsule.md")
        PRINT "[SYSTEM] ä¿¡æ¯èƒ¶å›ŠåŠ è½½æˆåŠŸ\n"
    CATCH FileNotFoundError:
        PRINT "[ERROR] æœªæ‰¾åˆ°Capsule.mdæ–‡ä»¶"
        RETURN
    END TRY
    
    # 2. æ‰§è¡Œä¸»æµç¨‹
    TRY:
        output = MAIN_EXECUTION(CAPSULE)
        
        # 3. è¾“å‡ºç»“æœ
        PRINT "\n==================== æ‰§è¡Œå®Œæˆ ====================\n"
        
        # 3.1 å¿«é€Ÿæ‘˜è¦
        PRINT output.summary
        
        # 3.2 ç« èŠ‚æ­£æ–‡
        PRINT "\n==================== ç« èŠ‚æ­£æ–‡ ====================\n"
        PRINT output.chapter_content
        
        # 3.3 æ–°å¢Fact
        PRINT "\n==================== æ–°å¢Fact ====================\n"
        PRINT output.new_facts
        
        # 3.4 è¯¦ç»†è¯Šæ–­
        PRINT "\n"
        PRINT output.details
        
    CATCH CapsuleParseError AS e:
        PRINT "[ERROR] èƒ¶å›Šè§£æå¤±è´¥: {e.message}"
        PRINT "[SUGGESTION] {GET_ERROR_SUGGESTION(e)}"
        
    CATCH SceneWriteError AS e:
        PRINT "[ERROR] åœºæ™¯å†™ä½œå¤±è´¥: {e.message}"
        PRINT "[SUGGESTION] {GET_ERROR_SUGGESTION(e)}"
        
    CATCH Exception AS e:
        PRINT "[ERROR] æœªçŸ¥é”™è¯¯: {e.message}"
        PRINT "[SUGGESTION] è¯·æ£€æŸ¥è¾“å…¥æ•°æ®å¹¶é‡è¯•"
    END TRY
    
    PRINT "\n==================== ç³»ç»Ÿç»“æŸ ====================\n"
END FUNCTION

# æ‰§è¡Œä¸»ç¨‹åº
IF __name__ == "__main__":
    MAIN()
END IF
```

---

## ã€é™„å½•ã€‘å¿«é€Ÿå‚è€ƒ

### æ ¸å¿ƒå¸¸é‡
- `WORD_COUNT_TARGET`: 4000å­—
- `SCENE_WORD_AVG`: 800å­—/åœºæ™¯
- `TOMATO_CORE_RULES.dialogue_ratio`: [0.35, 0.45]
- `TOMATO_CORE_RULES.inner_monologue_max`: 0.15
- `MAX_REWRITE_ATTEMPTS`: 2æ¬¡

### å…³é”®æµç¨‹
1. **è§£æèƒ¶å›Š** â†’ **ç”Ÿæˆä»»åŠ¡æ¸…å•** â†’ **åˆ†åœºæ™¯å†™ä½œ** â†’ **å®æ—¶æ£€æŸ¥** â†’ **é›†ä¸­æ¶¦è‰²** â†’ **å…¨å±€è¯Šæ–­** â†’ **é—®é¢˜åˆ†æ** â†’ **ä¿®æ­£é‡å†™** â†’ **äº¤ä»˜è¾“å‡º**

### è¯­æ³•è§„èŒƒ
- å˜é‡å­˜åœ¨æ£€æŸ¥: `"key" IN dict`
- é»˜è®¤å€¼è·å–: `dict.GET(key, default)`
- æ•°ç»„è¿‡æ»¤: `FILTER(array, condition)`

### ç—…ç—‡æƒé‡
- é«˜æƒé‡(1.5): D3å†…å¿ƒæˆã€D11å¯†åº¦ã€D18æƒ…ç»ªè¯ã€D20è§†è§’
- ä¸­æƒé‡(1.0-1.2): å¤§éƒ¨åˆ†ç—…ç—‡
- ä½æƒé‡(0.5): D7é€»è¾‘ï¼ˆå› æœè¯é«˜é¢‘ï¼‰
- é›¶æƒé‡(0.0): D6/D9/D10ï¼ˆå†™ä½œé˜¶æ®µå¤„ç†ï¼‰

---

**END OF SOP v3.1**
		
